<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 7 Two Parameters | Gov 50: Data" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="davidkane9/PPBDS" />



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Chapter 7 Two Parameters | Gov 50: Data">

<title>Chapter 7 Two Parameters | Gov 50: Data</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/str_view-0.1.0/str_view.css" rel="stylesheet" />
<script src="libs/str_view-binding-1.4.0/str_view.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Gov 50: Data<p><p class="author"></p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html"></a>
<a href="preamble.html">Preamble</a>
<a href="shopping-week.html">Shopping Week</a>
<a href="visualization.html"><span class="toc-section-number">1</span> Visualization</a>
<a href="wrangling.html"><span class="toc-section-number">2</span> Wrangling</a>
<a href="rubin-causal-model.html"><span class="toc-section-number">3</span> Rubin Causal Model</a>
<a href="functions.html"><span class="toc-section-number">4</span> Functions</a>
<a href="probability.html"><span class="toc-section-number">5</span> Probability</a>
<a href="one-parameter.html"><span class="toc-section-number">6</span> One Parameter</a>
<a id="active-page" href="two-parameters.html"><span class="toc-section-number">7</span> Two Parameters</a><ul class="toc-sections">
<li class="toc"><a href="#resampling-tactile"> Pennies example</a></li>
<li class="toc"><a href="#eda-for-nhanes"> EDA for <code>nhanes</code></a></li>
<li class="toc"><a href="#bootstrap-to-estimate-average-height"> Bootstrap to estimate average height</a></li>
<li class="toc"><a href="#probability-to-bootstrap-to-bayesian-models"> Probability to bootstrap to Bayesian models</a></li>
<li class="toc"><a href="#cardinal-virtues"> Cardinal Virtues</a></li>
<li class="toc"><a href="#conclusion"> Conclusion</a></li>
</ul>
<a href="three-parameters.html"><span class="toc-section-number">8</span> Three Parameters</a>
<a href="n-parameters.html"><span class="toc-section-number">9</span> N Parameters</a>
<a href="model-choice.html"><span class="toc-section-number">10</span> Model Choice</a>
<a href="continuous-response.html"><span class="toc-section-number">11</span> Continuous Response</a>
<a href="discrete-response.html"><span class="toc-section-number">12</span> Discrete Response</a>
<a href="appendices.html">Appendices</a>
<a href="tools.html">Tools</a>
<a href="shiny.html">Shiny</a>
<a href="maps.html">Maps</a>
<a href="animation.html">Animation</a>
<a href="references.html">References</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="two-parameters" class="section level1">
<h1>
<span class="header-section-number">Chapter 7</span> Two Parameters</h1>
<p>What is the average height of an American male? What is the 90th percentile of the distribution of height for American men? How certain are you are your estimates? If we pass 5 men walking down the street, what are the odds that the tallest will be at least 5 centimeters taller than the shortest?</p>
<div id="resampling-tactile" class="section level2">
<h2>
<span class="header-section-number">7.1</span> Pennies example</h2>
<p>In Chapter <a href="one-parameter.html#one-parameter">6</a>, we studied sampling. We started with a “tactile” exercise where we wanted to know the proportion of balls in the urn that are red. While we could have performed an exhaustive count, this would have been a tedious process. So instead, we used a shovel to extract a sample of 50 balls and used the resulting proportion that were red as an <em>estimate</em>. Furthermore, we made sure to mix the urn’s contents before every use of the shovel. Because of the randomness created by the mixing, different uses of the shovel yielded different proportions red and hence different estimates of the proportion of the urn’s balls that are red.</p>
<p>Remember: There is a <em>truth</em> here. There is an urn. It has red and white balls in it. An exact, but unknown, number of the balls are red. An exact, but unknown, number of the balls are white. An exact, but unknown, percentage of the balls are red – defined as the number red divided by the sum of the number red and the number white. Our goal was to estimate that unknown percentage. We wanted to make statements about the world, even if we can never be certain that those statements are <em>true</em>. We will never have the time or inclination to actually count all the balls. We use the term <em>parameter</em> for things that exist but which are unknown. We use statistics to estimate the true values of parameters.</p>
<p>We then mimicked this <em>physical</em> sampling exercise with an equivalent <em>virtual</em> sampling exercise using the computer. In Subsection <a href="one-parameter.html#different-shovels">6.2.4</a>, we repeated this sampling procedure 1,000 times, using three different virtual shovels with 25, 50, and 100 slots. We visualized these three sets of 1,000 estimates in Chapter <a href="one-parameter.html#one-parameter">6</a> and saw that as the sample size increased, the variation in the estimates decreased. We then expanded this for all sample sizes from 1 to 100.</p>
<p>In doing so, we constructed <em>sampling distributions</em>. The motivation for taking a 1,000 repeated samples and visualizing the resulting estimates was to study how these estimates varied from one sample to another; in other words, we wanted to study the effect of <em>sampling variation</em>. We quantified the variation of these estimates using their standard deviation, which has a special name: the <em>standard error</em>. In particular, we saw that as the sample size increased from 1 to 100, the standard error decreased and thus the sampling distributions narrowed. Larger sample sizes led to more <em>precise</em> estimates that varied less around the center.</p>
<!-- Distinguish further the difference between standard deviation and standard error in the above paragraph. Readers can confuse the two. -->
<p>We then tied these sampling exercises to terminology and mathematical notation related to sampling in Subsection <a href="one-parameter.html#terminology-and-notation">6.3.1</a>. Our <em>study population</em> was the large urn with <span class="math inline">\(N\)</span> = 2,400 balls, while the <em>population parameter</em>, the unknown quantity of interest, was the population proportion <span class="math inline">\(p\)</span> of the urn’s balls that were red. Since performing a <em>census</em> would be expensive in terms of time and energy, we instead extracted a <em>sample</em> of size <span class="math inline">\(n\)</span> = 50. The <em>point estimate</em>, also known as a <em>sample statistic</em>, used to estimate <span class="math inline">\(p\)</span> was the sample proportion <span class="math inline">\(\hat{p}\)</span> of these 50 sampled balls that were red. Furthermore, since the sample was obtained at <em>random</em>, it can be considered as <em>unbiased</em> and as <em>representative</em> of the population. Thus any results based on the sample could be <em>generalized</em> to the population. Therefore, the proportion of the shovel’s balls that were red was a “good guess” of the proportion of the urn’s balls that are red. In other words, we used the sample to draw <em>inferences</em> about the population.</p>
<p>However, as described in Section <a href="one-parameter.html#sampling-simulation">6.2</a>, both the physical and virtual sampling exercises are not what one would do in real life. This was merely an activity used to study the effects of sampling variation. In a real life situation, we would not take 1,000 samples of size <span class="math inline">\(n\)</span>, but rather take a <em>single</em> representative sample that’s as large as possible. Additionally, we knew that the true proportion of the urn’s balls that were red was 37.5%. In a real-life situation, we will not know what this value is. Because if we did, then why would we take a sample to estimate it?</p>
<p>An example of a realistic sampling situation would be a poll, like the <a href="https://www.npr.org/sections/itsallpolitics/2013/12/04/248793753/poll-support-for-obama-among-young-americans-eroding">Obama poll</a> you saw in Section <a href="one-parameter.html#sampling-case-study">6.4</a>. Pollsters did not know the true proportion of <em>all</em> young Americans who supported President Obama in 2013, and thus they took a single sample of size <span class="math inline">\(n\)</span> = 2,089 young Americans to estimate this value.</p>
<p>So how does one quantify the effects of sampling variation when you only have a <em>single sample</em> to work with? You cannot directly study the effects of sampling variation when you only have one sample. One common method to study this is <em>bootstrap resampling</em>, or simply <em>bootstrapping</em>.</p>
<p>What if we want, not only a single estimate of the unknown population parameter, but also a <em>range of highly plausible</em> values? Going back to the Obama poll article, it stated that the pollsters’ estimate of the proportion of all young Americans who supported President Obama was 41%. But in addition it stated that the poll’s “margin of error was plus or minus 2.1 percentage points.” This “plausible range” was [41% - 2.1%, 41% + 2.1%] = [38.9%, 43.1%]. This range of plausible values is what’s known as a <em>confidence interval</em>, which will be the focus of the later sections of this chapter. In Bayesian terms, we want the posterior distribution of the unknown parameter <span class="math inline">\(p\)</span>, the proportion of young Americans who supported Obama.</p>
<div id="to-the-bank" class="section level3">
<h3>
<span class="header-section-number">7.1.1</span> To the Bank</h3>
<p>As we did in Chapter <a href="one-parameter.html#one-parameter">6</a>, we’ll begin with a hands-on tactile activity. We almost always need the <strong>tidyverse</strong> package.</p>
<div class="sourceCode" id="cb845"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb845-1"><a href="two-parameters.html#cb845-1"></a><span class="kw">library</span>(PPBDS.data)</span>
<span id="cb845-2"><a href="two-parameters.html#cb845-2"></a><span class="kw">library</span>(rsample)</span>
<span id="cb845-3"><a href="two-parameters.html#cb845-3"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb845-4"><a href="two-parameters.html#cb845-4"></a><span class="kw">library</span>(skimr)</span>
<span id="cb845-5"><a href="two-parameters.html#cb845-5"></a><span class="kw">library</span>(rstanarm)</span></code></pre></div>
<p><strong>PPBDS.data</strong> includes the data sets for this book. <strong>rsample</strong> includes functions for bootstrapping. <strong>rstanarm</strong> makes it easy to create and display Bayesian models.</p>
</div>
<div id="what-was-the-average-year-of-us-pennies-in-2019" class="section level3">
<h3>
<span class="header-section-number">7.1.2</span> What was the average year of US pennies in 2019?</h3>
<p>Try to imagine all the pennies being used in the United States in 2019. That’s a lot of pennies! Now say we’re interested in the average year of minting of <em>all</em> these pennies. One way to compute this value would be to gather up all pennies being used in the US, record the year, and compute the average. However, this would be near impossible! So instead, let’s collect a <em>sample</em> of 50 pennies from a local bank in downtown Northampton, Massachusetts, USA as seen in the photo below</p>
<div class="figure">
<span id="fig:unnamed-chunk-650-1"></span>
<p class="caption marginnote shownote">
FIGURE 7.1: Collecting a sample of 50 US pennies from a local bank.
</p>
<img src="07-two-parameters/images/bank.jpg" alt="Collecting a sample of 50 US pennies from a local bank." width="2016">
</div>
<div class="figure">
<span id="fig:unnamed-chunk-650-2"></span>
<p class="caption marginnote shownote">
FIGURE 7.2: Collecting a sample of 50 US pennies from a local bank.
</p>
<img src="07-two-parameters/images/roll.jpg" alt="Collecting a sample of 50 US pennies from a local bank." width="2016">
</div>
<p>An image of these 50 pennies can be seen in below. For each of the 50 pennies starting in the top left, progressing row-by-row, and ending in the bottom right, note there is an “ID” identification variable printed in black and the year of minting printed in white.</p>
<div class="figure">
<span id="fig:unnamed-chunk-651"></span>
<p class="caption marginnote shownote">
FIGURE 7.3: 50 US pennies labelled.
</p>
<img src="07-two-parameters/images/3.jpg" alt="50 US pennies labelled." width="804">
</div>
<p>Run the <code>pennies_sample</code> code below to create our 50 sampled pennies.</p>
<div class="sourceCode" id="cb846"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb846-1"><a href="two-parameters.html#cb846-1"></a>pennies_sample &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">ID =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">50</span>), </span>
<span id="cb846-2"><a href="two-parameters.html#cb846-2"></a>                         <span class="dt">year =</span> <span class="kw">c</span>(<span class="dv">2002</span>, <span class="dv">1986</span>, <span class="dv">2017</span>, <span class="dv">1988</span>, <span class="dv">2008</span>, <span class="dv">1983</span>, <span class="dv">2008</span>, </span>
<span id="cb846-3"><a href="two-parameters.html#cb846-3"></a>                                  <span class="dv">1996</span>, <span class="dv">2004</span>, <span class="dv">2000</span>, <span class="dv">1994</span>, <span class="dv">1995</span>, <span class="dv">2015</span>, <span class="dv">1978</span>, </span>
<span id="cb846-4"><a href="two-parameters.html#cb846-4"></a>                                  <span class="dv">1974</span>, <span class="dv">2015</span>, <span class="dv">2016</span>, <span class="dv">1996</span>, <span class="dv">1983</span>, <span class="dv">1971</span>, <span class="dv">1981</span>, </span>
<span id="cb846-5"><a href="two-parameters.html#cb846-5"></a>                                  <span class="dv">1976</span>, <span class="dv">1998</span>, <span class="dv">2017</span>, <span class="dv">1979</span>, <span class="dv">1979</span>, <span class="dv">1993</span>, <span class="dv">2006</span>, </span>
<span id="cb846-6"><a href="two-parameters.html#cb846-6"></a>                                  <span class="dv">1988</span>, <span class="dv">1978</span>, <span class="dv">2013</span>, <span class="dv">1976</span>, <span class="dv">1979</span>, <span class="dv">1985</span>, <span class="dv">1985</span>, </span>
<span id="cb846-7"><a href="two-parameters.html#cb846-7"></a>                                  <span class="dv">2015</span>, <span class="dv">1962</span>, <span class="dv">1999</span>, <span class="dv">2015</span>, <span class="dv">1990</span>, <span class="dv">1992</span>, <span class="dv">1997</span>, </span>
<span id="cb846-8"><a href="two-parameters.html#cb846-8"></a>                                  <span class="dv">2018</span>, <span class="dv">2015</span>, <span class="dv">1997</span>, <span class="dv">2017</span>, <span class="dv">1982</span>, <span class="dv">1988</span>, <span class="dv">2006</span>, </span>
<span id="cb846-9"><a href="two-parameters.html#cb846-9"></a>                                  <span class="dv">2017</span>))</span>
<span id="cb846-10"><a href="two-parameters.html#cb846-10"></a>pennies_sample</span></code></pre></div>
<pre><code>## # A tibble: 50 x 2
##       ID  year
##    &lt;int&gt; &lt;dbl&gt;
##  1     1  2002
##  2     2  1986
##  3     3  2017
##  4     4  1988
##  5     5  2008
##  6     6  1983
##  7     7  2008
##  8     8  1996
##  9     9  2004
## 10    10  2000
## # … with 40 more rows</code></pre>
<p>The <code>pennies_sample</code> data frame has 50 rows corresponding to each penny with two variables. The first variable <code>ID</code> corresponds to the ID labels in our table above, whereas the second variable <code>year</code> corresponds to the year of minting saved as a numeric variable, also known as a double (<code>dbl</code>).</p>
<!-- DK: Shouldn't the year be an int? Should it be ID or id? Need to standardize this across the book. -->
<p>Based on these 50 sampled pennies, what can we say about <em>all</em> US pennies in 2019? Let’s study some properties of our sample by performing an exploratory data analysis. Let’s first visualize the distribution of the year of these 50 pennies using our data visualization tools from before. Since <code>year</code> is a numerical variable, we use a histogram to visualize its distribution.</p>
<div class="sourceCode" id="cb848"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb848-1"><a href="two-parameters.html#cb848-1"></a>pennies_sample <span class="op">%&gt;%</span></span>
<span id="cb848-2"><a href="two-parameters.html#cb848-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year)) <span class="op">+</span></span>
<span id="cb848-3"><a href="two-parameters.html#cb848-3"></a><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">10</span>, <span class="dt">color =</span> <span class="st">"white"</span>)</span></code></pre></div>
<div class="figure">
<span id="fig:unnamed-chunk-653"></span>
<p class="caption marginnote shownote">
FIGURE 7.4: Distribution of year on 50 US pennies.
</p>
<img src="book_temp_files/figure-html/unnamed-chunk-653-1.png" alt="Distribution of year on 50 US pennies." width="672">
</div>
<p>Observe a slightly left-skewed distribution, since most pennies fall between 1980 and 2010 with only a few pennies older than 1970. What is the average year for the 50 sampled pennies? Eyeballing the histogram it appears to be around 1990. Let’s now compute this value exactly using our data wrangling tools from Chapter <a href="wrangling.html#wrangling">2</a>.</p>
<!-- DK: In the book, this shows up as "1994." Why is that? This happens in other chapters. Need to figure out decimal display issues. -->
<div class="sourceCode" id="cb849"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb849-1"><a href="two-parameters.html#cb849-1"></a>pennies_sample <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb849-2"><a href="two-parameters.html#cb849-2"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">mean_year =</span> <span class="kw">mean</span>(year))</span></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   mean_year
##       &lt;dbl&gt;
## 1     1995.</code></pre>
<p>Thus, if we’re willing to assume that <code>pennies_sample</code> is a representative sample from <em>all</em> US pennies, a “good guess” of the average year of minting of all US pennies would be 1995.44. In other words, around 1995. This should all start sounding similar to what we did previously in Chapter <a href="one-parameter.html#one-parameter">6</a>!</p>
<p>In Chapter <a href="one-parameter.html#one-parameter">6</a>, our <em>study population</em> was the urn of <span class="math inline">\(N = 2400\)</span> balls. Our <em>population parameter</em> was the <em>population proportion</em> of these balls that were red, denoted by <span class="math inline">\(p\)</span>. In order to estimate <span class="math inline">\(p\)</span>, we extracted a sample of 50 balls using the shovel. We then computed the relevant <em>point estimate</em>: the <em>sample proportion</em> of these 50 balls that were red, denoted mathematically by <span class="math inline">\(\hat{p}\)</span>. We also calculated a posterior probability distribution for <span class="math inline">\(p\)</span>.</p>
<p>Here our population is <span class="math inline">\(N\)</span> – whatever the number of pennies are being used in the US, a value which we don’t know and probably never will. The population parameter of interest is now the <em>population mean</em> year of all these pennies, a value denoted mathematically by the Greek letter <span class="math inline">\(\mu\)</span>, pronounced “mu”. In order to estimate <span class="math inline">\(\mu\)</span>, we went to the bank and obtained a sample of 50 pennies and computed the relevant point estimate: the <em>sample mean</em> year of these 50 pennies, denoted mathematically by <span class="math inline">\(\overline{x}\)</span> (pronounced “x-bar”).</p>
<p>Going back to our 50 sampled pennies, the point estimate of interest is the sample mean <span class="math inline">\(\overline{x}\)</span> of 1995.44. This quantity is an <em>estimate</em> of the population mean year of <em>all</em> US pennies <span class="math inline">\(\mu\)</span>.</p>
<p>Recall that we also saw in Chapter <a href="one-parameter.html#one-parameter">6</a> that such estimates are prone to <em>sampling variation</em>. For example, in this particular sample, we observed three pennies with the year 1999. If we sampled another 50 pennies, would we observe exactly three pennies with the year 1999 again? More than likely not. We might observe none, one, two, or maybe even all 50! The same can be said for the other 26 unique years that are represented in our sample of 50 pennies.</p>
<p>So what do we do about this sampling variation? One solution is that we create bootstrap samples! Bootstrapping repeatedly draws independent samples from our data set with replacement. By sampling with replacement, the same observation can be sampled multiple times and each bootstrap sample will have the same number of observations as the original data set.</p>
<p><label for="tufte-mn-79" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-79" class="margin-toggle"><span class="marginnote">To conduct bootstraps, make sure you install both the rsample and tidyverse libraries</span></p>
<p>The intuition with bootstrapping is that we can model an inference about the population from resampling our sample data and then performing an inference about a sample from each resample. It will look something like this: resampled → sample → population.</p>
<p>The first thing we want to do when bootstrapping is to create our bootstrap samples. Since we are concerned with the <code>year</code> of pennies in 2019, let’s select <code>year</code> in our data set before we create our bootstraps. Let’s now perform the virtual analog for 1,000 resamples. Using these results, we’ll be able to study the variability in the sample means from 1,000 resamples of size 50. Let’s first add a <code>times = 1000</code> argument to <code>bootstraps()</code> to indicate we would like 1,000 replicates. Remember that we must use the <code>rsample</code> library to use bootstraps.</p>
<div class="sourceCode" id="cb851"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb851-1"><a href="two-parameters.html#cb851-1"></a><span class="kw">set.seed</span>(<span class="dv">9</span>)</span>
<span id="cb851-2"><a href="two-parameters.html#cb851-2"></a>virtual_resamples &lt;-<span class="st"> </span>pennies_sample <span class="op">%&gt;%</span></span>
<span id="cb851-3"><a href="two-parameters.html#cb851-3"></a><span class="st">  </span><span class="kw">select</span>(year) <span class="op">%&gt;%</span></span>
<span id="cb851-4"><a href="two-parameters.html#cb851-4"></a><span class="st">  </span><span class="kw">bootstraps</span>(<span class="dt">times =</span> <span class="dv">1000</span>)</span>
<span id="cb851-5"><a href="two-parameters.html#cb851-5"></a>virtual_resamples</span></code></pre></div>
<pre><code>## # Bootstrap sampling 
## # A tibble: 1,000 x 2
##    splits          id           
##    &lt;list&gt;          &lt;chr&gt;        
##  1 &lt;split [50/19]&gt; Bootstrap0001
##  2 &lt;split [50/20]&gt; Bootstrap0002
##  3 &lt;split [50/16]&gt; Bootstrap0003
##  4 &lt;split [50/18]&gt; Bootstrap0004
##  5 &lt;split [50/18]&gt; Bootstrap0005
##  6 &lt;split [50/18]&gt; Bootstrap0006
##  7 &lt;split [50/17]&gt; Bootstrap0007
##  8 &lt;split [50/14]&gt; Bootstrap0008
##  9 &lt;split [50/19]&gt; Bootstrap0009
## 10 &lt;split [50/20]&gt; Bootstrap0010
## # … with 990 more rows</code></pre>
<p>Our bootstrap samples are stored in a tibble-like object, with each bootstrap sample nested in the <code>splits</code> column. Each row is a different bootstrap sample and the <code>id</code> column is used to identify each bootstrap sample.</p>
<!-- DK: Can we use `id` with the backticks in a margin note? -->
<p><label for="tufte-mn-80" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-80" class="margin-toggle"><span class="marginnote">It is annoying that bootstraps() requires the use of an id column even though we have used ID in the past to identify specific observations. Here, instead, it is used to mean a specific bootstrapped tibble.</span></p>
<p>To view a specific bootstrap sample, use the <code>analysis()</code> function from the <code>rsample</code> package, which basically allows you to view a specific bootstrap sample as a data frame. Consider the first bootstrap sample:</p>
<div class="sourceCode" id="cb853"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb853-1"><a href="two-parameters.html#cb853-1"></a><span class="kw">analysis</span>(virtual_resamples<span class="op">$</span>splits[[<span class="dv">1</span>]]) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb853-2"><a href="two-parameters.html#cb853-2"></a><span class="st">  </span><span class="kw">as_tibble</span>()</span></code></pre></div>
<pre><code>## # A tibble: 50 x 1
##     year
##    &lt;dbl&gt;
##  1  1983
##  2  2017
##  3  1983
##  4  2017
##  5  1995
##  6  1988
##  7  1978
##  8  2015
##  9  1962
## 10  1996
## # … with 40 more rows</code></pre>
<p>Replace <code>1</code> with some other number to see a later bootstrap sample.</p>
<p>Notice that it has 50 rows, which is the same as the number of rows in our <code>pennies_sample</code>. Now that we know how to create bootstrap samples and view them, we can apply more code to our bootstraps to find our desired statistic, which is the average year of pennies in 2019.</p>
<p><label for="tufte-mn-81" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-81" class="margin-toggle"><span class="marginnote">In this chapter, bootstrap samples and resamples mean the same thing</span></p>
<p>To compute our desired statistics, we now create the column <code>boot</code>.</p>
<div class="sourceCode" id="cb855"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb855-1"><a href="two-parameters.html#cb855-1"></a>virtual_resamples &lt;-<span class="st"> </span>pennies_sample <span class="op">%&gt;%</span></span>
<span id="cb855-2"><a href="two-parameters.html#cb855-2"></a><span class="st">  </span><span class="kw">select</span>(year) <span class="op">%&gt;%</span></span>
<span id="cb855-3"><a href="two-parameters.html#cb855-3"></a><span class="st">  </span><span class="kw">bootstraps</span>(<span class="dt">times =</span> <span class="dv">1000</span>) <span class="op">%&gt;%</span></span>
<span id="cb855-4"><a href="two-parameters.html#cb855-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">boot =</span> <span class="kw">map</span>(splits, <span class="op">~</span><span class="st"> </span><span class="kw">analysis</span>(.)))</span>
<span id="cb855-5"><a href="two-parameters.html#cb855-5"></a>virtual_resamples</span></code></pre></div>
<pre><code>## # Bootstrap sampling 
## # A tibble: 1,000 x 3
##    splits          id            boot             
##    &lt;list&gt;          &lt;chr&gt;         &lt;list&gt;           
##  1 &lt;split [50/19]&gt; Bootstrap0001 &lt;tibble [50 × 1]&gt;
##  2 &lt;split [50/18]&gt; Bootstrap0002 &lt;tibble [50 × 1]&gt;
##  3 &lt;split [50/20]&gt; Bootstrap0003 &lt;tibble [50 × 1]&gt;
##  4 &lt;split [50/18]&gt; Bootstrap0004 &lt;tibble [50 × 1]&gt;
##  5 &lt;split [50/19]&gt; Bootstrap0005 &lt;tibble [50 × 1]&gt;
##  6 &lt;split [50/15]&gt; Bootstrap0006 &lt;tibble [50 × 1]&gt;
##  7 &lt;split [50/13]&gt; Bootstrap0007 &lt;tibble [50 × 1]&gt;
##  8 &lt;split [50/19]&gt; Bootstrap0008 &lt;tibble [50 × 1]&gt;
##  9 &lt;split [50/20]&gt; Bootstrap0009 &lt;tibble [50 × 1]&gt;
## 10 &lt;split [50/19]&gt; Bootstrap0010 &lt;tibble [50 × 1]&gt;
## # … with 990 more rows</code></pre>
<p>We are iterating over each bootstrap sample, applying <code>analysis()</code> to each row. <code>boot</code> is now a list-column in the tibble, which we can use if we want to find a specific characteristic of each sample like the average year. Given that <code>boot</code> is a list column and we want to pull out the mean year as we are interested in this, we can create two more columns:</p>
<div class="sourceCode" id="cb857"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb857-1"><a href="two-parameters.html#cb857-1"></a><span class="kw">set.seed</span>(<span class="dv">9</span>)</span>
<span id="cb857-2"><a href="two-parameters.html#cb857-2"></a>virtual_resamples &lt;-<span class="st"> </span>pennies_sample <span class="op">%&gt;%</span></span>
<span id="cb857-3"><a href="two-parameters.html#cb857-3"></a><span class="st">  </span><span class="kw">select</span>(year) <span class="op">%&gt;%</span></span>
<span id="cb857-4"><a href="two-parameters.html#cb857-4"></a><span class="st">  </span><span class="kw">bootstraps</span>(<span class="dt">times =</span> <span class="dv">1000</span>) <span class="op">%&gt;%</span></span>
<span id="cb857-5"><a href="two-parameters.html#cb857-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">boot =</span> <span class="kw">map</span>(splits, <span class="op">~</span><span class="st"> </span><span class="kw">analysis</span>(.))) <span class="op">%&gt;%</span></span>
<span id="cb857-6"><a href="two-parameters.html#cb857-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">years =</span> <span class="kw">map</span>(boot, <span class="op">~</span><span class="st"> </span><span class="kw">pull</span>(., year))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb857-7"><a href="two-parameters.html#cb857-7"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">year_mean =</span> <span class="kw">map_dbl</span>(years, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>(.)))</span>
<span id="cb857-8"><a href="two-parameters.html#cb857-8"></a>virtual_resamples</span></code></pre></div>
<pre><code>## # Bootstrap sampling 
## # A tibble: 1,000 x 5
##    splits          id            boot              years      year_mean
##    &lt;list&gt;          &lt;chr&gt;         &lt;list&gt;            &lt;list&gt;         &lt;dbl&gt;
##  1 &lt;split [50/19]&gt; Bootstrap0001 &lt;tibble [50 × 1]&gt; &lt;dbl [50]&gt;     1992.
##  2 &lt;split [50/20]&gt; Bootstrap0002 &lt;tibble [50 × 1]&gt; &lt;dbl [50]&gt;     1999.
##  3 &lt;split [50/16]&gt; Bootstrap0003 &lt;tibble [50 × 1]&gt; &lt;dbl [50]&gt;     1992.
##  4 &lt;split [50/18]&gt; Bootstrap0004 &lt;tibble [50 × 1]&gt; &lt;dbl [50]&gt;     1993.
##  5 &lt;split [50/18]&gt; Bootstrap0005 &lt;tibble [50 × 1]&gt; &lt;dbl [50]&gt;     1995.
##  6 &lt;split [50/18]&gt; Bootstrap0006 &lt;tibble [50 × 1]&gt; &lt;dbl [50]&gt;     1998.
##  7 &lt;split [50/17]&gt; Bootstrap0007 &lt;tibble [50 × 1]&gt; &lt;dbl [50]&gt;     1993.
##  8 &lt;split [50/14]&gt; Bootstrap0008 &lt;tibble [50 × 1]&gt; &lt;dbl [50]&gt;     1995.
##  9 &lt;split [50/19]&gt; Bootstrap0009 &lt;tibble [50 × 1]&gt; &lt;dbl [50]&gt;     1993.
## 10 &lt;split [50/20]&gt; Bootstrap0010 &lt;tibble [50 × 1]&gt; &lt;dbl [50]&gt;     2000.
## # … with 990 more rows</code></pre>
<p>Voila! We were able to create a thousand bootstrap samples and calculate the mean year for each resample. Let’s now create a plot to visualizes the posterior distribution for the mean year of American pennies in 2019.</p>
<!-- DK: Need to explain the .. syntax or replace it with the new stat_density trick. -->
<div class="sourceCode" id="cb859"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb859-1"><a href="two-parameters.html#cb859-1"></a>virtual_resamples <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb859-2"><a href="two-parameters.html#cb859-2"></a><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></span>
<span id="cb859-3"><a href="two-parameters.html#cb859-3"></a><span class="st">    </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year_mean, </span>
<span id="cb859-4"><a href="two-parameters.html#cb859-4"></a>                       <span class="dt">y =</span> <span class="kw">after_stat</span>(count<span class="op">/</span><span class="kw">sum</span>(count))), </span>
<span id="cb859-5"><a href="two-parameters.html#cb859-5"></a>                       <span class="dt">binwidth =</span> <span class="fl">.5</span>) <span class="op">+</span></span>
<span id="cb859-6"><a href="two-parameters.html#cb859-6"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Mean Year"</span>, </span>
<span id="cb859-7"><a href="two-parameters.html#cb859-7"></a>         <span class="dt">y =</span> <span class="st">"Probability"</span>,</span>
<span id="cb859-8"><a href="two-parameters.html#cb859-8"></a>         <span class="dt">title =</span> <span class="st">"Posterior Distribution for the Mean Year of American Pennies in 2019"</span>) </span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-660-1.png" width="672"></p>
<p>How did we sneak in the word “posterior” into this discussion. Recall in Chapter <a href="probability.html#probability">5</a> that we defined a posterior distribution as our beliefs about an unknown number: either a number which we don’t know now but which we will know, like Biden’s electoral vote total or a number which we can never know, like the average year for all pennies. In the case of the bootstrap samples we made, the posterior distribution represents our beliefs about <span class="math inline">\(\mu\)</span>, the mean year of American pennies in 2019 after taking into account the information from our bootstrap sample means. As demonstrated in the plot above, we can see that <span class="math inline">\(\mu\)</span> — an unknown parameter, the true value of which we will never know — is most likely between 1992 and 1998.</p>
<p>Have we proved how the bootstrap, almost magically, can create a reasonable posterior? Not at all! The mathematics of that proof are beyond the scope of this book.</p>
<!-- In the "resampling with replacement" scenario we are illustrating here, this histogram has a special name: the *bootstrap distribution of the sample mean*. Furthermore, recall it is an approximation to the *sampling distribution* of the sample mean, a concept you saw before. This distribution allows us to study the effect of sampling variation on our estimates of the true population mean, in this case the true mean year for *all* US pennies. However, unlike in Chapter \@ref(one-parameter) where we took multiple samples (something one would never do in practice), bootstrap distributions are constructed by taking multiple resamples from a *single* sample: in this case, the 50 original pennies from the bank.  -->
<!-- Congratulations! You've just constructed your first bootstrap distribution!  -->
<!-- ### Measuring uncertainty with confidence intervals {#ci-build-up} -->
<!-- Let's start this section with an analogy involving fishing. Say you are trying to catch a fish. On the one hand, you could use a spear, while on the other you could use a net. Using the net will probably allow you to catch more fish! -->
<!-- Now think back to our pennies exercise where you are trying to estimate the true population mean year $\mu$ of *all* US pennies. Think of the value of $\mu$ as a fish. -->
<!-- On the one hand, we could use the appropriate *point estimate/sample statistic* to estimate $\mu$, which we saw in the table in the previous section, is the sample mean $\overline{x}$. Based on our sample of 50 pennies from the bank, the sample mean was 1995.44. Think of using this value as "fishing with a spear." -->
<!-- What would "fishing with a net" correspond to? Look at the bootstrap distribution we created once more. Between which two years would you say that "most" sample means lie?  While this question is somewhat subjective, saying that most sample means lie between 1992 and 2000 would not be unreasonable. Think of this interval as the "net." -->
<!-- What we've just illustrated is the concept of a *confidence interval*, which we'll abbreviate with "CI" throughout this book. As opposed to a point estimate/sample statistic that estimates the value of an unknown population parameter with a single value, a *confidence interval* \index{confidence interval} gives what can be interpreted as a range of plausible values. Going back to our analogy, point estimates/sample statistics can be thought of as spears, whereas confidence intervals can be thought of as nets. -->
<!-- ```{r, echo=FALSE, fig.align='center', fig.cap="Analogy of difference between point estimates and confidence intervals."} -->
<!-- knitr::include_graphics("07-two-parameters/images/point_estimate_vs_conf_int.png") -->
<!-- ``` -->
<!-- Our proposed interval of 1992 to 2000 was constructed by eye and was thus somewhat subjective. We now introduce two methods for constructing such intervals in a more exact fashion: the *lm method* and the *quantile method*. -->
<!-- Second, they both require you to specify the *confidence level*. Commonly used confidence levels include 90%, 95%, and 99%.  All other things being equal, higher confidence levels correspond to wider confidence intervals, and lower confidence levels correspond to narrower confidence intervals. In this book, we'll be mostly using 95% and hence constructing "95% confidence intervals for $\mu$" for our pennies activity. -->
</div>
</div>
<div id="eda-for-nhanes" class="section level2">
<h2>
<span class="header-section-number">7.2</span> EDA for <code>nhanes</code>
</h2>
<p>Shifting away from dealing with pennies, let’s look at bootstrap modeling with the <code>nhanes</code> dataset from National Health and Nutrition Examination Survey conducted by the Centers for Disease Control and Prevention and covering children and adults in America. It is located in the <strong>PPBDS.data</strong> package which we loaded above.</p>
<div class="sourceCode" id="cb860"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb860-1"><a href="two-parameters.html#cb860-1"></a><span class="kw">glimpse</span>(nhanes)</span></code></pre></div>
<pre><code>## Rows: 10,000
## Columns: 15
## $ survey         &lt;int&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, …
## $ gender         &lt;chr&gt; "Male", "Male", "Male", "Male", "Female", "Male", "Mal…
## $ age            &lt;int&gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 5…
## $ race           &lt;chr&gt; "White", "White", "White", "Other", "White", "White", …
## $ education      &lt;ord&gt; High School, High School, High School, NA, Some Colleg…
## $ hh_income      &lt;ord&gt; 25000-34999, 25000-34999, 25000-34999, 20000-24999, 35…
## $ weight         &lt;dbl&gt; 87, 87, 87, 17, 87, 30, 35, 76, 76, 76, 68, 78, 75, 39…
## $ height         &lt;dbl&gt; 165, 165, 165, 105, 168, 133, 131, 167, 167, 167, 170,…
## $ bmi            &lt;dbl&gt; 32, 32, 32, 15, 31, 17, 21, 27, 27, 27, 24, 24, 26, 19…
## $ pulse          &lt;int&gt; 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, 80…
## $ diabetes       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ general_health &lt;int&gt; 3, 3, 3, NA, 3, NA, NA, 4, 4, 4, 4, 4, 2, NA, NA, 3, N…
## $ depressed      &lt;ord&gt; Several, Several, Several, NA, Several, NA, NA, None, …
## $ pregnancies    &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, NA…
## $ sleep          &lt;int&gt; 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, NA…</code></pre>
<p><code>nhanes</code> has data on a diverse array of things like physical attributes, education, and sleep. Let’s restrict our attention to a subset, focusing on gender, height and the year of the survey.</p>
<div class="sourceCode" id="cb862"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb862-1"><a href="two-parameters.html#cb862-1"></a>ch7 &lt;-<span class="st"> </span>nhanes <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb862-2"><a href="two-parameters.html#cb862-2"></a><span class="st">  </span><span class="kw">select</span>(age, gender, height, survey)</span></code></pre></div>
<p>Look at a random sample of our data:</p>
<div class="sourceCode" id="cb863"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb863-1"><a href="two-parameters.html#cb863-1"></a>ch7 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb863-2"><a href="two-parameters.html#cb863-2"></a><span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">5</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 x 4
##     age gender height survey
##   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;int&gt;
## 1    80 Female   157.   2009
## 2     0 Female    NA    2009
## 3    19 Female   161.   2011
## 4    39 Male     175    2009
## 5    24 Female   159.   2009</code></pre>
<p>Notice how there is a decimal in the <code>height</code> column of <code>ch7</code>. This is because <code>height</code> is a <code>&lt;dbl&gt;</code> and not an <code>&lt;int&gt;</code>.</p>
<p>Let’s also run <code>glimpse()</code> on our new data.</p>
<div class="sourceCode" id="cb865"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb865-1"><a href="two-parameters.html#cb865-1"></a>ch7 <span class="op">%&gt;%</span></span>
<span id="cb865-2"><a href="two-parameters.html#cb865-2"></a><span class="st">  </span><span class="kw">glimpse</span>()</span></code></pre></div>
<pre><code>## Rows: 10,000
## Columns: 4
## $ age    &lt;int&gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58, 50, 9…
## $ gender &lt;chr&gt; "Male", "Male", "Male", "Male", "Female", "Male", "Male", "Fem…
## $ height &lt;dbl&gt; 165, 165, 165, 105, 168, 133, 131, 167, 167, 167, 170, 182, 16…
## $ survey &lt;int&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 20…</code></pre>
<p>Be on the lookout for anything suspicious. Are there any NA’s in your data set? What types of data are the columns, i.e. why is <code>survey</code> characterized as integer instead of double? Was most of the data collected in 2009? Are there more females than males? You can never look at your data too closely.</p>
<p>In addition to <code>glimpse()</code>, we can run <code>skim()</code>, from the <strong>skimr</strong> package, to calculate some summary statistics.</p>
<div class="sourceCode" id="cb867"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb867-1"><a href="two-parameters.html#cb867-1"></a>ch7 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb867-2"><a href="two-parameters.html#cb867-2"></a><span class="st">  </span><span class="kw">skim</span>()</span></code></pre></div>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:unnamed-chunk-665">TABLE 7.1: </span>Data summary</span><!--</caption>--></p>
<table><tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Piped data</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">10000</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">4</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">3</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody></table>
<p><strong>Variable type: character</strong></p>
<table>
<thead><tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr></thead>
<tbody><tr class="odd">
<td align="left">gender</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">6</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
</tr></tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead><tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">age</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">37</td>
<td align="right">22</td>
<td align="right">0</td>
<td align="right">17</td>
<td align="right">36</td>
<td align="right">54</td>
<td align="right">80</td>
<td align="left">▇▇▇▆▅</td>
</tr>
<tr class="even">
<td align="left">height</td>
<td align="right">353</td>
<td align="right">0.96</td>
<td align="right">162</td>
<td align="right">20</td>
<td align="right">84</td>
<td align="right">157</td>
<td align="right">166</td>
<td align="right">174</td>
<td align="right">200</td>
<td align="left">▁▁▁▇▂</td>
</tr>
<tr class="odd">
<td align="left">survey</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2010</td>
<td align="right">1</td>
<td align="right">2009</td>
<td align="right">2009</td>
<td align="right">2010</td>
<td align="right">2011</td>
<td align="right">2011</td>
<td align="left">▇▁▁▁▇</td>
</tr>
</tbody>
</table>
<p>Interesting! There are 353 missing values of height in our subset of data. Just using <code>glimpse()</code> does not show us that. Let’s filter out the NA’s using <code>drop_na</code>. This we will delete the rows in which the value of any variable is missing. For simplicity, let’s only consider adults.</p>
<div class="sourceCode" id="cb868"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb868-1"><a href="two-parameters.html#cb868-1"></a>ch7 &lt;-<span class="st"> </span>nhanes <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb868-2"><a href="two-parameters.html#cb868-2"></a><span class="st">  </span><span class="kw">select</span>(age, gender, height, survey) <span class="op">%&gt;%</span></span>
<span id="cb868-3"><a href="two-parameters.html#cb868-3"></a><span class="st">  </span><span class="kw">filter</span>(age <span class="op">&gt;=</span><span class="st"> </span><span class="dv">18</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb868-4"><a href="two-parameters.html#cb868-4"></a><span class="st">  </span><span class="kw">drop_na</span>()</span></code></pre></div>
<p>Plot your data.</p>
<div class="sourceCode" id="cb869"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb869-1"><a href="two-parameters.html#cb869-1"></a>ch7 <span class="op">%&gt;%</span></span>
<span id="cb869-2"><a href="two-parameters.html#cb869-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> height, <span class="dt">color =</span> gender)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb869-3"><a href="two-parameters.html#cb869-3"></a><span class="st">  </span><span class="kw">geom_density</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb869-4"><a href="two-parameters.html#cb869-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Height"</span>,</span>
<span id="cb869-5"><a href="two-parameters.html#cb869-5"></a>       <span class="dt">title =</span> <span class="st">"Height by Gender in NHANES Dataset"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-667-1.png" width="672"></p>
<p>We can see the the most probable heights for both genders and that men are generally taller than women.</p>
<!-- ## Using Bootstraps with `nhanes` -->
<!-- 4. Prove that the bootstrap works, that our 95% confidence intervals provide correct coverage. We make a game and I give you a sample of like 40. Here's the 40, and you give me a 95% CI using the bootstrap tools we learned. Then I give you another 40. And another. If we do this 1,00 times, and we use the same procedure for calculating a confidence interval each time, then 950 should include the truth. That's how we know bootstraps is correct and I could only demonstrate this to you if we know what the truth is.   -->
<!-- Plan: create function called create_ci(), which takes a tibble with a single variable called height and returns the 95% confidence interval --- i.e., a numeric vector of length 2 --- for the 75th percentile.  Note that you get to hard code everything. -->
<!-- Then, create a tibble, first column is ID. Second column is height_sample, which is created by running sample(ch7$height, size = 40, replace = FALSE). Third column is ci, which is result mutate(ci = map(height_sample, ~ create_ci(.)). Fourth column is within_ci, which is TRUE if ci includes the TRUE value and FALSE otherwise. (If you want to have two columns, one for each limit, that is fine.) -->
</div>
<div id="bootstrap-to-estimate-average-height" class="section level2">
<h2>
<span class="header-section-number">7.3</span> Bootstrap to estimate average height</h2>
<p>We have shown you how to use bootstrap sampling to create a posterior distribution for an unknown parameter. Let’s use a similar approach to estimate the value of a different unknown parameter: the average height of an adult American male in 2009. Let’s also name this parameter <span class="math inline">\(\mu\)</span>. Is it confusing that we are using the same parameter? Yes! But, sadly, there are only so many Greek letters. We have no choice but to reuse them. By convention, <span class="math inline">\(\mu\)</span> is often used as the parameter name for an unknown mean. But we could have used a different letter, Greek or otherwise. And, symbols besides <span class="math inline">\(\mu\)</span> are often used for unknown means. It is up to you, but, in general, following the conventions in your field is wise.</p>
<p>First, filter the data set:</p>
<div class="sourceCode" id="cb870"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb870-1"><a href="two-parameters.html#cb870-1"></a>ch7_male &lt;-<span class="st"> </span>nhanes <span class="op">%&gt;%</span></span>
<span id="cb870-2"><a href="two-parameters.html#cb870-2"></a><span class="st">  </span><span class="kw">filter</span>(survey <span class="op">==</span><span class="st"> </span><span class="dv">2009</span>, gender <span class="op">==</span><span class="st"> "Male"</span>, age <span class="op">&gt;=</span><span class="st"> </span><span class="dv">18</span>) <span class="op">%&gt;%</span></span>
<span id="cb870-3"><a href="two-parameters.html#cb870-3"></a><span class="st">  </span><span class="kw">select</span>(height) <span class="op">%&gt;%</span></span>
<span id="cb870-4"><a href="two-parameters.html#cb870-4"></a><span class="st">  </span><span class="kw">drop_na</span>()</span></code></pre></div>
<p>Dropping missing values can be dangerous, depending on their origin and the goals of our analysis. <em>Never drop lightly.</em></p>
<p>Second, use (almost) the same code as before:</p>
<div class="sourceCode" id="cb871"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb871-1"><a href="two-parameters.html#cb871-1"></a><span class="kw">set.seed</span>(<span class="dv">9</span>)</span>
<span id="cb871-2"><a href="two-parameters.html#cb871-2"></a>virtual_resamples &lt;-<span class="st"> </span>ch7_male <span class="op">%&gt;%</span></span>
<span id="cb871-3"><a href="two-parameters.html#cb871-3"></a><span class="st">  </span><span class="kw">bootstraps</span>(<span class="dt">times =</span> <span class="dv">1000</span>) <span class="op">%&gt;%</span></span>
<span id="cb871-4"><a href="two-parameters.html#cb871-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">boot =</span> <span class="kw">map</span>(splits, <span class="op">~</span><span class="st"> </span><span class="kw">analysis</span>(.))) <span class="op">%&gt;%</span></span>
<span id="cb871-5"><a href="two-parameters.html#cb871-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">heights =</span> <span class="kw">map</span>(boot, <span class="op">~</span><span class="st"> </span><span class="kw">pull</span>(., height))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb871-6"><a href="two-parameters.html#cb871-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">height_mean =</span> <span class="kw">map_dbl</span>(heights, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>(.)))</span>
<span id="cb871-7"><a href="two-parameters.html#cb871-7"></a>virtual_resamples</span></code></pre></div>
<pre><code>## # Bootstrap sampling 
## # A tibble: 1,000 x 5
##    splits             id            boot                heights      height_mean
##    &lt;list&gt;             &lt;chr&gt;         &lt;list&gt;              &lt;list&gt;             &lt;dbl&gt;
##  1 &lt;split [1.8K/663]&gt; Bootstrap0001 &lt;tibble [1,814 × 1… &lt;dbl [1,814…        176.
##  2 &lt;split [1.8K/640]&gt; Bootstrap0002 &lt;tibble [1,814 × 1… &lt;dbl [1,814…        176.
##  3 &lt;split [1.8K/662]&gt; Bootstrap0003 &lt;tibble [1,814 × 1… &lt;dbl [1,814…        176.
##  4 &lt;split [1.8K/649]&gt; Bootstrap0004 &lt;tibble [1,814 × 1… &lt;dbl [1,814…        176.
##  5 &lt;split [1.8K/669]&gt; Bootstrap0005 &lt;tibble [1,814 × 1… &lt;dbl [1,814…        176.
##  6 &lt;split [1.8K/660]&gt; Bootstrap0006 &lt;tibble [1,814 × 1… &lt;dbl [1,814…        176.
##  7 &lt;split [1.8K/661]&gt; Bootstrap0007 &lt;tibble [1,814 × 1… &lt;dbl [1,814…        176.
##  8 &lt;split [1.8K/641]&gt; Bootstrap0008 &lt;tibble [1,814 × 1… &lt;dbl [1,814…        176.
##  9 &lt;split [1.8K/647]&gt; Bootstrap0009 &lt;tibble [1,814 × 1… &lt;dbl [1,814…        176.
## 10 &lt;split [1.8K/657]&gt; Bootstrap0010 &lt;tibble [1,814 × 1… &lt;dbl [1,814…        176.
## # … with 990 more rows</code></pre>
<p>Plot the results:</p>
<!-- DK: No decimals show up on the x-axis when this is knit, just 176, 176, 176.  -->
<div class="sourceCode" id="cb873"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb873-1"><a href="two-parameters.html#cb873-1"></a>virtual_resamples <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb873-2"><a href="two-parameters.html#cb873-2"></a><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></span>
<span id="cb873-3"><a href="two-parameters.html#cb873-3"></a><span class="st">    </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">x =</span> height_mean, </span>
<span id="cb873-4"><a href="two-parameters.html#cb873-4"></a>                       <span class="dt">y =</span> <span class="kw">after_stat</span>(count<span class="op">/</span><span class="kw">sum</span>(count))), </span>
<span id="cb873-5"><a href="two-parameters.html#cb873-5"></a>                       <span class="dt">binwidth =</span> <span class="fl">0.02</span>) <span class="op">+</span></span>
<span id="cb873-6"><a href="two-parameters.html#cb873-6"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"Mean Height"</span>, </span>
<span id="cb873-7"><a href="two-parameters.html#cb873-7"></a>         <span class="dt">y =</span> <span class="st">"Probability"</span>,</span>
<span id="cb873-8"><a href="two-parameters.html#cb873-8"></a>         <span class="dt">title =</span> <span class="st">"Posterior Distribution for the Mean Height of American Males in 2009"</span>) </span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-670-1.png" width="672"></p>
<p>The posterior distribution includes all the information we have about the unknown parameter — mean height of American males — which we have used our data to estimate. But we don’t always want the entire object. Instead, we might want to know the:</p>
<ul>
<li><p>Mean: 175.97</p></li>
<li><p>Median: 175.97</p></li>
<li><p>95% confidence interval: 175.66, 176.28</p></li>
</ul>
</div>
<div id="probability-to-bootstrap-to-bayesian-models" class="section level2">
<h2>
<span class="header-section-number">7.4</span> Probability to bootstrap to Bayesian models</h2>
<p><label for="tufte-mn-82" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-82" class="margin-toggle"><span class="marginnote"> In this sense, the bootstrap distribution represents an (approximate) nonparametric, noninformative posterior distribution for our parameter. But this bootstrap distribution is obtained painlessly — without having to formally specify a prior and without having to sample from the posterior distribution. Hence we might think of the bootstrap distribution as a “poor man’s” Bayes posterior. By perturbing the data, the bootstrap approximates the Bayesian effect of perturbing the parameters, and is typically much simpler to carry out. — Elements of Statistical Learning, 2nd edition, by Hastie et al, page 271.</span></p>
<p>Most textbooks would, at this stage, provide a more mathematical explanation of the transition we are making from Chapter <a href="probability.html#probability">5</a> to this chapter. In both Chapters <a href="probability.html#probability">5</a> and <a href="one-parameter.html#one-parameter">6</a> we dealt with a discrete set of possible models. We began with examples in which there were only two or three possible “true” states of the world. You were either infected or not infected. There were either zero, one or two white marbles in the bag. These examples grew more and more complex, both by increasing the number of models under consideration and by increasing the number of possible outcomes of the experiment. In the case of the urn, there were 2,401 possible models: either zero or one or two or . . . 2,400 red beans in the urn.</p>
<p>The transition from a discrete set of possible models to an infinite set of possible models is mathematically complex but easy on the intuition. Just wave you hands, imagine lots more models, and invoke the aesthetic appeal of smoothness. In the case of height, there are an infinite number of possible models: average height of adult American men in 2009 could be 175, 175.1, 175.14, 175.148, 175.1482, and so on. There are an infinite number of possible values since height is continuous. Yet, almost miraculously, the same intuition applies.</p>
<p>Let’s use <span class="math inline">\(\mu\)</span> as the parameter for the unknown average height of all the adult men in America in 2009. This is exactly analogous to the parameter <span class="math inline">\(p\)</span> from Chapter <a href="one-parameter.html#one-parameter">6</a>, the proportion of red beans in the urn. The only difference is that there are an infinite number of values which <span class="math inline">\(\mu\)</span> might take. We restricted <span class="math inline">\(p\)</span> to only 2,401 possible values: <span class="math inline">\(0\)</span>, <span class="math inline">\(1/2400\)</span>, <span class="math inline">\(2/2400\)</span>, …, <span class="math inline">\(2399/2400\)</span>, <span class="math inline">\(1\)</span>.</p>
<p>Although a bootstrap can create a posterior distribution, as above, there are much simpler ways to do so. The most common involves the function <code>stan_glm()</code> from the <strong>rstanarm</strong> library. Halfway through the book, we are now ready for our first full scale data science project. Let us be guided by the cardinal virtues.</p>
</div>
<div id="cardinal-virtues" class="section level2">
<h2>
<span class="header-section-number">7.5</span> Cardinal Virtues</h2>
<p>Data science is ultimately a moral act, so we will use the four <a href="https://en.wikipedia.org/wiki/Cardinal_virtues">Cardinal Virtues</a> — Wisdom, Justice, Courage and Temperance — to organize our approach. The purpose of this section is two-fold. First, we will show you that a more formal Bayesian approach results in, more or less, the same answer as the bootstrap above, but with much less code. Second, we will show how the Cardinal Virtues guide good data science.</p>
<div id="wisdom" class="section level3">
<h3>
<span class="header-section-number">7.5.1</span> Wisdom</h3>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">-->
<img src="other/images/Wisdom.jpg" alt=" " width="1280"><!--
<p class="caption marginnote">--><!--</p>--><!--</div>--></span>
</p>
<p>What decision do we face? The reason for making models is not, primarily that making models is fun, although it is! The reason is that we face a decision. We must decide between X or Y. We must choose from A, B or C. We must set D to a specific numeric value. Given that decision, we should make a model of the world to help us.</p>
<p>In any textbook, it will be tough to avoid the “toy problem” trap. The real world is complex. Any substantive decision problem includes a great deal of complexity and requires a great deal of context. We do not have the time to get into that level of detail. So, we simplify. We are going to create a model of height for adult men. We will then use that model to answer three questions:</p>
<ul>
<li><p>What is the probability that the next adult male we meet will be taller than 180 centimeters?</p></li>
<li><p>What is the probability that, among the next 4 men we meet, the tallest is at least 10 cm taller than the shortest?</p></li>
<li><p>What is our posterior probability distribution for the height of the 3rd tallest man out of the next 100 we meet?</p></li>
</ul>
<p>The first two questions have a single number, a single probability, as their answer. The third question requires a full scale posterior probability distribution.</p>
<p>But before starting that process, we need to check that the data we have — which is only for a sample of adult American men in 2009 — will allow us to answer these questions, however roughly.</p>
<p>That is where Wisdom comes in. In the social sciences, <em>there is never a perfect relationship between the data you have and the question you are trying to answer.</em> Data for American males in 2009 is not the same thing as data for American males today. Nor is it the same as the data for men in France or Mexico. Moreover, the problem hasn’t specified where on Earth we are, nor who we are near. Walking near a basketball tournament will generate different answers than walking around Times Square would.</p>
<p>Yet this data is relevant. Right? It is certainly better than nothing. That is, using not-perfect data is better than using no data at all.</p>
<p>Is not-perfect data always better? No! If your problem is estimating the median height of 5th grade girls in Toyko, we doubt that our data is at all relevant for that problem. Wisdom recognizes the danger of using non-relevant data to build a model and then mistakenly using that model in a way which will only make the situation worse. If the data won’t help, don’t use the data, don’t build a model. Better to just use your common sense and experience. Or find better data.</p>
<p>The other aspect of Wisdom is ethics. Just because we <em>can</em> make a model does not mean we <em>should</em> make that model. Models can be used for evil and, if at all possible, you should do no evil. Fortunately, it is hard to generate many ethical worries about height models. If, instead, we were modeling criminality, the ethics become much more complex . . .</p>
</div>
<div id="justice" class="section level3">
<h3>
<span class="header-section-number">7.5.2</span> Justice</h3>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">-->
<img src="other/images/Justice.jpg" alt=" " width="960"><!--
<p class="caption marginnote">--><!--</p>--><!--</div>--></span>
</p>
<p>Mathematical knowledge is the least important skill for a data scientist.</p>
<p>However, a little mathematical notation will make our modeling assumptions clear, will bring some precision to our approach. In this case:</p>
<p><span class="math display">\[ y_i =  \mu + \epsilon_i \]</span>
with <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span>. <span class="math inline">\(y_i\)</span> is the height of male <span class="math inline">\(i\)</span>. <span class="math inline">\(\mu\)</span> is the average height of all males in the population. <span class="math inline">\(\epsilon_i\)</span> is the “error term,” the difference between the height of male <span class="math inline">\(i\)</span> and the average height of all males. <span class="math inline">\(\epsilon_i\)</span> is normally distributed with a mean of 0 and a standard deviation of <span class="math inline">\(\sigma\)</span>.</p>
<p>This is the simplest model we can construct. Note:</p>
<ul>
<li>The model has two unknown parameters: <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. Before we can do anything else we need to estimate the values of these parameters. Can we ever know their exact value? No! Perfection lies only in God’s own R code. But, by using a Bayesian approach similar to what we used in Chapters <a href="probability.html#probability">5</a> and <a href="one-parameter.html#one-parameter">6</a>, we will be able to create <em>posterior probability distributions</em> for each parameter.</li>
</ul>
<!-- DK: Box quote. --><ul>
<li><p>The model is wrong, as are all models.</p></li>
<li><p>The parameter we most care about is <span class="math inline">\(\mu\)</span>. That is the parameter with a substantively meaningful interpretation. Not only is the meaning of <span class="math inline">\(\sigma\)</span> difficult to describe, we also don’t particular care about its value. Parameters like <span class="math inline">\(\sigma\)</span> in this context are <em>nuisance</em> or <em>auxiliary</em> parameters. We still have to estimate their posterior distributions, but we don’t really care what those posteriors look like.</p></li>
<li><p><span class="math inline">\(\mu\)</span> is not the average height of the men in the sample. We can calculate that directly. It is 175.97. No estimation required! Instead, <span class="math inline">\(\mu\)</span> is the average height of men in the <em>population</em>. Recall from the discussions in Chapter <a href="one-parameter.html#one-parameter">6</a> that the population is the universe of people/units/whatever about which we seek to draw conclusions. On some level, this seems simple. On a deeper level, it is very subtle. For example, if we are walking around Copenhagen, then the population we really care about, in order to answer our three questions, is the set of adult men into which we might run today. This is not the same as the population of adult men in the US in 2009. But is it close enough? Is it better than nothing? Each case is a different and the details matter.</p></li>
</ul>
<!-- DK: Should we discuss what a superpopulation is? --><p>Consider:</p>
<p><span class="math display">\[outcome = model + what\ is\ not\ in\ the\ model\]</span>
In this case, the <em>outcome</em> is the height of an individual male. This, also called the “response,” is what we are trying to understand and/or explain and/or predict. The <em>model</em> is our creation, a mixture of data and parameters, an attempt to capture the underlying structure in the world which generates the outcome.</p>
<p>What is the difference between the <em>outcome</em> and the <em>model</em>? By definition, it is <em>what is not in the model</em>, all the blooming and buzzing complexity of the real world. The model will always be incomplete in that it won’t capture everything. Whatever the model misses is thrown into the error term.</p>
<p>The Preceptor Table for this problem is almost identical to the one we saw in Chapter <a href="rubin-causal-model.html#rubin-causal-model">3</a>:</p>
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#kmckzevqzr .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#kmckzevqzr .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#kmckzevqzr .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#kmckzevqzr .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#kmckzevqzr .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#kmckzevqzr .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#kmckzevqzr .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#kmckzevqzr .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#kmckzevqzr .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#kmckzevqzr .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#kmckzevqzr .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#kmckzevqzr .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#kmckzevqzr .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#kmckzevqzr .gt_from_md > :first-child {
  margin-top: 0;
}

#kmckzevqzr .gt_from_md > :last-child {
  margin-bottom: 0;
}

#kmckzevqzr .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#kmckzevqzr .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#kmckzevqzr .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#kmckzevqzr .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#kmckzevqzr .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#kmckzevqzr .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#kmckzevqzr .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#kmckzevqzr .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#kmckzevqzr .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#kmckzevqzr .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#kmckzevqzr .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#kmckzevqzr .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#kmckzevqzr .gt_left {
  text-align: left;
}

#kmckzevqzr .gt_center {
  text-align: center;
}

#kmckzevqzr .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#kmckzevqzr .gt_font_normal {
  font-weight: normal;
}

#kmckzevqzr .gt_font_bold {
  font-weight: bold;
}

#kmckzevqzr .gt_font_italic {
  font-style: italic;
}

#kmckzevqzr .gt_super {
  font-size: 65%;
}

#kmckzevqzr .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
</style>
<div id="kmckzevqzr" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;"><table class="gt_table">
<thead class="gt_col_headings">
<tr>
<th class="gt_col_heading gt_center gt_columns_bottom_border" rowspan="2" colspan="1" style="text-align: left; vertical-align: middle;">ID</th>
      <th class="gt_center gt_columns_top_border gt_column_spanner_outer" rowspan="1" colspan="1">
        <span class="gt_column_spanner">Outcome</span>
      </th>
    </tr>
<tr>
<th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">Heights (cm)</th>
    </tr>
</thead>
<tbody class="gt_table_body">
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">1</td>
      <td class="gt_row gt_center">?</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">2</td>
      <td class="gt_row gt_center">?</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">...</td>
      <td class="gt_row gt_center">...</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">473</td>
      <td class="gt_row gt_center">172</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">474</td>
      <td class="gt_row gt_center">?</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">...</td>
      <td class="gt_row gt_center">...</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">3,258</td>
      <td class="gt_row gt_center">?</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">3,259</td>
      <td class="gt_row gt_center">162</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">...</td>
      <td class="gt_row gt_center">...</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">N</td>
      <td class="gt_row gt_center">?</td>
    </tr>
</tbody>
</table></div>
<p>Since this is not a causal model, there is only one potential outcome — which is to say, only one outcome: an individual’s height.</p>
</div>
<div id="courage" class="section level3">
<h3>
<span class="header-section-number">7.5.3</span> Courage</h3>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">-->
<img src="other/images/Courage.jpg" alt=" " width="1024"><!--
<p class="caption marginnote">--><!--</p>--><!--</div>--></span>
</p>
<p>In data science, we deal with math, words, and code, but the most important of these is code. We need Courage to create the model, to take the leap of faith that we can make our ideas real.</p>
<div id="stan_glm" class="section level4">
<h4>
<span class="header-section-number">7.5.3.1</span> stan_glm</h4>
<p>Bayesian models are not hard to create in R. Sticking to the same filtered adult male 2009 data, we can reduce all the work we did for the bootstrap approach to the <code>stan_glm()</code> function which, when fed the correct inputs, creates a Bayesian generalized linear model of height. This function comes from the <strong>rstanarm</strong> package, which is very useful for Bayesian models in general.</p>
<p>The first argument in the <code>stan_glm()</code> function is <code>data</code>, which in our case is the filtered <code>ch7_male</code> tibble we used in the bootstrap example. The only other mandatory argument is the formula that we want to build a model around. In this case, since we have no predictor variables, our equation will be <code>height ~ 1</code>.</p>
<div class="sourceCode" id="cb874"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb874-1"><a href="two-parameters.html#cb874-1"></a><span class="kw">set.seed</span>(<span class="dv">9</span>)</span>
<span id="cb874-2"><a href="two-parameters.html#cb874-2"></a>fit_obj &lt;-<span class="st"> </span><span class="kw">stan_glm</span>(<span class="dt">data =</span> ch7_male, </span>
<span id="cb874-3"><a href="two-parameters.html#cb874-3"></a>                    height <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, </span>
<span id="cb874-4"><a href="two-parameters.html#cb874-4"></a>                    <span class="dt">family =</span> <span class="kw">gaussian</span>(), </span>
<span id="cb874-5"><a href="two-parameters.html#cb874-5"></a>                    <span class="dt">refresh =</span> <span class="dv">0</span>)</span></code></pre></div>
<p>Details:</p>
<ul>
<li><p>This may take time. Bayesian models, especially ones with large amounts of data, can take longer than we might like. Indeed, computational limits were the main reason why Bayesian approaches were — and, to some extent, still are — little used. When creating your own models, you will often want to use the <code>cache = TRUE</code> code chunk option. This saves the result of the model so that you don’t recalculate it every time you knit.</p></li>
<li><p>The <code>data</code> argument, like all such usage in R, is used for the input data for the model.</p></li>
<li><p>If you don’t set <code>refresh = 0</code>, the model will puke out many lines of confusing output. You can learn more about that output by reading the help page for <code>stan_glm()</code>. The output provides details on the fitting process as it runs as well as diagnostics about the final result. All of those details are beyond the scope of this book.</p></li>
<li><p>You should always assign the result of the call of <code>stan_glm()</code> to an object, as we do above. By convention, the name of that object will often included the word “fit” to indicate that it is a <em>fitted</em> model object.</p></li>
<li><p>There is a direct connection between the mathematical form of the model created under Justice and the code we use to fit the model under Courage. <code>height ~ 1</code> is the code equivalent of <span class="math inline">\(y_i = \mu\)</span>.</p></li>
<li><p>The default value for <code>family</code> is <code>gaussian()</code>, so we did not need to include it in the call above. From the Justice section, the assumption that <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span> is equivalent to using <code>gaussian()</code>. If <span class="math inline">\(\epsilon_i\)</span> has a different distribution, we would need to use a different <code>family</code>.</p></li>
</ul>
<div id="printed-model" class="section level5">
<h5>
<span class="header-section-number">7.5.3.1.1</span> Printed model</h5>
<p>There are several ways to examine the fitted model. The simplest is to print it:</p>
<div class="sourceCode" id="cb875"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb875-1"><a href="two-parameters.html#cb875-1"></a>fit_obj</span></code></pre></div>
<pre><code>## stan_glm
##  family:       gaussian [identity]
##  formula:      height ~ 1
##  observations: 1814
##  predictors:   1
## ------
##             Median MAD_SD
## (Intercept) 176.0    0.2 
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 7.3    0.1   
## 
## ------
## * For help interpreting the printed output see ?print.stanreg
## * For info on the priors used see ?prior_summary.stanreg</code></pre>
<p>The first line is telling us which model we used, in our case a <code>stan_glm()</code>.</p>
<p>The second line tells us this model is using a Gaussian, or normal, distribution. We discussed this distribution in Section <a href="wrangling.html#normal">2.9.3</a>. The normal is a probability distribution that is symmetric about the mean and unimodal. For that reason, we typically leave it as the default unless we are working with a lefthand variable that is extremely non-normal, e.g., something which only takes two values like 0/1 or TRUE/FALSE. Since height is (very roughly) normally distributed, the Gaussian distribution is a good choice.</p>
<p>The third line gives us back the formula we provided. We are creating a model predicting height with a constant — which is just about the simplest model you can create. Formulas in R are constructed in two parts. First, on the left side of the tilde (the “~” symbol) is the “response” or “dependent” variable, the thing which we are trying to explain. Since this is a model about <code>height</code>, <code>height</code> goes on the lefthand side. Second, we have the “explanatory” or “independent” variables on the righthand side of the tilde. There will often be many such variables but in this, the simplest possible model, there is only one, a single constant. (The number <code>1</code> indicates that constant. It does not mean that we think that everyone is height <code>1</code>.)</p>
<p>The fourth and fifth lines of the output tell us that we have 1814 observations and that we only have one predictor (the constant). Again, the terminology is a bit confusing. What does it mean to suggest that <span class="math inline">\(\mu\)</span> is “constant?” It means that, although <span class="math inline">\(\mu\)</span>’s value is unknown, it is <em>fixed</em>. It does not change from person to person. The <code>1</code> in the formula corresponds to the parameter <span class="math inline">\(\mu\)</span> in our mathematical definition of the model.</p>
<p>We knew all this information before we fit the model. R records it in the <code>fit_obj</code> because we don’t want to forget what we did. The second half of the display gives a summary of the parameter values.</p>
<p>We see the output for the two parameters of the model: intercept and sigma. This can be confusing! Recall that the thing we care most about is <span class="math inline">\(\mu\)</span>, the average height in the population. If we had the ideal Preceptor Table — with a row for every adult male in the population we care about and no missing data — <span class="math inline">\(\mu\)</span> would be trivial to calculate, and with no uncertainty. But only we know that we named that parameter <span class="math inline">\(\mu\)</span>. All that R sees is the <code>1</code> in the formula. In most fields of statistics, this constant term is called the “intercept.” So, now we have three things — <span class="math inline">\(\mu\)</span> (from the math), <code>1</code> (from the code), and “intercept” (from the output) — all of which refer to the exact same concept. This will not be the last time that terminology will be confusing.</p>
<p>At this point, <code>stan_glm()</code> — or rather the <code>print()</code> method for rstan objects — has a problem. We have full posteriors for both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. But this is a simple printed summary. We can’t show the entire distribution. So, what are the best few numbers to provide? There is no right answer to this question! Here, the choice is to provide the median of the posterior and the “MAD_SD.”</p>
<ul>
<li><p>Anytime you have a distribution, whether posterior probability or otherwise, the most important single number associated with it is some measure of its <em>location</em>. Where is the data? The two most common choices for this measure are the mean and median. We use the median here because posterior distributions can often be quite skewed, making the mean a less stable measure.</p></li>
<li><p>The second most important number for summarizing a distribution concerns its <em>spread</em>. How far is the data spread around its center? The most common measure used for this is the standard deviation. MAD SD, the scaled standard deviations of the absolute difference between each observation and the median of all observations, is another. If the variable has a normal distribution, then the standard deviation and the MAD SD will be very similar. But the MAD SD is much more robust to outliers, which is why it is used here.</p></li>
</ul>
<p>Instead of printing the whole model, we can just print out the parameter values:</p>
<div class="sourceCode" id="cb877"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb877-1"><a href="two-parameters.html#cb877-1"></a><span class="kw">print</span>(fit_obj, <span class="dt">detail =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<pre><code>##             Median MAD_SD
## (Intercept) 176.0    0.2 
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 7.3    0.1</code></pre>
<!-- DK: Are we sure that coef() and sigma() give us the median values? -->
<p>Now that we understand the meaning of Median and MAD_SD in the above display, we can interpret the actual numbers. The median of the intercept, 175.98, is the median of our posterior distribution for <span class="math inline">\(\mu\)</span>, the average height of all American men in 2009. The median of sigma, 7.29, is the median of our posterior distribution for the true <span class="math inline">\(\sigma\)</span>, which can be roughly understood as the variability in the height of men, once we account for our estimate of <span class="math inline">\(\mu\)</span>.</p>
<p>The MAD SD for each parameter is a measure of the variability of our posterior distributions. How spread out are they? Speaking roughly, 95% of the mass of a posterior distribution is located within +/- 2 MAD SDs from the median. For example, we would be about 95% confident that the true value of <span class="math inline">\(\mu\)</span> is somewhere between 175.6 and 176.3.</p>
</div>
<div id="plotting-the-posterior-distributions" class="section level5">
<h5>
<span class="header-section-number">7.5.3.1.2</span> Plotting the posterior distributions</h5>
<p>Instead of doing this math in our heads, we can display both posterior distributions. <em>Pictures speak where math mumbles.</em> Fortunately, getting draws from those posteriors is easy:</p>
<!-- DK: Why don't we see decimals for the intercept in this print out? -->
<div class="sourceCode" id="cb879"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb879-1"><a href="two-parameters.html#cb879-1"></a>fit_obj <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb879-2"><a href="two-parameters.html#cb879-2"></a><span class="st">  </span><span class="kw">as_tibble</span>()</span></code></pre></div>
<pre><code>## # A tibble: 4,000 x 2
##    `(Intercept)` sigma
##            &lt;dbl&gt; &lt;dbl&gt;
##  1          176.  7.10
##  2          176.  7.39
##  3          176.  7.20
##  4          176.  7.28
##  5          176.  7.19
##  6          176.  7.43
##  7          176.  7.22
##  8          176.  7.32
##  9          176.  7.38
## 10          176.  7.28
## # … with 3,990 more rows</code></pre>
<p>These 4,000 rows are “draws” from the estimated posteriors, each in its own column. These are like the vectors which result from calling functions like <code>rnorm()</code> or <code>rbinom()</code>. We can create the plot in a similar way:</p>
<div class="sourceCode" id="cb881"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb881-1"><a href="two-parameters.html#cb881-1"></a>fit_obj <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb881-2"><a href="two-parameters.html#cb881-2"></a><span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb881-3"><a href="two-parameters.html#cb881-3"></a><span class="st">  </span><span class="kw">rename</span>(<span class="dt">mu =</span> <span class="st">`</span><span class="dt">(Intercept)</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb881-4"><a href="two-parameters.html#cb881-4"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> mu)) <span class="op">+</span></span>
<span id="cb881-5"><a href="two-parameters.html#cb881-5"></a><span class="st">    </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">after_stat</span>(count<span class="op">/</span><span class="kw">sum</span>(count))), </span>
<span id="cb881-6"><a href="two-parameters.html#cb881-6"></a>                   <span class="dt">binwidth =</span> <span class="fl">0.01</span>, </span>
<span id="cb881-7"><a href="two-parameters.html#cb881-7"></a>                   <span class="dt">color =</span> <span class="st">"white"</span>) <span class="op">+</span></span>
<span id="cb881-8"><a href="two-parameters.html#cb881-8"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">"Posterior Probability Distribution"</span>,</span>
<span id="cb881-9"><a href="two-parameters.html#cb881-9"></a>         <span class="dt">subtitle =</span> <span class="st">"Average height among American adult men in 2009"</span>,</span>
<span id="cb881-10"><a href="two-parameters.html#cb881-10"></a>         <span class="dt">x =</span> <span class="st">"Height in Centimeters"</span>,</span>
<span id="cb881-11"><a href="two-parameters.html#cb881-11"></a>         <span class="dt">y =</span> <span class="st">"Probability"</span>) <span class="op">+</span></span>
<span id="cb881-12"><a href="two-parameters.html#cb881-12"></a><span class="st">    </span><span class="kw">theme_classic</span>()</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-679-1.png" width="672"></p>
<p>Although it is possible to have variable names like “(Intercept)”, it is not recommended. Avoid weird names! When you are stuck with them, place them in backticks. Better, rename them, as we do above.</p>
<div class="sourceCode" id="cb882"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb882-1"><a href="two-parameters.html#cb882-1"></a>fit_obj <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb882-2"><a href="two-parameters.html#cb882-2"></a><span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb882-3"><a href="two-parameters.html#cb882-3"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> sigma)) <span class="op">+</span></span>
<span id="cb882-4"><a href="two-parameters.html#cb882-4"></a><span class="st">    </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">after_stat</span>(count<span class="op">/</span><span class="kw">sum</span>(count))), <span class="dt">binwidth =</span> <span class="fl">0.01</span>, </span>
<span id="cb882-5"><a href="two-parameters.html#cb882-5"></a>                   <span class="dt">color =</span> <span class="st">"white"</span>) <span class="op">+</span></span>
<span id="cb882-6"><a href="two-parameters.html#cb882-6"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">"Posterior Probability Distribution"</span>,</span>
<span id="cb882-7"><a href="two-parameters.html#cb882-7"></a>         <span class="dt">subtitle =</span> <span class="st">"Height standard deviation among American adult men in 2009"</span>,</span>
<span id="cb882-8"><a href="two-parameters.html#cb882-8"></a>         <span class="dt">x =</span> <span class="st">"Sigma in Centimeters"</span>,</span>
<span id="cb882-9"><a href="two-parameters.html#cb882-9"></a>         <span class="dt">y =</span> <span class="st">"Probability"</span>) <span class="op">+</span></span>
<span id="cb882-10"><a href="two-parameters.html#cb882-10"></a><span class="st">    </span><span class="kw">theme_classic</span>()</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-680-1.png" width="672"></p>
<p>Again, <span class="math inline">\(\sigma\)</span> is usually a nuisance parameter. We don’t really care what its value us, so we rarely plot it.</p>
<!-- DK: Discuss the meaning in more detail. -->
<!-- DK: Fun problem set exercise is to pull this data, pivot it, and then create one graphic with all distributions showing. -->
</div>
</div>
<div id="decomposing-the-outcome" class="section level4">
<h4>
<span class="header-section-number">7.5.3.2</span> Decomposing the outcome</h4>
<p>Two other important concepts in model creation are “fitted” values and residuals.</p>
<div class="sourceCode" id="cb883"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb883-1"><a href="two-parameters.html#cb883-1"></a>ch7_male <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb883-2"><a href="two-parameters.html#cb883-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">fitted_value =</span> <span class="kw">fitted</span>(fit_obj)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb883-3"><a href="two-parameters.html#cb883-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">residual =</span> <span class="kw">residuals</span>(fit_obj)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb883-4"><a href="two-parameters.html#cb883-4"></a><span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">5</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 x 3
##   height fitted_value residual
##    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;
## 1   179.         176.     2.92
## 2   191.         176.    14.7 
## 3   172.         176.    -3.48
## 4   168.         176.    -8.08
## 5   164.         176.   -12.1</code></pre>
<p>The fitted value represents the model’s best guess at to what the true value of the outcome should be for that individual, given information about any covariates. This is a tricky concept since, after all, we already know what the actual value is. The residual is the difference between the outcome and the fitted value. These definitions lead to a natural decomposition of the outcome data:</p>
<p><img src="book_temp_files/figure-html/unnamed-chunk-682-1.png" width="672"></p>
<!-- DK: More details. And don't forget to compare this confidence interval to the bootstrapped one. And do a posterior predictive check! -->
</div>
</div>
<div id="temperance" class="section level3">
<h3>
<span class="header-section-number">7.5.4</span> Temperance</h3>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">-->
<img src="other/images/Temperance.jpg" alt=" " width="960"><!--
<p class="caption marginnote">--><!--</p>--><!--</div>--></span>
</p>
<!-- DK: Should introduce matrices in chapter 2. -->
<p>Recall that a “matrix” in R is a rectangular array of data, shaped like a data frame or tibble, but containing only one type of data, e.g., numeric. Large matrices also print out ugly. (There are other differences, none of which we care about here.) Example:</p>
<div class="sourceCode" id="cb885"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb885-1"><a href="two-parameters.html#cb885-1"></a>m &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">12</span>, <span class="dv">13</span>), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb885-2"><a href="two-parameters.html#cb885-2"></a>m</span></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]    3    9
## [2,]    4   12
## [3,]    8   13</code></pre>
<p>The easiest way to pull information from a matrix is to use <code>[]</code>, the subset operator. Here is how we grab the second column of <code>m</code>:</p>
<div class="sourceCode" id="cb887"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb887-1"><a href="two-parameters.html#cb887-1"></a>m[, <span class="dv">2</span>]</span></code></pre></div>
<pre><code>## [1]  9 12 13</code></pre>
<p>Note how matrices with just one dimension “collapse” into single vectors. Tibbles, on the other hand, always maintain their rectangular shapes, even with only one column or row. Matrices are important because <code>posterior_predict()</code> and other functions from <strong>rstanarm</strong> return matrices.</p>
<!-- DK: Teach about rowwise, c_across, and ungroup? Or do that earlier? -->
<!-- DK: Need better explanations about what a draw is, why it is not the same thing as the posterior, but why graphing the draws gives you the posterior. -->
<p>We have a model. What can we do with it? Let’s answer the three questions with which we started this section.</p>
<ul>
<li>What is the probability that the next adult male we meet will be taller than 180 centimeters?</li>
</ul>
<p>We have a model of American male height from 2009, <code>fit_obj</code>, which we can use for this purpose.</p>
<div class="sourceCode" id="cb889"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb889-1"><a href="two-parameters.html#cb889-1"></a><span class="kw">set.seed</span>(<span class="dv">11</span>)</span>
<span id="cb889-2"><a href="two-parameters.html#cb889-2"></a>pp &lt;-<span class="st"> </span><span class="kw">posterior_predict</span>(fit_obj)</span></code></pre></div>
<p>Unfortunately, <code>posterior_predict()</code> returns a weird object with class “ppd”, which stands for posterior probability distribution. There are some advanced use cases in which this is a useful class of object to work with. But, for the purposes of this book, the “ppd” class is too complex. So, whenever we call <code>posterior_predict()</code>, we will always transform it into a tibble like so:</p>
<div class="sourceCode" id="cb890"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb890-1"><a href="two-parameters.html#cb890-1"></a><span class="kw">set.seed</span>(<span class="dv">11</span>)</span>
<span id="cb890-2"><a href="two-parameters.html#cb890-2"></a>pp &lt;-<span class="st"> </span><span class="kw">posterior_predict</span>(fit_obj) <span class="op">%&gt;%</span></span>
<span id="cb890-3"><a href="two-parameters.html#cb890-3"></a><span class="st">    </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span></span>
<span id="cb890-4"><a href="two-parameters.html#cb890-4"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="kw">across</span>(<span class="kw">everything</span>(), as.numeric))</span></code></pre></div>
<p>Doing so requires two steps. First, use <code>as_tibble()</code>, just as you might expect. In R, we often transform one thing into another thing with functions which begin with <code>as_</code>. Unfortunately, that does not solve our problem because each column is still of class <code>ppd</code>. So, second, we use an <code>across</code> incantation to transform each column. The resulting object, <code>pp</code>, is still not easy to work with, both because the variable names are all numbers and because of how big it is.</p>
<div class="sourceCode" id="cb891"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb891-1"><a href="two-parameters.html#cb891-1"></a><span class="kw">dim</span>(pp)</span></code></pre></div>
<pre><code>## [1] 4000 1814</code></pre>
<p>There are 4000 rows because, by default, <code>stan_glm()</code> gives us 4000 draws from the posterior distributions, both for the distribution of the parameters (which we looked at above under Courage) and for the predicted values. There
are 1814 because the matrix provides a (potentially) different posterior prediction for each of the input data rows. (In this case, they are all the same because the model does not use any covariates. In more complex models, the columns in the <code>pp</code> tibble can be very different.)</p>
<p>Why do we want a posterior prediction for each observation? After all, we already know the value for each observation! We know everyone’s height in our data set. No prediction is necessary.</p>
<p>The reason is that, in order to confirm that our model is consistent with the data, <em>we should compare the posterior probability distribution for each observation to the actual value for that observation.</em> They should be consistent. That is, if the model is sensible, about 95% of the true observations should lie within the 95% confidence interval of their respective posterior probability distributions. The process of doing this comparison is a <em>posterior predictive check.</em></p>
<!-- DK: Do this now or save for later? -->
<p>In the meantime, we can still use any column in <code>pp</code> to answer our question. (We will use the first column for convenience.) Consider:</p>
<div class="sourceCode" id="cb893"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb893-1"><a href="two-parameters.html#cb893-1"></a><span class="kw">tibble</span>(<span class="dt">pred =</span> pp<span class="op">$</span><span class="st">`</span><span class="dt">1</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb893-2"><a href="two-parameters.html#cb893-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">gt_180 =</span> <span class="kw">ifelse</span>(pred <span class="op">&gt;</span><span class="st"> </span><span class="dv">180</span>, <span class="ot">TRUE</span>, <span class="ot">FALSE</span>))</span></code></pre></div>
<pre><code>## # A tibble: 4,000 x 2
##     pred gt_180
##    &lt;dbl&gt; &lt;lgl&gt; 
##  1  172. FALSE 
##  2  191. TRUE  
##  3  166. FALSE 
##  4  169. FALSE 
##  5  178. FALSE 
##  6  164. FALSE 
##  7  172. FALSE 
##  8  185. TRUE  
##  9  173. FALSE 
## 10  183. TRUE  
## # … with 3,990 more rows</code></pre>
<p>We don’t have to put the posterior predictions in a tibble, but doing so makes everything easier. What are the odds that the next adult male will be taller than 180 centimeters?</p>
<div class="sourceCode" id="cb895"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb895-1"><a href="two-parameters.html#cb895-1"></a><span class="kw">tibble</span>(<span class="dt">pred =</span> pp<span class="op">$</span><span class="st">`</span><span class="dt">1</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb895-2"><a href="two-parameters.html#cb895-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">gt_180 =</span> <span class="kw">ifelse</span>(pred <span class="op">&gt;</span><span class="st"> </span><span class="dv">180</span>, <span class="ot">TRUE</span>, <span class="ot">FALSE</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb895-3"><a href="two-parameters.html#cb895-3"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">answer =</span> <span class="kw">sum</span>(gt_<span class="dv">180</span>) <span class="op">/</span><span class="st"> </span><span class="kw">n</span>())</span></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   answer
##    &lt;dbl&gt;
## 1  0.290</code></pre>
<p>Somewhere around 29% or so.</p>
<p>Again, the key difficulty is the population. The problem we actually have involves walking around London, or wherever, today. The data we have involve America in 2009. Those are not the same things! But they are not totally different. Knowing whether the data we have is “close enough” to the problem we want to solve is at the heart of Wisdom. Yet that was the decision we made at the start of the process, the decision to create a model in the first place. Now that we have created a model, we look to the virtue of Temperance for guidance in using that model. The data we have is never a perfect match for the world we face. We need to temper our confidence and act with humility. Our forecasts will never be as good as a naive use of the model might suggest. Reality will surprise us. We need to take the model’s claims with a family-sized portion of salt.</p>
<!-- DK: More on temperance and the many ways that we should be less confident. -->
<ul>
<li>What is the probability that, among the next 4 men we meet, the tallest is at least 10 cm taller than the shortest?</li>
</ul>
<p>Bayesian models are beautiful because, via the magic of simulation, we can answer (almost!) any question. With simulation, we just need to answer this step by step.</p>
<div class="sourceCode" id="cb897"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb897-1"><a href="two-parameters.html#cb897-1"></a><span class="kw">tibble</span>(<span class="dt">pred_1 =</span> pp<span class="op">$</span><span class="st">`</span><span class="dt">1</span><span class="st">`</span>,</span>
<span id="cb897-2"><a href="two-parameters.html#cb897-2"></a>       <span class="dt">pred_2 =</span> pp<span class="op">$</span><span class="st">`</span><span class="dt">2</span><span class="st">`</span>,</span>
<span id="cb897-3"><a href="two-parameters.html#cb897-3"></a>       <span class="dt">pred_3 =</span> pp<span class="op">$</span><span class="st">`</span><span class="dt">3</span><span class="st">`</span>,</span>
<span id="cb897-4"><a href="two-parameters.html#cb897-4"></a>       <span class="dt">pred_4 =</span> pp<span class="op">$</span><span class="st">`</span><span class="dt">4</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb897-5"><a href="two-parameters.html#cb897-5"></a><span class="st">  </span><span class="kw">rowwise</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb897-6"><a href="two-parameters.html#cb897-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">tallest =</span> <span class="kw">max</span>(<span class="kw">c_across</span>(pred_<span class="dv">1</span><span class="op">:</span>pred_<span class="dv">4</span>))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb897-7"><a href="two-parameters.html#cb897-7"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">shortest =</span> <span class="kw">min</span>(<span class="kw">c_across</span>(pred_<span class="dv">1</span><span class="op">:</span>pred_<span class="dv">4</span>))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb897-8"><a href="two-parameters.html#cb897-8"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">diff =</span> tallest <span class="op">-</span><span class="st"> </span>shortest) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb897-9"><a href="two-parameters.html#cb897-9"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">gt_10 =</span> <span class="kw">ifelse</span>(diff <span class="op">&gt;=</span><span class="st"> </span><span class="dv">10</span>, <span class="ot">TRUE</span>, <span class="ot">FALSE</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb897-10"><a href="two-parameters.html#cb897-10"></a><span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb897-11"><a href="two-parameters.html#cb897-11"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">answer =</span> <span class="kw">sum</span>(gt_<span class="dv">10</span>) <span class="op">/</span><span class="st"> </span><span class="kw">n</span>())</span></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   answer
##    &lt;dbl&gt;
## 1  0.758</code></pre>
<p>There is about a 75% chance that, when meeting 4 random men, the tallest will be at least 10 cm taller than the shortest.</p>
<!-- DK: Discuss all the reasons why this might not be true. -->
<ul>
<li>What is our posterior probability of the height of the 3rd tallest man out of the next 100 we meet?</li>
</ul>
<p>The same approach will work for almost any question.</p>
<div class="sourceCode" id="cb899"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb899-1"><a href="two-parameters.html#cb899-1"></a>pp[, <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>] <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb899-2"><a href="two-parameters.html#cb899-2"></a><span class="st">  </span><span class="kw">rowwise</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb899-3"><a href="two-parameters.html#cb899-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">third_tallest =</span> <span class="kw">sort</span>(<span class="kw">c_across</span>(<span class="st">`</span><span class="dt">1</span><span class="st">`</span><span class="op">:</span><span class="st">`</span><span class="dt">100</span><span class="st">`</span>), <span class="dt">decreasing =</span> <span class="ot">TRUE</span>)[<span class="dv">3</span>]) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb899-4"><a href="two-parameters.html#cb899-4"></a><span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb899-5"><a href="two-parameters.html#cb899-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> third_tallest, <span class="dt">y =</span> <span class="kw">after_stat</span>(count <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(count)))) <span class="op">+</span></span>
<span id="cb899-6"><a href="two-parameters.html#cb899-6"></a><span class="st">    </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">100</span>) <span class="op">+</span></span>
<span id="cb899-7"><a href="two-parameters.html#cb899-7"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">"Posterior Probability of the Height"</span>,</span>
<span id="cb899-8"><a href="two-parameters.html#cb899-8"></a>         <span class="dt">subtitle =</span> <span class="st">"of the 3rd Tallest from One Hundred Random Men"</span>,</span>
<span id="cb899-9"><a href="two-parameters.html#cb899-9"></a>         <span class="dt">x =</span> <span class="st">"Height (cm)"</span>,</span>
<span id="cb899-10"><a href="two-parameters.html#cb899-10"></a>         <span class="dt">y =</span> <span class="st">"Probability"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-692-1.png" width="672"></p>
<!-- DK: Need more text. Explain all the things that could be wrong with the model. Explain what is going on in different columns. Explain all the cool R code tricks.  -->
</div>
</div>
<div id="conclusion" class="section level2">
<h2>
<span class="header-section-number">7.6</span> Conclusion</h2>
<p>The next five chapters will follow the same process we have just completed here. We start with a decision we have to make. With luck, we will have some data to guide us. (Without data, even the best data scientist will struggle to make progress.) <em>Wisdom</em> asks us: “Is the data we have close enough to the decision we face to make using that data likely to be helpful?” Often times, the answer is “No.” Even if we do have data, and the ability to make a model, Wisdom will tap us on the shoulder and say, “Even if you can make a model, don’t forget to ask yourself if you should.” Ethics matter.</p>
<p>Once we start to build the model, <em>Justice</em> will guide us. Is the model descriptive or causal? What is the mathematical relationship between the dependent variable we are trying to explain and the independent variables we can use to explain it? What assumptions are we making about distributions, especially with regard to the error term?</p>
<p>Having set up the model framework, we need <em>Courage</em> to implement the model in code. Without code, all the math in the world is useless. Once we have created the model, we need to understand it. What are the posterior distributions of the unknown parameters? Do they seem sensible? How should we interpret them?</p>
<p><em>Temperance</em> guides the final step. With a model, we can finally get back to the decision which motivated the exercise in the first place. We can use the model to make statements about the world, both to confirm that the model is consistent with the world and to use the model to make predictions about numbers which we do not know.</p>
<p>Let’s practice this process another dozen or so times.</p>

<!-- BG: test commit take 2 -->
</div>
</div></body></html>

<p style="text-align: center;">
<a href="one-parameter.html"><button class="btn btn-default">Previous</button></a>
<a href="three-parameters.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-10-17
</p>
</div>
</div>



</body>
</html>
