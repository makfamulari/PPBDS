<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 12 Discrete Response | Gov 50: Data" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="davidkane9/PPBDS" />



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Chapter 12 Discrete Response | Gov 50: Data">

<title>Chapter 12 Discrete Response | Gov 50: Data</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/str_view-0.1.0/str_view.css" rel="stylesheet" />
<script src="libs/str_view-binding-1.4.0/str_view.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Gov 50: Data<p><p class="author"></p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html"></a>
<a href="preamble.html">Preamble</a>
<a href="shopping-week.html">Shopping Week</a>
<a href="visualization.html"><span class="toc-section-number">1</span> Visualization</a>
<a href="wrangling.html"><span class="toc-section-number">2</span> Wrangling</a>
<a href="rubin-causal-model.html"><span class="toc-section-number">3</span> Rubin Causal Model</a>
<a href="functions.html"><span class="toc-section-number">4</span> Functions</a>
<a href="probability.html"><span class="toc-section-number">5</span> Probability</a>
<a href="one-parameter.html"><span class="toc-section-number">6</span> One Parameter</a>
<a href="two-parameters.html"><span class="toc-section-number">7</span> Two Parameters</a>
<a href="three-parameters.html"><span class="toc-section-number">8</span> Three Parameters</a>
<a href="n-parameters.html"><span class="toc-section-number">9</span> N Parameters</a>
<a href="model-choice.html"><span class="toc-section-number">10</span> Model Choice</a>
<a href="continuous-response.html"><span class="toc-section-number">11</span> Continuous Response</a>
<a id="active-page" href="discrete-response.html"><span class="toc-section-number">12</span> Discrete Response</a><ul class="toc-sections">
<li class="toc"><a href="#binary-classification-metrics"> Binary classification metrics</a></li>
<li class="toc"><a href="#exploratory-data-analysis-eda"> Exploratory Data Analysis (EDA)</a></li>
<li class="toc"><a href="#logistic-regression"> Logistic regression</a></li>
<li class="toc"><a href="#classification-and-regression-trees-cart"> Classification and regression trees (CART)</a></li>
<li class="toc"><a href="#random-forests"> Random forests</a></li>
<li class="toc"><a href="#cardinal-virtues-2"> Cardinal Virtues</a></li>
</ul>
<a href="appendices.html">Appendices</a>
<a href="tools.html">Tools</a>
<a href="shiny.html">Shiny</a>
<a href="maps.html">Maps</a>
<a href="animation.html">Animation</a>
<a href="references.html">References</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="discrete-response" class="section level1">
<h1>
<span class="header-section-number">Chapter 12</span> Discrete Response</h1>
<p>Packages:</p>
<div class="sourceCode" id="cb1127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1127-1"><a href="discrete-response.html#cb1127-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1127-2"><a href="discrete-response.html#cb1127-2"></a><span class="kw">library</span>(skimr)</span>
<span id="cb1127-3"><a href="discrete-response.html#cb1127-3"></a><span class="kw">library</span>(PPBDS.data)</span>
<span id="cb1127-4"><a href="discrete-response.html#cb1127-4"></a><span class="kw">library</span>(tidymodels)</span></code></pre></div>
<p><strong>Binary responses</strong> take on only two values: success (<span class="math inline">\(Y=1\)</span>) or failure (<span class="math inline">\(Y=0\)</span>), yes (<span class="math inline">\(Y=1\)</span>) or no (<span class="math inline">\(Y=0\)</span>), et cetera. Binary responses are one of the most common types of data that statisticians encounter. We are often interested in modeling the probability of success, <span class="math inline">\(p\)</span>, based on a set of covariates. As with regression, there are two broad categories of problems: <em>modeling for prediction</em> and <em>modeling for causation</em>. Although terminology varies across fields, “regression” is generally used for situations in which our <em>dependent variable</em> is continuous, as in Chapter <a href="continuous-response.html#continuous-response">11</a>. “Classification” applies to cases in which the dependent variable takes on discrete values, the simplest of which is the binary case.</p>
<p>In this chapter, we will look at three common techniques of <strong>classification</strong> of binary data. First, we will consider logistic regression. Second, we will consider classification and regression trees (CART). Third, we will discuss random forests. We use the <strong>tidymodels</strong> tools for all examples. At the end, we will compare the performances of all three models.</p>
<!-- DK: Finish with a question. -->
<div id="binary-classification-metrics" class="section level2">
<h2>
<span class="header-section-number">12.1</span> Binary classification metrics</h2>
<!-- DK: Get rid of most of this. -->
<p>Recall the discussion in Chapter <a href="model-choice.html#model-choice">10</a> about how to measure model accuracy. Classification models require different measures than those we used in Chapter <a href="continuous-response.html#continuous-response">11</a> when considering continuous outcome measures.</p>
<p>The <strong>modeldata</strong> package contains example predictions from a test data set with two classes (“Class1” and “Class2”):</p>
<div class="sourceCode" id="cb1128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1128-1"><a href="discrete-response.html#cb1128-1"></a><span class="kw">data</span>(two_class_example)</span>
<span id="cb1128-2"><a href="discrete-response.html#cb1128-2"></a><span class="kw">str</span>(two_class_example)</span></code></pre></div>
<pre><code>## 'data.frame':    500 obs. of  4 variables:
##  $ truth    : Factor w/ 2 levels "Class1","Class2": 2 1 2 1 2 1 1 1 2 2 ...
##  $ Class1   : num  0.00359 0.67862 0.11089 0.73516 0.01624 ...
##  $ Class2   : num  0.996 0.321 0.889 0.265 0.984 ...
##  $ predicted: Factor w/ 2 levels "Class1","Class2": 2 1 2 1 2 1 1 1 2 2 ...</code></pre>
<p>The second and third columns are the predicted class probabilities for the test set while <code>predicted</code> are the discrete predictions.</p>
<p>For the hard class predictions, there are a variety of <strong>yardstick</strong> functions that are helpful:</p>
<div class="sourceCode" id="cb1130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1130-1"><a href="discrete-response.html#cb1130-1"></a><span class="co"># A confusion matrix: </span></span>
<span id="cb1130-2"><a href="discrete-response.html#cb1130-2"></a><span class="kw">conf_mat</span>(two_class_example, <span class="dt">truth =</span> truth, <span class="dt">estimate =</span> predicted)</span></code></pre></div>
<pre><code>##           Truth
## Prediction Class1 Class2
##     Class1    227     50
##     Class2     31    192</code></pre>
<div class="sourceCode" id="cb1132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1132-1"><a href="discrete-response.html#cb1132-1"></a><span class="kw">accuracy</span>(two_class_example, <span class="dt">truth =</span> truth, <span class="dt">estimate =</span> predicted)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.838</code></pre>
<div class="sourceCode" id="cb1134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1134-1"><a href="discrete-response.html#cb1134-1"></a><span class="co"># Matthews correlation coefficient:</span></span>
<span id="cb1134-2"><a href="discrete-response.html#cb1134-2"></a><span class="kw">mcc</span>(two_class_example, truth, predicted)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 mcc     binary         0.677</code></pre>
<div class="sourceCode" id="cb1136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1136-1"><a href="discrete-response.html#cb1136-1"></a><span class="co"># F1 metric:</span></span>
<span id="cb1136-2"><a href="discrete-response.html#cb1136-2"></a><span class="kw">f_meas</span>(two_class_example, truth, predicted)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 f_meas  binary         0.849</code></pre>
<p>For binary classification data sets, these functions have a standard argument called <code>event_level</code>. The <em>default</em> is that the <strong>first</strong> level of the outcome factor is the event of interest.</p>
<div class="rmdnote">
<p>
There is some heterogeneity in R functions in this regard; some use the first level and others the second to denote the event of interest. We consider it more intuitive that the first level is the most important. The second level logic is borne of encoding the outcome as 0/1 (in which case the second value is the event) and unfortunately remains in some packages. However, tidymodels (along with many other R packages) <em>require</em> a categorical outcome to be encoded as a factor and, for this reason, the legacy justification for the second level as the event becomes irrelevant.
</p>
</div>
<p>As an example where the second class is the event:</p>
<div class="sourceCode" id="cb1138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1138-1"><a href="discrete-response.html#cb1138-1"></a><span class="kw">f_meas</span>(two_class_example, truth, predicted, <span class="dt">event_level =</span> <span class="st">"second"</span>)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 f_meas  binary         0.826</code></pre>
<p>In the output above, the <code>.estimator</code> value of “binary” indicates that the standard formula for binary classes will be used.</p>
<p>There are numerous classification metrics that use the predicted probabilities as inputs rather than the hard class predictions. For example, the receiver operating characteristic (ROC) curve computes the sensitivity and specificity over a continuum of different event thresholds. The predicted class column is not used. There are two <strong>yardstick</strong> functions for this method: <code>roc_curve()</code> computes the data points that make up the ROC curve and <code>roc_auc()</code> computes the area under the curve.</p>
<p>The interfaces to these types of metric functions use the <code>...</code> argument placeholder to pass in the appropriate class probability column. For two-class problems, the probability column for the event of interest is passed into the function:</p>
<div class="sourceCode" id="cb1140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1140-1"><a href="discrete-response.html#cb1140-1"></a>two_class_curve &lt;-<span class="st"> </span><span class="kw">roc_curve</span>(two_class_example, truth, Class1)</span>
<span id="cb1140-2"><a href="discrete-response.html#cb1140-2"></a>two_class_curve</span></code></pre></div>
<pre><code>## # A tibble: 502 x 3
##    .threshold specificity sensitivity
##         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
##  1 -Inf           0                 1
##  2    1.79e-7     0                 1
##  3    4.50e-6     0.00413           1
##  4    5.81e-6     0.00826           1
##  5    5.92e-6     0.0124            1
##  6    1.22e-5     0.0165            1
##  7    1.40e-5     0.0207            1
##  8    1.43e-5     0.0248            1
##  9    2.38e-5     0.0289            1
## 10    3.30e-5     0.0331            1
## # … with 492 more rows</code></pre>
<div class="sourceCode" id="cb1142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1142-1"><a href="discrete-response.html#cb1142-1"></a><span class="kw">roc_auc</span>(two_class_example, truth, Class1)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 roc_auc binary         0.939</code></pre>
<p>The <code>two_class_curve</code> object can be used in a <code>ggplot</code> call to visualize the curve. There is an <code>autoplot()</code> method that will take care of the details:</p>
<div class="sourceCode" id="cb1144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1144-1"><a href="discrete-response.html#cb1144-1"></a><span class="kw">autoplot</span>(two_class_curve)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/performance-2class-roc-curve-1.png" width="672"></p>
<p>There are a number of other functions that use probability estimates, including <code>gain_curve()</code>, <code>lift_curve()</code>, and <code>pr_curve()</code>.</p>
</div>
<div id="exploratory-data-analysis-eda" class="section level2">
<h2>
<span class="header-section-number">12.2</span> Exploratory Data Analysis (EDA)</h2>
<p>Begin with our usual libraries:</p>
<div class="sourceCode" id="cb1145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1145-1"><a href="discrete-response.html#cb1145-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1145-2"><a href="discrete-response.html#cb1145-2"></a><span class="kw">library</span>(broom)</span>
<span id="cb1145-3"><a href="discrete-response.html#cb1145-3"></a><span class="kw">library</span>(skimr)</span>
<span id="cb1145-4"><a href="discrete-response.html#cb1145-4"></a><span class="kw">library</span>(PPBDS.data)</span>
<span id="cb1145-5"><a href="discrete-response.html#cb1145-5"></a><span class="kw">library</span>(tidymodels)</span></code></pre></div>
<p>Before we start modeling, let’s perform some exploratory analysis on the dataset we’ll be working with, cces. Cces stands for the Cooperative Congressional Election Study, a study regarding the approval rating of individual voters to their sitting president. Each row captures one voter, some of their demographic information, and how highly they approve (or disaprprove) of the president.</p>
<div class="sourceCode" id="cb1146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1146-1"><a href="discrete-response.html#cb1146-1"></a><span class="kw">glimpse</span>(cces)</span></code></pre></div>
<pre><code>## Rows: 452,755
## Columns: 12
## $ year        &lt;int&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 200…
## $ state       &lt;chr&gt; "North Carolina", "Ohio", "New Jersey", "Illinois", "New …
## $ gender      &lt;chr&gt; "Female", "Male", "Female", "Female", "Male", "Female", "…
## $ age         &lt;int&gt; 32, 49, 54, 34, 20, 27, 47, 20, 77, 19, 53, 55, 38, 72, 4…
## $ race        &lt;chr&gt; "White", "White", "White", "Black", "White", "White", "Wh…
## $ marstat     &lt;chr&gt; "Divorced", "Married", "Divorced", "Single / Never Marrie…
## $ ideology    &lt;fct&gt; Liberal, Moderate, Liberal, Liberal, Liberal, Liberal, Co…
## $ education   &lt;fct&gt; High School Graduate, Post-Grad, High School Graduate, 4-…
## $ news        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ econ        &lt;chr&gt; "Gotten Worse / Somewhat Worse", "Gotten Much Worse", "Go…
## $ approval_ch &lt;chr&gt; "Strongly Disapprove", "Strongly Disapprove", "Strongly D…
## $ approval    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 5, 1, 1, 2, 4, 1, 4, 5, 1, 1, 1, 3, 1, …</code></pre>
<p>We will tweak the data by only looking at observations recorded in the year 2018 so that all the responses are about the same president. We’ll also select the variables that are currently of interest to us. Finally, because this chapter will be dealing with logistic regressions, we want to convert the numeric <code>approval</code> variable into a binary variable. <code>approval</code> is a numeric variable from 1-5 with 5 representing the highest approval of the president. In order to do this, we have to turn approval into a binary variable. 1-2 will be coded to 0 to signify disapproval and 3-5 will be coded to 1 for approval. We will also cast <code>approval</code> as a factor variable rather than a number, which is useful information for models.</p>
<div class="sourceCode" id="cb1148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1148-1"><a href="discrete-response.html#cb1148-1"></a>ch12 &lt;-<span class="st"> </span>cces <span class="op">%&gt;%</span></span>
<span id="cb1148-2"><a href="discrete-response.html#cb1148-2"></a><span class="st">  </span><span class="kw">filter</span>(year <span class="op">==</span><span class="st"> </span><span class="dv">2018</span>) <span class="op">%&gt;%</span></span>
<span id="cb1148-3"><a href="discrete-response.html#cb1148-3"></a><span class="st">  </span><span class="kw">select</span>(state, age, gender, race, education, ideology, approval) <span class="op">%&gt;%</span></span>
<span id="cb1148-4"><a href="discrete-response.html#cb1148-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">approval =</span> <span class="kw">as.factor</span>(<span class="kw">case_when</span>(</span>
<span id="cb1148-5"><a href="discrete-response.html#cb1148-5"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">~</span><span class="st"> </span><span class="dv">0</span>,</span>
<span id="cb1148-6"><a href="discrete-response.html#cb1148-6"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">2</span> <span class="op">~</span><span class="st"> </span><span class="dv">0</span>,</span>
<span id="cb1148-7"><a href="discrete-response.html#cb1148-7"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">3</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb1148-8"><a href="discrete-response.html#cb1148-8"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">4</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb1148-9"><a href="discrete-response.html#cb1148-9"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">5</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>)))</span></code></pre></div>
<p>From this, we can gather that there are 16 variables. Notably, there are 60,000 observations even after filtering only for the year 2018.</p>
<p>Let’s also display a random sample of 5 rows of the 60,000 rows.</p>
<div class="sourceCode" id="cb1149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1149-1"><a href="discrete-response.html#cb1149-1"></a>ch12 <span class="op">%&gt;%</span></span>
<span id="cb1149-2"><a href="discrete-response.html#cb1149-2"></a><span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">5</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 x 7
##   state            age gender race  education          ideology         approval
##   &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;fct&gt;              &lt;fct&gt;            &lt;fct&gt;   
## 1 South Carolina    61 Female White High School Gradu… Not Sure         1       
## 2 Arizona           68 Female White High School Gradu… Very Liberal     1       
## 3 Florida           77 Male   White 2-Year             Very Conservati… 1       
## 4 Maryland          40 Male   White 4-Year             Very Conservati… 1       
## 5 Florida           56 Female White 4-Year             Moderate         0</code></pre>
<p>Now, let’s compute summary statistics. Let’s use the <code>skim()</code> function from the <code>skimr</code> package.</p>
<div class="sourceCode" id="cb1151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1151-1"><a href="discrete-response.html#cb1151-1"></a>ch12 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1151-2"><a href="discrete-response.html#cb1151-2"></a><span class="st">  </span><span class="kw">skim</span>()</span></code></pre></div>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:unnamed-chunk-885">TABLE 12.1: </span>Data summary</span><!--</caption>--></p>
<table><tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Piped data</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">60000</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">7</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">3</td>
</tr>
<tr class="odd">
<td align="left">factor</td>
<td align="left">3</td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody></table>
<p><strong>Variable type: character</strong></p>
<table>
<thead><tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">state</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">20</td>
<td align="right">0</td>
<td align="right">51</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">gender</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">6</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">race</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">15</td>
<td align="right">0</td>
<td align="right">8</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<thead><tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">ordered</th>
<th align="right">n_unique</th>
<th align="left">top_counts</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">education</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="left">FALSE</td>
<td align="right">6</td>
<td align="left">Hig: 16617, 4-Y: 14256, Som: 12631, Pos: 8316</td>
</tr>
<tr class="even">
<td align="left">ideology</td>
<td align="right">419</td>
<td align="right">0.99</td>
<td align="left">FALSE</td>
<td align="right">6</td>
<td align="left">Mod: 17302, Con: 12052, Lib: 10929, Ver: 7434</td>
</tr>
<tr class="odd">
<td align="left">approval</td>
<td align="right">33</td>
<td align="right">1.00</td>
<td align="left">FALSE</td>
<td align="right">2</td>
<td align="left">0: 33743, 1: 26224</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead><tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr></thead>
<tbody><tr class="odd">
<td align="left">age</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">48</td>
<td align="right">18</td>
<td align="right">18</td>
<td align="right">32</td>
<td align="right">48</td>
<td align="right">62</td>
<td align="right">95</td>
<td align="left">▇▇▇▅▁</td>
</tr></tbody>
</table>
<p>You’ll notice that we are missing data for our ideology and approval variables. The <code>complete_rate</code> column tells us that approval has 3% missing observations and ideology has 0.7% missing observations. Let’s use the function <code>drop_na()</code> to get rid of these missing observations so they don’t interfere with our models later in the chapter:</p>
<div class="sourceCode" id="cb1152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1152-1"><a href="discrete-response.html#cb1152-1"></a>ch12 &lt;-<span class="st"> </span>cces <span class="op">%&gt;%</span></span>
<span id="cb1152-2"><a href="discrete-response.html#cb1152-2"></a><span class="st">  </span><span class="kw">filter</span>(year <span class="op">==</span><span class="st"> </span><span class="dv">2018</span>) <span class="op">%&gt;%</span></span>
<span id="cb1152-3"><a href="discrete-response.html#cb1152-3"></a><span class="st">  </span><span class="kw">select</span>(state, age, gender, race, education, ideology, approval) <span class="op">%&gt;%</span></span>
<span id="cb1152-4"><a href="discrete-response.html#cb1152-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">approval =</span> <span class="kw">as.factor</span>(<span class="kw">case_when</span>(</span>
<span id="cb1152-5"><a href="discrete-response.html#cb1152-5"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">~</span><span class="st"> </span><span class="dv">0</span>,</span>
<span id="cb1152-6"><a href="discrete-response.html#cb1152-6"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">2</span> <span class="op">~</span><span class="st"> </span><span class="dv">0</span>,</span>
<span id="cb1152-7"><a href="discrete-response.html#cb1152-7"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">3</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb1152-8"><a href="discrete-response.html#cb1152-8"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">4</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb1152-9"><a href="discrete-response.html#cb1152-9"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">5</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1152-10"><a href="discrete-response.html#cb1152-10"></a><span class="st">  </span><span class="kw">drop_na</span>()</span></code></pre></div>
<p>To complete our exploratory data analysis, let’s create some data visualizations.</p>
<p>The primary response variable left in our dataset is <code>approval</code>, a (newly) binary variable with 0 representing disapproval of the President and 1 representing approval. So, let’s start by looking at the overall distribution of <code>approval</code>.</p>
<div class="sourceCode" id="cb1153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1153-1"><a href="discrete-response.html#cb1153-1"></a>ch12 <span class="op">%&gt;%</span></span>
<span id="cb1153-2"><a href="discrete-response.html#cb1153-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> approval)) <span class="op">+</span></span>
<span id="cb1153-3"><a href="discrete-response.html#cb1153-3"></a><span class="st">    </span><span class="kw">geom_bar</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb1153-4"><a href="discrete-response.html#cb1153-4"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"Count"</span>,</span>
<span id="cb1153-5"><a href="discrete-response.html#cb1153-5"></a>         <span class="dt">x =</span> <span class="st">"Presidential Approval"</span>,</span>
<span id="cb1153-6"><a href="discrete-response.html#cb1153-6"></a>         <span class="dt">title =</span> <span class="st">"Presidential Approval in 2018"</span>) </span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-887-1.png" width="672"></p>
<p>According to this graph, there are roughly 10,000 more voters who disapprove of Trump. To make things more interesting, let’s look at <code>approval</code> across gender and then race.</p>
<div class="sourceCode" id="cb1154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1154-1"><a href="discrete-response.html#cb1154-1"></a>ch12 <span class="op">%&gt;%</span></span>
<span id="cb1154-2"><a href="discrete-response.html#cb1154-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> approval, <span class="dt">fill =</span> gender)) <span class="op">+</span></span>
<span id="cb1154-3"><a href="discrete-response.html#cb1154-3"></a><span class="st">  </span><span class="kw">geom_bar</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb1154-4"><a href="discrete-response.html#cb1154-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"Count"</span>,</span>
<span id="cb1154-5"><a href="discrete-response.html#cb1154-5"></a>       <span class="dt">x =</span> <span class="st">"Presidential Approval"</span>,</span>
<span id="cb1154-6"><a href="discrete-response.html#cb1154-6"></a>       <span class="dt">title =</span> <span class="st">"Presidential Approval in 2018 by Gender"</span>) <span class="op">+</span></span>
<span id="cb1154-7"><a href="discrete-response.html#cb1154-7"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>gender) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1154-8"><a href="discrete-response.html#cb1154-8"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">"none"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-888-1.png" width="672"></p>
<p>It seems that females have higher rates of disapproval of the President than males have.</p>
<div class="sourceCode" id="cb1155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1155-1"><a href="discrete-response.html#cb1155-1"></a>ch12 <span class="op">%&gt;%</span></span>
<span id="cb1155-2"><a href="discrete-response.html#cb1155-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">fill =</span> approval, <span class="dt">x =</span> race, <span class="dt">y =</span> age)) <span class="op">+</span></span>
<span id="cb1155-3"><a href="discrete-response.html#cb1155-3"></a><span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="st">"fill"</span>, <span class="dt">stat=</span><span class="st">"identity"</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1155-4"><a href="discrete-response.html#cb1155-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"Percentage"</span>,</span>
<span id="cb1155-5"><a href="discrete-response.html#cb1155-5"></a>       <span class="dt">x =</span> <span class="st">"Presidential Approval"</span>,</span>
<span id="cb1155-6"><a href="discrete-response.html#cb1155-6"></a>       <span class="dt">title =</span> <span class="st">"Presidential Approval in 2018 by Race"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-889-1.png" width="672"></p>
<p>This segmented bar graph shows us the percentage of each race that approved of the president. We can see that the disapproving majority in the overall data is present across most races.</p>
<p>Now, let’s use our state variable. Let’s create a scatterplot with <code>approval</code> to see how the rate of approval varied across states.</p>
<div class="sourceCode" id="cb1156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1156-1"><a href="discrete-response.html#cb1156-1"></a>ch12 <span class="op">%&gt;%</span></span>
<span id="cb1156-2"><a href="discrete-response.html#cb1156-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">approval =</span> <span class="kw">as.integer</span>(approval)) <span class="op">%&gt;%</span></span>
<span id="cb1156-3"><a href="discrete-response.html#cb1156-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">approval =</span> <span class="kw">case_when</span>(</span>
<span id="cb1156-4"><a href="discrete-response.html#cb1156-4"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">~</span><span class="st"> </span><span class="dv">0</span>,</span>
<span id="cb1156-5"><a href="discrete-response.html#cb1156-5"></a>    approval <span class="op">==</span><span class="st"> </span><span class="dv">2</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>)) <span class="op">%&gt;%</span></span>
<span id="cb1156-6"><a href="discrete-response.html#cb1156-6"></a><span class="st">  </span><span class="kw">group_by</span>(state) <span class="op">%&gt;%</span></span>
<span id="cb1156-7"><a href="discrete-response.html#cb1156-7"></a><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">avg_approval =</span> <span class="kw">sum</span>(approval)<span class="op">/</span><span class="kw">n</span>()) <span class="op">%&gt;%</span></span>
<span id="cb1156-8"><a href="discrete-response.html#cb1156-8"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> avg_approval, <span class="dt">y =</span> <span class="kw">reorder</span>(state, avg_approval))) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1156-9"><a href="discrete-response.html#cb1156-9"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb1156-10"><a href="discrete-response.html#cb1156-10"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"State"</span>,</span>
<span id="cb1156-11"><a href="discrete-response.html#cb1156-11"></a>       <span class="dt">x =</span> <span class="st">"Approval Rate of President"</span>,</span>
<span id="cb1156-12"><a href="discrete-response.html#cb1156-12"></a>       <span class="dt">title =</span> <span class="st">"Presidential Approval by State"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-890-1.png" width="672"></p>
<p>Let’s now complete the first step for a proper data science project. We will split our data set into training and testing samples, then create cross-validations from the training data.</p>
<div class="sourceCode" id="cb1157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1157-1"><a href="discrete-response.html#cb1157-1"></a><span class="kw">set.seed</span>(<span class="dv">10</span>)</span>
<span id="cb1157-2"><a href="discrete-response.html#cb1157-2"></a>ch12_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(ch12, <span class="dt">prob =</span> <span class="fl">0.80</span>)</span>
<span id="cb1157-3"><a href="discrete-response.html#cb1157-3"></a>ch12_train &lt;-<span class="st"> </span><span class="kw">training</span>(ch12_split)</span>
<span id="cb1157-4"><a href="discrete-response.html#cb1157-4"></a>ch12_test  &lt;-<span class="st"> </span><span class="kw">testing</span>(ch12_split)</span>
<span id="cb1157-5"><a href="discrete-response.html#cb1157-5"></a>ch12_folds &lt;-<span class="st"> </span><span class="kw">vfold_cv</span>(ch12_train, <span class="dt">v =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>We can now use the split data to explore different models.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2>
<span class="header-section-number">12.3</span> Logistic regression</h2>
<p>Now that we know our dataset a little better, let’s begin our first way of modeling binary/discrete data: logistic regressions.</p>
<p>Figure <a href="discrete-response.html#fig:OLSlogistic">12.1</a> illustrates a data set with a binary (0 or 1) response (<span class="math inline">\(Y\)</span>) and a single continuous predictor (<span class="math inline">\(X\)</span>). The blue line is a linear regression to model the probability of a success (<span class="math inline">\(Y=1\)</span>) for a given value of <span class="math inline">\(X\)</span>. With a binary response, the linear regression has an obvious problem: it can produce predicted probabilities below 0 and above 1. Probabilities can only range from 0 up to and including 1 as these represent a 0% and 100% chance of an event happening, respectively.</p>
<p>The red curve is the <em>logistic regression</em> curve. Note that its characteristic “S” shape always produces predicted probabilities between 0 and 1. Here is the formula for a logistic regression:</p>
<p>Where <span class="math inline">\(p\)</span> is the probability of a “yes” or “success” for a given set of predictors <span class="math inline">\(X\)</span>.</p>
<!-- Revisit nomenclature after chapter 5 -->
<div class="figure" style="text-align: center">
<span id="fig:OLSlogistic"></span>
<p class="caption marginnote shownote">
FIGURE 12.1: Linear vs. logistic regression models for binary response data.
</p>
<img src="book_temp_files/figure-html/OLSlogistic-1.png" alt="Linear vs. logistic regression models for binary response data." width="60%">
</div>
<!-- DK: How does the math work here? log(p/1-p) seems, to me, to map 0,1 to 0,infinity. -->
<p>The mathematical function <span class="math inline">\(log\left(\frac{p}{1 - p}\right)\)</span> is called the <em>logit function</em> and it transforms variables from the space <span class="math inline">\((0, 1)\)</span> (like probabilities) to <span class="math inline">\((-\infty, \infty)\)</span>. The inverse of that function, the <em>standard logistic function</em>, is <span class="math inline">\(\frac{1}{1 + e^{-x}}\)</span> and transforms variables from the space <span class="math inline">\((-\infty, \infty)\)</span> to <span class="math inline">\((0, 1)\)</span>. From that latter function’s name we get the terminology of <em>logistic regression</em>.</p>
<p>The process begins with the creation of a workflow object with our model engine.</p>
<div class="sourceCode" id="cb1158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1158-1"><a href="discrete-response.html#cb1158-1"></a>glm_wfl &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1158-2"><a href="discrete-response.html#cb1158-2"></a><span class="st">   </span><span class="kw">add_model</span>(<span class="kw">logistic_reg</span>() <span class="op">%&gt;%</span></span>
<span id="cb1158-3"><a href="discrete-response.html#cb1158-3"></a><span class="st">            </span><span class="kw">set_engine</span>(<span class="st">"glm"</span>) <span class="op">%&gt;%</span></span>
<span id="cb1158-4"><a href="discrete-response.html#cb1158-4"></a><span class="st">            </span><span class="kw">set_mode</span>(<span class="st">"classification"</span>))</span></code></pre></div>
<p>Add a recipe.</p>
<div class="sourceCode" id="cb1159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1159-1"><a href="discrete-response.html#cb1159-1"></a>glm_wfl &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1159-2"><a href="discrete-response.html#cb1159-2"></a><span class="st">  </span><span class="kw">add_model</span>(<span class="kw">logistic_reg</span>() <span class="op">%&gt;%</span></span>
<span id="cb1159-3"><a href="discrete-response.html#cb1159-3"></a><span class="st">            </span><span class="kw">set_engine</span>(<span class="st">"glm"</span>) <span class="op">%&gt;%</span></span>
<span id="cb1159-4"><a href="discrete-response.html#cb1159-4"></a><span class="st">            </span><span class="kw">set_mode</span>(<span class="st">"classification"</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1159-5"><a href="discrete-response.html#cb1159-5"></a><span class="st">  </span><span class="kw">add_recipe</span>(<span class="kw">recipe</span>(approval <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>race,</span>
<span id="cb1159-6"><a href="discrete-response.html#cb1159-6"></a>                    <span class="dt">data =</span> ch12_train) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1159-7"><a href="discrete-response.html#cb1159-7"></a><span class="st">               </span><span class="kw">step_dummy</span>(gender, race) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1159-8"><a href="discrete-response.html#cb1159-8"></a><span class="st">               </span><span class="kw">step_interact</span>(<span class="op">~</span>age<span class="op">:</span><span class="kw">starts_with</span>(<span class="st">"race_"</span>))</span>
<span id="cb1159-9"><a href="discrete-response.html#cb1159-9"></a>             )</span></code></pre></div>
<p>Examine performance on the cross-validation samples.</p>
<div class="sourceCode" id="cb1160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1160-1"><a href="discrete-response.html#cb1160-1"></a>glm_metrics &lt;-<span class="st"> </span>glm_wfl <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1160-2"><a href="discrete-response.html#cb1160-2"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">resamples =</span> ch12_folds) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1160-3"><a href="discrete-response.html#cb1160-3"></a><span class="st">  </span><span class="kw">collect_metrics</span>()</span>
<span id="cb1160-4"><a href="discrete-response.html#cb1160-4"></a></span>
<span id="cb1160-5"><a href="discrete-response.html#cb1160-5"></a>glm_metrics</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 accuracy binary     0.606    10 0.00254
## 2 roc_auc  binary     0.651    10 0.00283</code></pre>
<p>Check the predictions against the actual values.</p>
<div class="sourceCode" id="cb1162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1162-1"><a href="discrete-response.html#cb1162-1"></a>glm_wfl <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1162-2"><a href="discrete-response.html#cb1162-2"></a><span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> ch12_train) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1162-3"><a href="discrete-response.html#cb1162-3"></a><span class="st">  </span><span class="kw">predict</span>(<span class="dt">new_data =</span> ch12_train) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1162-4"><a href="discrete-response.html#cb1162-4"></a><span class="st">  </span><span class="kw">bind_cols</span>(ch12_train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(approval)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1162-5"><a href="discrete-response.html#cb1162-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> approval, <span class="dt">x =</span> <span class="st">`</span><span class="dt">.pred_class</span><span class="st">`</span>)) <span class="op">+</span></span>
<span id="cb1162-6"><a href="discrete-response.html#cb1162-6"></a><span class="st">    </span><span class="kw">geom_jitter</span>(<span class="dt">height =</span> <span class="fl">0.2</span>, <span class="dt">alpha =</span> <span class="fl">0.01</span>) <span class="op">+</span></span>
<span id="cb1162-7"><a href="discrete-response.html#cb1162-7"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">"Predicting Approval"</span>,</span>
<span id="cb1162-8"><a href="discrete-response.html#cb1162-8"></a>         <span class="dt">subtitle =</span> <span class="st">"Using demographic predictors"</span>,</span>
<span id="cb1162-9"><a href="discrete-response.html#cb1162-9"></a>         <span class="dt">x =</span> <span class="st">"Predicted Approval"</span>,</span>
<span id="cb1162-10"><a href="discrete-response.html#cb1162-10"></a>         <span class="dt">y =</span> <span class="st">"Approval"</span></span>
<span id="cb1162-11"><a href="discrete-response.html#cb1162-11"></a>         )</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-895-1.png" width="672"></p>
</div>
<div id="classification-and-regression-trees-cart" class="section level2">
<h2>
<span class="header-section-number">12.4</span> Classification and regression trees (CART)</h2>
<p>Logistic regression is just one of many methods we can use to model binary responses. CART is another approach, which we’ll learn about in this section.</p>
<p>A <strong>tree</strong> is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as <em>nodes</em>. Decision trees predict an outcome variable <span class="math inline">\(Y\)</span> by <em>partitioning</em> the predictors.</p>
<p><strong>Classification trees</strong>, or decision trees, are used in prediction problems where the outcome is categorical. When the outcome is numerical, they are called <strong>regression trees</strong>; hence the acronym <strong>CART</strong>, standing for Classification and Regression Trees. The general idea here is to build a decision tree and, at the end of each <em>node</em>, obtain a predictor <span class="math inline">\(\hat{y}\)</span>. In this case, <span class="math inline">\(\hat{y}\)</span> would identify the likelihood of a voter in that node approving of the President.</p>
<p>But how do we decide on which partitions to make (<span class="math inline">\(R_1, R_2, \ldots, R_J\)</span>) and how do we choose <span class="math inline">\(J\)</span>, the total number of partitions? Here is where the algorithm gets a bit complicated.</p>
<p>Classification trees create partitions recursively. We start the algorithm with one partition in which every observation is classified as either 0 or 1. But after the first step we will have two partitions. After the second step we will split one of these partitions into two and will have three partitions, then four, then five, and so on.</p>
<p>Now, after we define the new partitions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>, and we decide to stop the partitioning process, we compute predictors by taking the most common category of all the observations <span class="math inline">\(y\)</span> for which the associated <span class="math inline">\(\mathbf{x}\)</span> is in <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>. We refer to these two as <span class="math inline">\(\hat{y}_{R_1}\)</span> and <span class="math inline">\(\hat{y}_{R_2}\)</span> respectively.</p>
<p>Once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region.</p>
<p>Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough). Finally, they can model human decision processes. However, in terms of accuracy, they are rarely the best performing method since they are not very flexible. Random forests, explained in the next section, improve on some of the shortcomings of classification trees.</p>
<p>One limitation of CART is its lack of fitted values. Unlike a <code>glm()</code>, you can’t clean predicted probabilities or point estimates from CART. Rather, you simply get a prediction of 0 or 1 for whatever observation you pass through the tree.</p>
<!-- DK: Previous version had a nice graphic showing what a tree looks like. Find another example. -->
<p>First create a workflow object with our model engine.</p>
<div class="sourceCode" id="cb1163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1163-1"><a href="discrete-response.html#cb1163-1"></a>cart_wfl &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1163-2"><a href="discrete-response.html#cb1163-2"></a><span class="st">  </span><span class="kw">add_model</span>(<span class="kw">decision_tree</span>() <span class="op">%&gt;%</span></span>
<span id="cb1163-3"><a href="discrete-response.html#cb1163-3"></a><span class="st">            </span><span class="kw">set_engine</span>(<span class="st">"rpart"</span>,</span>
<span id="cb1163-4"><a href="discrete-response.html#cb1163-4"></a>             <span class="dt">model =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb1163-5"><a href="discrete-response.html#cb1163-5"></a><span class="st">            </span><span class="kw">set_mode</span>(<span class="st">"classification"</span>))</span></code></pre></div>
<p>Add a recipe.</p>
<div class="sourceCode" id="cb1164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1164-1"><a href="discrete-response.html#cb1164-1"></a>cart_wfl &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1164-2"><a href="discrete-response.html#cb1164-2"></a><span class="st">  </span><span class="kw">add_model</span>(<span class="kw">decision_tree</span>() <span class="op">%&gt;%</span></span>
<span id="cb1164-3"><a href="discrete-response.html#cb1164-3"></a><span class="st">            </span><span class="kw">set_engine</span>(<span class="st">"rpart"</span>,</span>
<span id="cb1164-4"><a href="discrete-response.html#cb1164-4"></a>             <span class="dt">model =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb1164-5"><a href="discrete-response.html#cb1164-5"></a><span class="st">            </span><span class="kw">set_mode</span>(<span class="st">"classification"</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1164-6"><a href="discrete-response.html#cb1164-6"></a><span class="st"> </span><span class="kw">add_recipe</span>(<span class="kw">recipe</span>(approval <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>race,</span>
<span id="cb1164-7"><a href="discrete-response.html#cb1164-7"></a>                    <span class="dt">data =</span> ch12_train) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1164-8"><a href="discrete-response.html#cb1164-8"></a><span class="st">               </span><span class="kw">step_dummy</span>(gender, race) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1164-9"><a href="discrete-response.html#cb1164-9"></a><span class="st">               </span><span class="kw">step_interact</span>(<span class="op">~</span>age<span class="op">:</span><span class="kw">starts_with</span>(<span class="st">"race_"</span>) )</span>
<span id="cb1164-10"><a href="discrete-response.html#cb1164-10"></a>             )</span></code></pre></div>
<p>Examine performance on the cross-validation samples.</p>
<div class="sourceCode" id="cb1165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1165-1"><a href="discrete-response.html#cb1165-1"></a>cart_metrics &lt;-<span class="st"> </span>cart_wfl <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1165-2"><a href="discrete-response.html#cb1165-2"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">resamples =</span> ch12_folds) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1165-3"><a href="discrete-response.html#cb1165-3"></a><span class="st">  </span><span class="kw">collect_metrics</span>()</span>
<span id="cb1165-4"><a href="discrete-response.html#cb1165-4"></a></span>
<span id="cb1165-5"><a href="discrete-response.html#cb1165-5"></a>cart_metrics</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 accuracy binary     0.607    10 0.00293
## 2 roc_auc  binary     0.612    10 0.00348</code></pre>
<p>Check the predictions against the actual values.</p>
<div class="sourceCode" id="cb1167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1167-1"><a href="discrete-response.html#cb1167-1"></a>cart_wfl <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1167-2"><a href="discrete-response.html#cb1167-2"></a><span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> ch12_train) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1167-3"><a href="discrete-response.html#cb1167-3"></a><span class="st">  </span><span class="kw">predict</span>(<span class="dt">new_data =</span> ch12_train) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1167-4"><a href="discrete-response.html#cb1167-4"></a><span class="st">  </span><span class="kw">bind_cols</span>(ch12_train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(approval)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1167-5"><a href="discrete-response.html#cb1167-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> approval, <span class="dt">x =</span> <span class="st">`</span><span class="dt">.pred_class</span><span class="st">`</span>)) <span class="op">+</span></span>
<span id="cb1167-6"><a href="discrete-response.html#cb1167-6"></a><span class="st">    </span><span class="kw">geom_jitter</span>(<span class="dt">height =</span> <span class="fl">0.2</span>, <span class="dt">alpha =</span> <span class="fl">0.01</span>) <span class="op">+</span></span>
<span id="cb1167-7"><a href="discrete-response.html#cb1167-7"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">"Predicting Approval"</span>,</span>
<span id="cb1167-8"><a href="discrete-response.html#cb1167-8"></a>        <span class="dt">subtitle =</span> <span class="st">"Using demographic predictors"</span>,</span>
<span id="cb1167-9"><a href="discrete-response.html#cb1167-9"></a>         <span class="dt">x =</span> <span class="st">"Predicted Approval"</span>,</span>
<span id="cb1167-10"><a href="discrete-response.html#cb1167-10"></a>         <span class="dt">y =</span> <span class="st">"Approval"</span></span>
<span id="cb1167-11"><a href="discrete-response.html#cb1167-11"></a>         )</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-899-1.png" width="672"></p>
</div>
<div id="random-forests" class="section level2">
<h2>
<span class="header-section-number">12.5</span> Random forests</h2>
<p>Random forests are a very popular machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by <em>averaging</em> multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.</p>
<p>The first step is <em>bootstrap aggregation</em> or <em>bagging</em>. The general idea is to generate many predictors, each using classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees <strong>randomly</strong> different, and the combination of trees is the <strong>forest</strong>. The specific steps are as follows.</p>
<ol style="list-style-type: decimal">
<li><p>Build decision trees using a portion of the data called the training set. We refer to the fitted models as <span class="math inline">\(T_1, T_2, \dots, T_B\)</span>. We later explain how we ensure they are different.</p></li>
<li><p>For every observation in the test set, form a prediction <span class="math inline">\(\hat{y}_j\)</span> using tree <span class="math inline">\(T_j\)</span>.</p></li>
<li><p>For categorical data classification, predict <span class="math inline">\(\hat{y}\)</span> with majority vote (most frequent class among <span class="math inline">\(\hat{y}_1, \dots, \hat{y}_T\)</span>).</p></li>
</ol>
<p>So how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let <span class="math inline">\(N\)</span> be the number of observations in the training set. To create <span class="math inline">\(T_j, \, j=1,\ldots,B\)</span> from the training set we do the following:</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Create a bootstrap training set by sampling <span class="math inline">\(N\)</span> observations from the training set <strong>with replacement</strong>. This is the first way to induce randomness.</p></li>
<li><p>A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy.</p></li>
</ol>
<p>First create a workflow object with our model engine.</p>
<div class="sourceCode" id="cb1168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1168-1"><a href="discrete-response.html#cb1168-1"></a>rf_wfl &lt;-<span class="kw">workflow</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1168-2"><a href="discrete-response.html#cb1168-2"></a><span class="st">  </span><span class="kw">add_model</span>(<span class="kw">rand_forest</span>() <span class="op">%&gt;%</span></span>
<span id="cb1168-3"><a href="discrete-response.html#cb1168-3"></a><span class="st">            </span><span class="kw">set_engine</span>(<span class="st">"randomForest"</span>) <span class="op">%&gt;%</span></span>
<span id="cb1168-4"><a href="discrete-response.html#cb1168-4"></a><span class="st">             </span><span class="kw">set_mode</span>(<span class="st">"classification"</span>))</span></code></pre></div>
<p>Add a recipe.</p>
<div class="sourceCode" id="cb1169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1169-1"><a href="discrete-response.html#cb1169-1"></a>rf_wfl &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1169-2"><a href="discrete-response.html#cb1169-2"></a><span class="st">  </span><span class="kw">add_model</span>(<span class="kw">rand_forest</span>() <span class="op">%&gt;%</span></span>
<span id="cb1169-3"><a href="discrete-response.html#cb1169-3"></a><span class="st">            </span><span class="kw">set_engine</span>(<span class="st">"randomForest"</span>) <span class="op">%&gt;%</span></span>
<span id="cb1169-4"><a href="discrete-response.html#cb1169-4"></a><span class="st">             </span><span class="kw">set_mode</span>(<span class="st">"classification"</span>)) <span class="op">%&gt;%</span><span class="st">  </span></span>
<span id="cb1169-5"><a href="discrete-response.html#cb1169-5"></a><span class="st">  </span><span class="kw">add_recipe</span>(<span class="kw">recipe</span>(approval <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>race,</span>
<span id="cb1169-6"><a href="discrete-response.html#cb1169-6"></a>                    <span class="dt">data =</span> ch12_train) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1169-7"><a href="discrete-response.html#cb1169-7"></a><span class="st">               </span><span class="kw">step_dummy</span>(gender, race) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1169-8"><a href="discrete-response.html#cb1169-8"></a><span class="st">               </span><span class="kw">step_interact</span>(<span class="op">~</span>age<span class="op">:</span><span class="kw">starts_with</span>(<span class="st">"race_"</span>) )</span>
<span id="cb1169-9"><a href="discrete-response.html#cb1169-9"></a>             )</span></code></pre></div>
<p>Examine performance on the cross-validation samples.</p>
<div class="sourceCode" id="cb1170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1170-1"><a href="discrete-response.html#cb1170-1"></a>rf_metrics &lt;-<span class="st"> </span>rf_wfl <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1170-2"><a href="discrete-response.html#cb1170-2"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">resamples =</span> ch12_folds) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1170-3"><a href="discrete-response.html#cb1170-3"></a><span class="st">  </span><span class="kw">collect_metrics</span>()</span>
<span id="cb1170-4"><a href="discrete-response.html#cb1170-4"></a></span>
<span id="cb1170-5"><a href="discrete-response.html#cb1170-5"></a>rf_metrics</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 accuracy binary     0.609    10 0.00255
## 2 roc_auc  binary     0.651    10 0.00228</code></pre>
<p>Check the predictions against the actual values.</p>
<div class="sourceCode" id="cb1172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1172-1"><a href="discrete-response.html#cb1172-1"></a>rf_wfl <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1172-2"><a href="discrete-response.html#cb1172-2"></a><span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> ch12_train) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1172-3"><a href="discrete-response.html#cb1172-3"></a><span class="st">  </span><span class="kw">predict</span>(<span class="dt">new_data =</span> ch12_train) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1172-4"><a href="discrete-response.html#cb1172-4"></a><span class="st">  </span><span class="kw">bind_cols</span>(ch12_train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(approval)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1172-5"><a href="discrete-response.html#cb1172-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> approval, <span class="dt">x =</span> <span class="st">`</span><span class="dt">.pred_class</span><span class="st">`</span>)) <span class="op">+</span></span>
<span id="cb1172-6"><a href="discrete-response.html#cb1172-6"></a><span class="st">    </span><span class="kw">geom_jitter</span>(<span class="dt">height =</span> <span class="fl">0.2</span>, <span class="dt">alpha =</span> <span class="fl">0.01</span>) <span class="op">+</span></span>
<span id="cb1172-7"><a href="discrete-response.html#cb1172-7"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">"Predicting Approval"</span>,</span>
<span id="cb1172-8"><a href="discrete-response.html#cb1172-8"></a>         <span class="dt">subtitle =</span> <span class="st">"Using demographic predictors"</span>,</span>
<span id="cb1172-9"><a href="discrete-response.html#cb1172-9"></a>         <span class="dt">x =</span> <span class="st">"Predicted Approval"</span>,</span>
<span id="cb1172-10"><a href="discrete-response.html#cb1172-10"></a>         <span class="dt">y =</span> <span class="st">"Approval"</span></span>
<span id="cb1172-11"><a href="discrete-response.html#cb1172-11"></a>         )</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-903-1.png" width="672"></p>
</div>
<div id="cardinal-virtues-2" class="section level2">
<h2>
<span class="header-section-number">12.6</span> Cardinal Virtues</h2>
<!-- DK: Make this aprt like Chapter 11. Put model comparison under courage. Once we choose a model then, under Temperance, we use all our data to fit it. Then we use the resulting _fit object to answer our question. -->
<div id="comparing-models" class="section level3">
<h3>
<span class="header-section-number">12.6.1</span> Comparing Models</h3>
<p>Recall that the most common method for deciding which model to choose is to look at which one does the best predicting out of sample. Below is a summary of how well our three different models performed.</p>
<div class="sourceCode" id="cb1173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1173-1"><a href="discrete-response.html#cb1173-1"></a><span class="kw">tibble</span>(<span class="dt">model =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="st">"Logistic Regression"</span>, </span>
<span id="cb1173-2"><a href="discrete-response.html#cb1173-2"></a>                     <span class="st">"Classification and Regression Tree"</span>, </span>
<span id="cb1173-3"><a href="discrete-response.html#cb1173-3"></a>                     <span class="st">"Random Forest"</span>), <span class="dt">each =</span> <span class="dv">2</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1173-4"><a href="discrete-response.html#cb1173-4"></a><span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">bind_rows</span>(glm_metrics, cart_metrics, rf_metrics)) </span></code></pre></div>
<pre><code>## # A tibble: 6 x 6
##   model                              .metric  .estimator  mean     n std_err
##   &lt;chr&gt;                              &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 Logistic Regression                accuracy binary     0.606    10 0.00254
## 2 Logistic Regression                roc_auc  binary     0.651    10 0.00283
## 3 Classification and Regression Tree accuracy binary     0.607    10 0.00293
## 4 Classification and Regression Tree roc_auc  binary     0.612    10 0.00348
## 5 Random Forest                      accuracy binary     0.609    10 0.00255
## 6 Random Forest                      roc_auc  binary     0.651    10 0.00228</code></pre>
<!-- BG: This code returns an error. -->
<!-- BG: once code errors are sorted, talk about results here -->

</div>
</div>
</div></body></html>

<p style="text-align: center;">
<a href="continuous-response.html"><button class="btn btn-default">Previous</button></a>
<a href="appendices.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-11-13
</p>
</div>
</div>



</body>
</html>
