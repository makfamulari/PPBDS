<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 10 Model Choice | Gov 50: Data" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="davidkane9/PPBDS" />



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Chapter 10 Model Choice | Gov 50: Data">

<title>Chapter 10 Model Choice | Gov 50: Data</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/str_view-0.1.0/str_view.css" rel="stylesheet" />
<script src="libs/str_view-binding-1.4.0/str_view.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Gov 50: Data<p><p class="author"></p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html"></a>
<a href="preamble.html">Preamble</a>
<a href="shopping-week.html">Shopping Week</a>
<a href="visualization.html"><span class="toc-section-number">1</span> Visualization</a>
<a href="wrangling.html"><span class="toc-section-number">2</span> Wrangling</a>
<a href="rubin-causal-model.html"><span class="toc-section-number">3</span> Rubin Causal Model</a>
<a href="functions.html"><span class="toc-section-number">4</span> Functions</a>
<a href="probability.html"><span class="toc-section-number">5</span> Probability</a>
<a href="one-parameter.html"><span class="toc-section-number">6</span> One Parameter</a>
<a href="two-parameters.html"><span class="toc-section-number">7</span> Two Parameters</a>
<a href="three-parameters.html"><span class="toc-section-number">8</span> Three Parameters</a>
<a href="n-parameters.html"><span class="toc-section-number">9</span> N Parameters</a>
<a id="active-page" href="model-choice.html"><span class="toc-section-number">10</span> Model Choice</a><ul class="toc-sections">
<li class="toc"><a href="#ames"> The Ames housing data</a></li>
<li class="toc"><a href="#splitting"> Spending our data</a></li>
<li class="toc"><a href="#recipes"> Feature engineering with recipes</a></li>
<li class="toc"><a href="#models"> Fitting models with parsnip</a></li>
<li class="toc"><a href="#workflows"> A model workflow</a></li>
<li class="toc"><a href="#performance"> Judging model effectiveness</a></li>
<li class="toc"><a href="#compare"> Comparing models with resampling</a></li>
<li class="toc"><a href="#conclusion"> Conclusion</a></li>
</ul>
<a href="continuous-response.html"><span class="toc-section-number">11</span> Continuous Response</a>
<a href="discrete-response.html"><span class="toc-section-number">12</span> Discrete Response</a>
<a href="appendices.html">Appendices</a>
<a href="tools.html">Tools</a>
<a href="shiny.html">Shiny</a>
<a href="maps.html">Maps</a>
<a href="animation.html">Animation</a>
<a href="references.html">References</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="model-choice" class="section level1">
<h1>
<span class="header-section-number">Chapter 10</span> Model Choice</h1>
<p>There are many models we could make to explain a given outcome variable. The chapter provides the tools for exploring which model we “ought” to use. We will use these tools in Chapters <a href="continuous-response.html#continuous-response">11</a> (for continuous outcomes) and <a href="discrete-response.html#discrete-response">12</a> (for discrete outcomes).</p>
<div id="ames" class="section level2">
<h2>
<span class="header-section-number">10.1</span> The Ames housing data</h2>
<p>Packages:</p>
<div class="sourceCode" id="cb1012"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1012-1"><a href="model-choice.html#cb1012-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1012-2"><a href="model-choice.html#cb1012-2"></a><span class="kw">library</span>(tidymodels)</span>
<span id="cb1012-3"><a href="model-choice.html#cb1012-3"></a><span class="kw">library</span>(ranger)</span>
<span id="cb1012-4"><a href="model-choice.html#cb1012-4"></a><span class="kw">data</span>(ames)</span></code></pre></div>
<p>The <strong>tidymodels</strong> library includes all the necessary libraries, in the same way that <strong>tidyverse</strong> includes libraries like <strong>ggplot2</strong> and <strong>dplyr</strong>. The <code>ames</code> data lives in the <strong>modeldata</strong> library, which is part of <strong>tidymodels</strong>. The <strong>ranger</strong> package is used for creating <em>regression trees</em>, a type of model which we will use in Chapter <a href="discrete-response.html#discrete-response">12</a>.</p>
<p>The Ames housing data set is an excellent resource for learning about models. It contains data on 2,930 properties in Ames, Iowa, including columns related to</p>
<ul>
<li>house characteristics (bedrooms, garage, fireplace, pool, porch, etc.),</li>
<li>location (neighborhood),</li>
<li>lot information (zoning, shape, size, etc.),</li>
<li>ratings of condition and quality, and</li>
<li>sale price.</li>
</ul>
<p>It makes sense to start with the outcome we want to predict: the last sale price of the house (in USD):</p>
<!-- DK: Clean up these plots? Why not? -->
<div class="sourceCode" id="cb1013"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1013-1"><a href="model-choice.html#cb1013-1"></a><span class="kw">ggplot</span>(ames, <span class="kw">aes</span>(<span class="dt">x =</span> Sale_Price)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1013-2"><a href="model-choice.html#cb1013-2"></a><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">50</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-806-1.png" width="672"></p>
<p>The data are right-skewed; there are more inexpensive houses than expensive ones. The median sale price was $160,000 and the most expensive house was $755,000. When modeling this outcome, a strong argument can be made that the price should be log-transformed. The advantages of doing this are that no houses would be predicted with negative sale prices and that errors in predicting expensive houses will not have an undue influence on the model. Also, from a statistical perspective, a logarithmic transform may also <em>stabilize the variance</em> in a way that makes inference more legitimate. Let’s visualize the transformed data:</p>
<!-- DK: Do we need to discuss logs, either here or earlier in the book? -->
<div class="sourceCode" id="cb1014"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1014-1"><a href="model-choice.html#cb1014-1"></a><span class="kw">ggplot</span>(ames, <span class="kw">aes</span>(<span class="dt">x =</span> Sale_Price)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1014-2"><a href="model-choice.html#cb1014-2"></a><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">50</span>) <span class="op">+</span></span>
<span id="cb1014-3"><a href="model-choice.html#cb1014-3"></a><span class="st">  </span><span class="kw">scale_x_log10</span>()</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-807-1.png" width="672"></p>
<p>While not perfect, this will probably result in better models than using the untransformed data.</p>
<p>The units of the model coefficients might be more difficult to interpret, as will measures of performance. For example, the root mean squared error (RMSE) is a common performance metric that is used in regression models. It uses the difference between the observed and predicted values in its calculations. If the sale price is on the log scale, these differences (i.e. the residuals) are also in log units. For this reason, it can be difficult to understand the quality of a model whose RMSE is 0.15 log units.</p>
<p>Despite these drawbacks, we will utilize the log transformation for this outcome. <em>From this point on</em>, the outcome column is pre-logged in the <code>ames</code> data frame:</p>
<!-- DK: Should we just define this as log in the original data? Would be one less thing for students to deal with. -->
<div class="sourceCode" id="cb1015"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1015-1"><a href="model-choice.html#cb1015-1"></a>ames &lt;-<span class="st"> </span>ames <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1015-2"><a href="model-choice.html#cb1015-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Sale_Price =</span> <span class="kw">log10</span>(Sale_Price))</span></code></pre></div>
<!-- DK: Delete this interesting geographic stuff? It is good but the chapter is too long. -->
<p>Another important aspect of these data for our modeling are their geographic locations. This spatial information is contained in the data in two ways: a qualitative <code>Neighborhood</code> label as well as quantitative longitude and latitude data. To visualize the spatial information, let’s use both together to plot the data on a map and color by neighborhood:</p>
<p><img src="10-model-choice/images/ames.png" width="1230"></p>
<p>We can see a few noticeable patterns. First, there is a void of data points in the center of Ames. This corresponds to Iowa State University. Second, while there are a number of neighborhoods that are geographically isolated, there are others that are adjacent to each other. For example, Timberland is located apart from almost all other neighborhoods:</p>
<p><img src="10-model-choice/images/timberland.png" width="488"></p>
<p>The Meadow Village neighborhood in Southwest Ames is like an island of properties ensconced inside the sea of properties that make up the Mitchell neighborhood:</p>
<p><img src="10-model-choice/images/mitchell.png" width="710"></p>
<p>A detailed inspection of the map also shows that the neighborhood labels are not completely reliable. For example, there are some properties labeled as being in Northridge that are surrounded by houses in the adjacent Somerset neighborhood:</p>
<p><img src="10-model-choice/images/northridge.png" width="580"></p>
<p>Also, there are ten isolated houses labeled as being in Crawford but which are not close to the majority of the other houses in that neighborhood:</p>
<p><img src="10-model-choice/images/crawford.png" width="460"></p>
<p>Also notable is the “Iowa Department of Transportation (DOT) and Rail Road” neighborhood adjacent to the main road on the east side of Ames. There are several clusters of houses within this neighborhood as well as some longitudinal outliers; the two houses furthest east are isolated from the other locations.</p>
<p><img src="10-model-choice/images/dot_rr.png" width="792"></p>
<p>It is <em>critical</em> to conduct <em>exploratory data analysis</em> prior to beginning any modeling. These housing data have characteristics that present interesting challenges about how the data should be processed and modeled. We describe many of these in later sections. Some basic questions that could be examined include:</p>
<ul>
<li><p>Are there any odd or noticeable things about the distributions of the individual predictors? Is there much skewness or any pathological distributions?</p></li>
<li><p>Are there high correlations between predictors? For example, there are multiple predictors related to the size of the house. Are some redundant?</p></li>
<li><p>Are there associations between predictors and the outcomes?</p></li>
</ul>
<p>Many of these questions will be revisited as these data are used in upcoming examples.</p>
</div>
<div id="splitting" class="section level2">
<h2>
<span class="header-section-number">10.2</span> Spending our data</h2>
<p>There are several steps to create a useful model, including parameter estimation, model selection and tuning, and performance assessment. At the start of a new project, there is usually an initial finite pool of data available for all these tasks. How should the data be applied to these steps? The idea of <em>data spending</em> is an important first consideration when modeling, especially as it relates to empirical validation.</p>
<p>When there are copious amounts of data available, a smart strategy is to allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only. There may be questions about many modeling project steps that must be answered with limited prior knowledge. For example, one possible strategy (when both data and predictors are abundant) is to spend a specific subset of data to determine which predictors are informative, before considering parameter estimation at all.</p>
<p>If the initial pool of data available is not huge, there will be some overlap of how and when our data is “spent” or allocated, and a solid methodology for data spending is important. Here, we demonstrate the basics of <em>splitting</em> our initial pool of samples for different purposes.</p>
<div id="splitting-methods" class="section level3">
<h3>
<span class="header-section-number">10.2.1</span> Common methods for splitting data</h3>
<p>The primary approach for empirical model validation is to split the existing pool of data into two distinct sets. Some observations are used to develop and optimize the model. This <em>training set</em> is usually the majority of the data. These data are a sandbox for model building where different models can be fit, feature engineering strategies are investigated, and so on. We as modeling practitioners spend the vast majority of the modeling process using the training set as the substrate to develop the model.</p>
<p>The other portion of the observations are placed into the <em>test set</em>. This is held in reserve until one or two models are chosen as the methods that are mostly likely to succeed. The test set is then used as the final arbiter to determine the efficacy of the model. It is critical to only look at the test set once; otherwise, it becomes part of the modeling process.</p>
<p>Suppose we allocate 80% of the data to the training set and the remaining 20% for testing. The most common method is to use simple random sampling. The <strong>rsample</strong> package has tools for making data splits such as this; the function <code>intial_split()</code> was created for this purpose. It takes the data frame as an argument as well as the proportion to be placed into training. We use <code>set.seed()</code> so that the results can be reproduced later. Create a data split which we will use throughout this chapter.</p>
<div class="sourceCode" id="cb1016"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1016-1"><a href="model-choice.html#cb1016-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1016-2"><a href="model-choice.html#cb1016-2"></a></span>
<span id="cb1016-3"><a href="model-choice.html#cb1016-3"></a>ames_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(ames, <span class="dt">prob =</span> <span class="fl">0.80</span>)</span>
<span id="cb1016-4"><a href="model-choice.html#cb1016-4"></a>ames_split</span></code></pre></div>
<pre><code>## &lt;Analysis/Assess/Total&gt;
## &lt;2198/732/2930&gt;</code></pre>
<p>The printed information denotes the amount of data in the training set (<span class="math inline">\(n = 2,198\)</span>), the amount in the test set (<span class="math inline">\(n = 732\)</span>), and the size of the original pool of samples (<span class="math inline">\(n = 2,930\)</span>).</p>
<p>The object <code>ames_split</code> is an <code>rsplit</code> object and only contains the partitioning information; to get the resulting data sets, we apply two more functions:</p>
<div class="sourceCode" id="cb1018"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1018-1"><a href="model-choice.html#cb1018-1"></a>ames_train &lt;-<span class="st"> </span><span class="kw">training</span>(ames_split)</span>
<span id="cb1018-2"><a href="model-choice.html#cb1018-2"></a>ames_test  &lt;-<span class="st">  </span><span class="kw">testing</span>(ames_split)</span>
<span id="cb1018-3"><a href="model-choice.html#cb1018-3"></a></span>
<span id="cb1018-4"><a href="model-choice.html#cb1018-4"></a><span class="kw">dim</span>(ames_train)</span></code></pre></div>
<pre><code>## [1] 2198   74</code></pre>
<p>These objects are data frames with the same <em>columns</em> as the original data but only the appropriate <em>rows</em> for each set.</p>
<p>The amount of data that should be allocated when splitting the data is highly dependent on the context of the problem at hand. Too much data in the training set lowers the quality of the performance estimates. Conversely, too much data in the test set handicaps the model’s ability to find appropriate parameter estimates. There are parts of the statistics community that eschew test sets in general because they believe all of the data should be used for parameter estimation. While there is merit to this argument, it is good modeling practice to have an unbiased set of observations as the final arbiter of model quality.</p>
</div>
</div>
<div id="recipes" class="section level2">
<h2>
<span class="header-section-number">10.3</span> Feature engineering with recipes</h2>
<p>Feature engineering encompasses activities that reformat predictor values to make them easier for a model to use effectively. This includes transformations and encodings of the data to best represent their important characteristics. Imagine that you have two predictors in a data set that can be more effectively represented in your model of interest as a ratio; creating a new predictor from the ratio of the original two is a simple example of feature engineering.</p>
<p>Take the location of a house in Ames as a more involved example. There are a variety of ways that this spatial information can be exposed to a model, including neighborhood (a qualitative measure), longitude/latitude, distance to the nearest school or Iowa State University, and so on. When choosing how to encode these data in modeling, we might choose an option we believe most associated with the outcome. The original format of the data (e.g., numeric like distance versus categorical like neighborhood) is also a driving factor in feature engineering choices.</p>
<p>There are many other examples of preprocessing to build better features for modeling:</p>
<ul>
<li><p>Correlation between predictors can be reduced via feature extraction or the removal of some predictors.</p></li>
<li><p>When some predictors have missing values, they can be imputed using a sub-model.</p></li>
<li><p>Models that use variance-type measures may benefit from coercing the distribution of some skewed predictors to be symmetric by estimating a transformation.</p></li>
</ul>
<p>Feature engineering and data preprocessing can also involve reformatting <em>required</em> by the model. Some models use geometric distance metrics and, consequently, numeric predictors should be centered and scaled so that they are all in the same units. Otherwise, the distance values would be biased by the scale of each column.</p>
<p>The <strong>recipes</strong> package combines different feature engineering and preprocessing tasks into a single object and then applies these transformations to different data sets.</p>
<div id="a-simple-recipe-for-the-ames-housing-data" class="section level3">
<h3>
<span class="header-section-number">10.3.1</span> A simple recipe for the Ames housing data</h3>
<p>In this section, we will focus on a small subset of the predictors available in the Ames housing data:</p>
<ul>
<li><p>The neighborhood (qualitative, with 29 neighborhoods in the training set)</p></li>
<li><p>The general living area (continuous, named <code>Gr_Liv_Area</code>)</p></li>
<li><p>The year built (<code>Year_Built</code>)</p></li>
<li><p>The type of building (<code>Bldg_Type</code>)</p></li>
</ul>
<!-- DK: Introduce lm here? Or earlier? --><p>Suppose that an initial ordinary linear regression model were fit to these data. A standard call to <code>lm()</code>, a function similar to <code>stan_glm()</code>, might look like:</p>
<div class="sourceCode" id="cb1020"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1020-1"><a href="model-choice.html#cb1020-1"></a><span class="kw">lm</span>(Sale_Price <span class="op">~</span><span class="st"> </span>Neighborhood <span class="op">+</span><span class="st"> </span><span class="kw">log10</span>(Gr_Liv_Area) <span class="op">+</span><span class="st"> </span>Year_Built <span class="op">+</span><span class="st"> </span>Bldg_Type)</span></code></pre></div>
<p>What the formula above does can be decomposed into a series of <em>steps</em>:</p>
<ol style="list-style-type: decimal">
<li><p>Sale price is defined as the outcome while neighborhood, general living area, the year built, and building type variables are all defined as predictors.</p></li>
<li><p>A log transformation is applied to the general living area predictor.</p></li>
<li><p>The neighborhood and building type columns are converted from a non-numeric format to a numeric format (since least squares requires numeric predictors).</p></li>
</ol>
<p>The formula method will apply these data manipulations to any data, including new data, that are passed to the <code>predict()</code> function.</p>
<p>A recipe is also an object that defines a series of steps for data processing. Unlike the formula method inside a modeling function, the recipe defines the steps without immediately executing them; it is only a specification of what <em>should</em> be done. Here is a recipe equivalent to the formula above:</p>
<div class="sourceCode" id="cb1021"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1021-1"><a href="model-choice.html#cb1021-1"></a><span class="kw">library</span>(tidymodels) <span class="co"># Includes the recipes package</span></span>
<span id="cb1021-2"><a href="model-choice.html#cb1021-2"></a></span>
<span id="cb1021-3"><a href="model-choice.html#cb1021-3"></a>simple_ames &lt;-<span class="st"> </span></span>
<span id="cb1021-4"><a href="model-choice.html#cb1021-4"></a><span class="st">  </span><span class="kw">recipe</span>(Sale_Price <span class="op">~</span><span class="st"> </span>Neighborhood <span class="op">+</span><span class="st"> </span>Gr_Liv_Area <span class="op">+</span><span class="st"> </span></span>
<span id="cb1021-5"><a href="model-choice.html#cb1021-5"></a><span class="st">           </span>Year_Built <span class="op">+</span><span class="st"> </span>Bldg_Type,</span>
<span id="cb1021-6"><a href="model-choice.html#cb1021-6"></a>         <span class="dt">data =</span> ames_train) <span class="op">%&gt;%</span></span>
<span id="cb1021-7"><a href="model-choice.html#cb1021-7"></a><span class="st">  </span><span class="kw">step_log</span>(Gr_Liv_Area, <span class="dt">base =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1021-8"><a href="model-choice.html#cb1021-8"></a><span class="st">  </span><span class="kw">step_dummy</span>(<span class="kw">all_nominal</span>())</span>
<span id="cb1021-9"><a href="model-choice.html#cb1021-9"></a>simple_ames</span></code></pre></div>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          4
## 
## Operations:
## 
## Log transformation on Gr_Liv_Area
## Dummy variables from all_nominal()</code></pre>
<p>Let’s break this down:</p>
<ol style="list-style-type: decimal">
<li><p>The call to <code>recipe()</code> with a formula tells the recipe the <em>roles</em> of the variables (e.g., predictor, outcome). It only uses the data to determine the data types for the columns.</p></li>
<li><p><code>step_log()</code> declares that <code>Gr_Liv_Area</code> should be log transformed.</p></li>
<li><p><code>step_dummy()</code> is used to specify which variables should be converted from a qualitative format to a quantitative format, in this case, using dummy or indicator variables. An indicator or dummy variable is a binary numeric variable (a column of ones and zeroes) that encodes qualitative information; we will dig deeper into these kinds of variables in Section <a href="model-choice.html#dummies">10.3.3</a>.</p></li>
</ol>
<p>The function <code>all_nominal()</code> captures the names of any columns that are currently factor or character (i.e., nominal) in nature. This is a <strong>dplyr</strong> selector function similar to <code>starts_with()</code> or <code>matches()</code> but which can only be used inside of a recipe.</p>
<p>What is the advantage to using a recipe? There are a few, including:</p>
<ul>
<li><p>These computations can be recycled across models since they are not tightly coupled to the modeling function.</p></li>
<li><p>A recipe enables a broader set of data processing choices than formulas can offer.</p></li>
<li><p>The syntax can be very compact. For example, <code>all_nominal()</code> can be used to capture many variables for specific types of processing while a formula would require each to be explicitly listed.</p></li>
<li><p>All data processing can be captured in a single R object instead of in scripts that are repeated, or even spread across different files.</p></li>
</ul>
</div>
<div id="using-recipes" class="section level3">
<h3>
<span class="header-section-number">10.3.2</span> Using recipes</h3>
<p>Remember that when invoking the <code>recipe()</code> function, the steps are not estimated or executed in any way. The second phase for using a recipe is to estimate any quantities required by the steps using the <code>prep()</code> function. For example, we can use <code>step_normalize()</code> to center and scale any predictors selected in the step. When we call <code>prep(recipe, training)</code>, this function estimates the required means and standard deviations from the data in the <code>training</code> argument. The transformations specified by each step are also sequentially executed on the data set. Again using normalization as the example, the means and variances are estimated and then used to standardize the columns.</p>
<p>For our example recipe, we can now <code>prep()</code>:</p>
<div class="sourceCode" id="cb1023"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1023-1"><a href="model-choice.html#cb1023-1"></a>simple_ames &lt;-<span class="st"> </span><span class="kw">prep</span>(simple_ames, </span>
<span id="cb1023-2"><a href="model-choice.html#cb1023-2"></a>                    <span class="dt">training =</span> ames_train)</span>
<span id="cb1023-3"><a href="model-choice.html#cb1023-3"></a>simple_ames</span></code></pre></div>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          4
## 
## Training data contained 2198 data points and no missing data.
## 
## Operations:
## 
## Log transformation on Gr_Liv_Area [trained]
## Dummy variables from Neighborhood, Bldg_Type [trained]</code></pre>
<p>Note that, after preparing the recipe, the print statement shows the results of the selectors (e.g., <code>Neighborhood</code> and <code>Bldg_Type</code> are listed instead of <code>all_nominal</code>).</p>
<p>The third phase of recipe usage is to apply the preprocessing operations to a data set using the <code>bake()</code> function. The <code>bake()</code> function can apply the recipe to <em>any</em> data set. To use the test set, the syntax would be:</p>
<!--DK: Fix the NULL problem! The book uses NULL in place of ames_test. Is that an error? Is using ames_test OK? I am not sure! -->
<div class="sourceCode" id="cb1025"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1025-1"><a href="model-choice.html#cb1025-1"></a>test_ex &lt;-<span class="st"> </span><span class="kw">bake</span>(simple_ames, <span class="dt">new_data =</span> ames_test)</span>
<span id="cb1025-2"><a href="model-choice.html#cb1025-2"></a><span class="kw">names</span>(test_ex) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1025-3"><a href="model-choice.html#cb1025-3"></a><span class="st">  </span><span class="kw">head</span>()</span></code></pre></div>
<pre><code>## [1] "Gr_Liv_Area"                "Year_Built"                
## [3] "Sale_Price"                 "Neighborhood_College_Creek"
## [5] "Neighborhood_Old_Town"      "Neighborhood_Edwards"</code></pre>
<p>Note the dummy variable columns starting with <code>Neighborhood_</code>. The <code>bake()</code> function can also take selectors so that, if we only wanted the neighborhood results, we could use:</p>
<!-- DK: When do we explain starts_with() and friends? Maybe there is a preliminaries section for each chapter which teaches some more cool stuff? -->
<div class="sourceCode" id="cb1027"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1027-1"><a href="model-choice.html#cb1027-1"></a><span class="kw">bake</span>(simple_ames, </span>
<span id="cb1027-2"><a href="model-choice.html#cb1027-2"></a>     ames_test, </span>
<span id="cb1027-3"><a href="model-choice.html#cb1027-3"></a>     <span class="kw">starts_with</span>(<span class="st">"Neighborhood_"</span>))</span></code></pre></div>
<p>To get the processed version of the training set, we use <code>bake()</code> and pass in the argument <code>ames_train</code>.</p>
<!-- DK: Same NULL question as above. Or could this just be cut. -->
<div class="sourceCode" id="cb1028"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1028-1"><a href="model-choice.html#cb1028-1"></a><span class="kw">bake</span>(simple_ames, <span class="dt">new_data =</span> ames_train) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1028-2"><a href="model-choice.html#cb1028-2"></a><span class="st">  </span><span class="kw">nrow</span>()</span></code></pre></div>
<pre><code>## [1] 2198</code></pre>
<p>To reiterate, using a recipe is a three phase process summarized as:</p>
<p><img src="10-model-choice/images/recipes-process.svg"></p>
</div>
<div id="dummies" class="section level3">
<h3>
<span class="header-section-number">10.3.3</span> Encoding qualitative data in a numeric format</h3>
<p>One of the most common feature engineering tasks is transforming nominal or qualitative data (factors or characters) so that they can be encoded or represented numerically. Sometimes we can alter the factor levels of a qualitative column in helpful ways <em>prior</em> to such a transformation. For example, <code>step_unknown()</code> can be used to change missing values to a dedicated factor level. Similarly, if we anticipate that a new factor level may be encountered in future data, <code>step_novel()</code> can allot a new level for this purpose.</p>
<p>Additionally, <code>step_other()</code> can be used to analyze the frequencies of the factor levels in the training set and convert infrequently occurring values to a catch-all level of “other”, with a specific threshold that can be specified. A good example is the <code>Neighborhood</code> predictor in our data:</p>
<div class="sourceCode" id="cb1030"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1030-1"><a href="model-choice.html#cb1030-1"></a><span class="kw">ggplot</span>(ames_train, <span class="kw">aes</span>(<span class="dt">y =</span> Neighborhood)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1030-2"><a href="model-choice.html#cb1030-2"></a><span class="st">  </span><span class="kw">geom_bar</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb1030-3"><a href="model-choice.html#cb1030-3"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="ot">NULL</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-824-1.png" width="672"></p>
<p>Here there are two neighborhoods that have less than five properties in the training data; in this case, no houses at all in the Landmark neighborhood were included in the training set. For some models, it may be problematic to have dummy variables with a single non-zero entry in the column. At a minimum, it is highly improbable that these features would be important to a model. If we add <code>step_other(Neighborhood, threshold = 0.01)</code> to our recipe, the bottom 1% of the neighborhoods will be lumped into a new level called “other”. In this training set, this will catch 8 neighborhoods.</p>
<p>For the Ames data, we can amend the recipe to use:</p>
<div class="sourceCode" id="cb1031"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1031-1"><a href="model-choice.html#cb1031-1"></a>simple_ames &lt;-<span class="st"> </span></span>
<span id="cb1031-2"><a href="model-choice.html#cb1031-2"></a><span class="st">  </span><span class="kw">recipe</span>(Sale_Price <span class="op">~</span><span class="st"> </span>Neighborhood <span class="op">+</span><span class="st"> </span>Gr_Liv_Area <span class="op">+</span><span class="st"> </span>Year_Built <span class="op">+</span><span class="st"> </span>Bldg_Type,</span>
<span id="cb1031-3"><a href="model-choice.html#cb1031-3"></a>         <span class="dt">data =</span> ames_train) <span class="op">%&gt;%</span></span>
<span id="cb1031-4"><a href="model-choice.html#cb1031-4"></a><span class="st">  </span><span class="kw">step_log</span>(Gr_Liv_Area, <span class="dt">base =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1031-5"><a href="model-choice.html#cb1031-5"></a><span class="st">  </span><span class="kw">step_other</span>(Neighborhood, <span class="dt">threshold =</span> <span class="fl">0.01</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1031-6"><a href="model-choice.html#cb1031-6"></a><span class="st">  </span><span class="kw">step_dummy</span>(<span class="kw">all_nominal</span>())</span></code></pre></div>
<p>There are a few strategies for converting a factor predictor to a numeric format. The most common method is to create “dummy” or indicator variables. Let’s take the predictor in the Ames data for the building type, which is a factor variable with five levels. For dummy variables, the single <code>Bldg_Type</code> column would be replaced with four numeric columns whose values are either zero or one. These binary variables represent specific factor level values. In R, the convention is to <em>exclude</em> a column for the first factor level (<code>OneFam</code>, in this case). The <code>Bldg_Type</code> column would be replaced with a column called <code>TwoFmCon</code> that is one when the row has that value and zero otherwise. Three other columns are similarly created:</p>
<pre><code>## # A tibble: 5 x 5
##   `Raw Data` TwoFmCon Duplex Twnhs TwnhsE
##   &lt;fct&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1 OneFam            0      0     0      0
## 2 TwoFmCon          1      0     0      0
## 3 Duplex            0      1     0      0
## 4 Twnhs             0      0     1      0
## 5 TwnhsE            0      0     0      1</code></pre>
<p>Why not all five? The most basic reason is simplicity; if you know the value for these four columns, you can determine the last value because these are mutually exclusive categories. More technically, the classical justification is that a number of models, including ordinary linear regression, have numerical issues when there are linear dependencies between columns. If all five building type indicator columns are included, they would add up to the intercept column (if there is one). This would cause an issue, or perhaps an outright error, in the underlying matrix algebra.</p>
<p>Different recipe steps can have different effects on columns of the data. For example, <code>step_log()</code> modifies a column in-place without changing the name. Other steps, such as <code>step_dummy()</code> eliminate the original data column and replace it with one or more columns with different names. This behavior depends on the type of operation being done.</p>
</div>
<div id="interaction-terms" class="section level3">
<h3>
<span class="header-section-number">10.3.4</span> Interaction terms</h3>
<p>Interaction effects involve two or more predictors. Such an effect occurs when one predictor has an effect on the outcome that is contingent on one or more other predictors. For example, if you were trying to predict your morning commute time, two potential predictors could be the amount of traffic and the time of day. However, the relationship between commute time and the amount of traffic is different for different times of day. In this case, you could add an interaction term between the two predictors to the model along with the original two predictors (which are called the “main effects”). Numerically, an interaction term between predictors is encoded as their product. Interactions are only defined in terms of their effect on the outcome and can be combinations of different types of data (e.g., numeric, categorical, etc).</p>
<p>After exploring the Ames training set, we might find that the regression slopes for the general living area differ for different building types:</p>
<div class="sourceCode" id="cb1033"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1033-1"><a href="model-choice.html#cb1033-1"></a><span class="kw">ggplot</span>(ames_train, <span class="kw">aes</span>(<span class="dt">x =</span> Gr_Liv_Area, <span class="dt">y =</span> <span class="dv">10</span><span class="op">^</span>Sale_Price)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1033-2"><a href="model-choice.html#cb1033-2"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">.2</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1033-3"><a href="model-choice.html#cb1033-3"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>Bldg_Type) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1033-4"><a href="model-choice.html#cb1033-4"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> lm, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>x, <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">col =</span> <span class="st">"red"</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1033-5"><a href="model-choice.html#cb1033-5"></a><span class="st">  </span><span class="kw">scale_x_log10</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb1033-6"><a href="model-choice.html#cb1033-6"></a><span class="st">  </span><span class="kw">scale_y_log10</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb1033-7"><a href="model-choice.html#cb1033-7"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">"General Living Area"</span>, <span class="dt">y =</span> <span class="st">"Sale Price (USD)"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-827-1.png" width="672"></p>
<p>How are interactions specified? Recipes are explicit and sequential. With the current recipe, <code>step_dummy()</code> has already created dummy variables. How would we combine these for an interaction? The additional step would look like <code>step_interact(~ interaction terms)</code> where the terms on the right-hand side of the tilde are the interactions. These can include selectors, so it would be appropriate to use:</p>
<div class="sourceCode" id="cb1034"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1034-1"><a href="model-choice.html#cb1034-1"></a>simple_ames &lt;-<span class="st"> </span></span>
<span id="cb1034-2"><a href="model-choice.html#cb1034-2"></a><span class="st">  </span><span class="kw">recipe</span>(Sale_Price <span class="op">~</span><span class="st"> </span>Neighborhood <span class="op">+</span><span class="st"> </span>Gr_Liv_Area <span class="op">+</span><span class="st"> </span>Year_Built <span class="op">+</span><span class="st"> </span>Bldg_Type,</span>
<span id="cb1034-3"><a href="model-choice.html#cb1034-3"></a>         <span class="dt">data =</span> ames_train) <span class="op">%&gt;%</span></span>
<span id="cb1034-4"><a href="model-choice.html#cb1034-4"></a><span class="st">  </span><span class="kw">step_log</span>(Gr_Liv_Area, <span class="dt">base =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1034-5"><a href="model-choice.html#cb1034-5"></a><span class="st">  </span><span class="kw">step_other</span>(Neighborhood, <span class="dt">threshold =</span> <span class="fl">0.01</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1034-6"><a href="model-choice.html#cb1034-6"></a><span class="st">  </span><span class="kw">step_dummy</span>(<span class="kw">all_nominal</span>()) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1034-7"><a href="model-choice.html#cb1034-7"></a><span class="st">  </span><span class="co"># Gr_Liv_Area is on the log scale from a previous step</span></span>
<span id="cb1034-8"><a href="model-choice.html#cb1034-8"></a><span class="st">  </span><span class="kw">step_interact</span>( <span class="op">~</span><span class="st"> </span>Gr_Liv_Area<span class="op">:</span><span class="kw">starts_with</span>(<span class="st">"Bldg_Type_"</span>) )</span></code></pre></div>
<p>Additional interactions can be specified in this formula by separating them by <code>+</code>. Also note that the recipe will only utilize interactions between different variables; if the formula uses <code>var_1:var_1</code>, this term will be ignored.</p>
</div>
<div id="how-data-are-used-by-the-recipe" class="section level3">
<h3>
<span class="header-section-number">10.3.5</span> How data are used by the recipe</h3>
<p>Data are given to recipes at different stages. When calling <code>recipe(..., data)</code>, the data set is used to determine the data types of each column so that selectors such as <code>all_numeric()</code> can be used. When preparing the data using <code>prep(recipe, training)</code>, the data in <code>training</code> are used for all estimation operations, from determining factor levels to computing PCA components and everything in between. It is important to realize that all preprocessing and feature engineering steps only utilize the training data. Otherwise, information leakage can negatively impact the model.</p>
<p>When using <code>bake(recipe, new_data)</code>, no quantities are re-estimated using the values in <code>new_data</code>. Take centering and scaling using <code>step_normalize()</code> as an example. Using this step, the means and standard deviations from the appropriate columns are determined from the training set; new samples are standardized using these values when <code>bake()</code> is invoked.</p>
<p>Our code so far:</p>
<div class="sourceCode" id="cb1035"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1035-1"><a href="model-choice.html#cb1035-1"></a><span class="kw">library</span>(tidymodels)</span>
<span id="cb1035-2"><a href="model-choice.html#cb1035-2"></a><span class="kw">data</span>(ames)</span>
<span id="cb1035-3"><a href="model-choice.html#cb1035-3"></a>ames &lt;-<span class="st"> </span><span class="kw">mutate</span>(ames, <span class="dt">Sale_Price =</span> <span class="kw">log10</span>(Sale_Price))</span>
<span id="cb1035-4"><a href="model-choice.html#cb1035-4"></a></span>
<span id="cb1035-5"><a href="model-choice.html#cb1035-5"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1035-6"><a href="model-choice.html#cb1035-6"></a>ames_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(ames, <span class="dt">prob =</span> <span class="fl">0.80</span>, <span class="dt">strata =</span> Sale_Price)</span>
<span id="cb1035-7"><a href="model-choice.html#cb1035-7"></a>ames_train &lt;-<span class="st"> </span><span class="kw">training</span>(ames_split)</span>
<span id="cb1035-8"><a href="model-choice.html#cb1035-8"></a>ames_test  &lt;-<span class="st"> </span><span class="kw">testing</span>(ames_split)</span>
<span id="cb1035-9"><a href="model-choice.html#cb1035-9"></a></span>
<span id="cb1035-10"><a href="model-choice.html#cb1035-10"></a>ames_rec &lt;-<span class="st"> </span></span>
<span id="cb1035-11"><a href="model-choice.html#cb1035-11"></a><span class="st">  </span><span class="kw">recipe</span>(Sale_Price <span class="op">~</span><span class="st"> </span>Neighborhood <span class="op">+</span><span class="st"> </span>Gr_Liv_Area <span class="op">+</span><span class="st"> </span>Year_Built <span class="op">+</span><span class="st"> </span>Bldg_Type <span class="op">+</span><span class="st"> </span></span>
<span id="cb1035-12"><a href="model-choice.html#cb1035-12"></a><span class="st">           </span>Latitude <span class="op">+</span><span class="st"> </span>Longitude, <span class="dt">data =</span> ames_train) <span class="op">%&gt;%</span></span>
<span id="cb1035-13"><a href="model-choice.html#cb1035-13"></a><span class="st">  </span><span class="kw">step_log</span>(Gr_Liv_Area, <span class="dt">base =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1035-14"><a href="model-choice.html#cb1035-14"></a><span class="st">  </span><span class="kw">step_other</span>(Neighborhood, <span class="dt">threshold =</span> <span class="fl">0.01</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1035-15"><a href="model-choice.html#cb1035-15"></a><span class="st">  </span><span class="kw">step_dummy</span>(<span class="kw">all_nominal</span>()) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1035-16"><a href="model-choice.html#cb1035-16"></a><span class="st">  </span><span class="kw">step_interact</span>( <span class="op">~</span><span class="st"> </span>Gr_Liv_Area<span class="op">:</span><span class="kw">starts_with</span>(<span class="st">"Bldg_Type_"</span>) ) </span></code></pre></div>
</div>
</div>
<div id="models" class="section level2">
<h2>
<span class="header-section-number">10.4</span> Fitting models with parsnip</h2>
<p>The <strong>parsnip</strong> package provides a fluent and standardized interface for a variety of different models. In this section, we both give some motivation for why a common interface is beneficial and show how to use the package.</p>
<div id="create-a-model" class="section level3">
<h3>
<span class="header-section-number">10.4.1</span> Create a model</h3>
<p>Once the data have been encoded in a format ready for a modeling algorithm, such as a numeric matrix, they can be used in the model building process.</p>
<p>Suppose that a linear regression model was our initial choice for the model. This is equivalent to specifying that the outcome data is numeric and that the predictors are related to the model in terms of simple slopes and intercepts:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 x_{1i} + \ldots + \beta_p x_{pi}\]</span></p>
<p>There are a variety of methods that can be used to estimate the model parameters:</p>
<ul>
<li><p><em>Ordinary linear regression</em> uses the traditional method of least squares to solve for the model parameters.</p></li>
<li><p><em>Regularized linear regression</em> adds a penalty to the least squares method to encourage simplicity by removing predictors and/or shrinking their coefficients towards zero. This can be executed using Bayesian or non-Bayesian techniques.</p></li>
</ul>
<p>In R, the <strong>stats</strong> package can be used for the first case. The syntax for <code>lm()</code> is</p>
<div class="sourceCode" id="cb1036"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1036-1"><a href="model-choice.html#cb1036-1"></a>model &lt;-<span class="st"> </span><span class="kw">lm</span>(formula, data, ...)</span></code></pre></div>
<p>where <code>...</code> symbolizes other options to pass to <code>lm()</code>.</p>
<p>To estimate a Bayesian model, we use <code>stan_glm()</code> from the <strong>rstanarm</strong> package, as we have done in the last few chapters.</p>
<div class="sourceCode" id="cb1037"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1037-1"><a href="model-choice.html#cb1037-1"></a>model &lt;-<span class="st"> </span><span class="kw">stan_glm</span>(formula, </span>
<span id="cb1037-2"><a href="model-choice.html#cb1037-2"></a>                  data, </span>
<span id="cb1037-3"><a href="model-choice.html#cb1037-3"></a>                  <span class="dt">family =</span> <span class="st">"gaussian"</span>, ...)</span></code></pre></div>
<p>A popular non-Bayesian approach to regression is the glmnet model. Its syntax is:</p>
<div class="sourceCode" id="cb1038"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1038-1"><a href="model-choice.html#cb1038-1"></a>model &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x =</span> matrix, </span>
<span id="cb1038-2"><a href="model-choice.html#cb1038-2"></a>                <span class="dt">y =</span> vector, </span>
<span id="cb1038-3"><a href="model-choice.html#cb1038-3"></a>                <span class="dt">family =</span> <span class="st">"gaussian"</span>, ...)</span></code></pre></div>
<p>In this case, the predictor data must already be formatted into a numeric matrix; there is only and <code>x</code>/<code>y</code> method and no formula method.</p>
<p>Note that these interfaces are heterogeneous in either how the data are passed to the model function or in terms of their arguments. The first issue is that, to fit models across different packages, the data must be formatted in different ways. <code>lm()</code> and <code>stan_glm()</code> only have formula interfaces while <code>glmnet()</code> does not. For other types of models, the interfaces may be even more disparate. For a person trying to do data analysis, these differences require the memorization of each package’s syntax and can be very frustrating.</p>
<p>For tidymodels, the approach to specifying a model is intended to be more unified:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Specify the <em>type</em> of model based on its mathematical structure</strong>.</p></li>
<li><p><strong>Specify the <em>engine</em> for fitting the model.</strong> Most often this reflects the software package that should be used.</p></li>
<li><p><strong>When required, declare the <em>mode</em> of the model.</strong> The mode reflects the type of prediction outcome. For numeric outcomes, the mode is <em>regression</em>; for qualitative outcomes, it is <em>classification</em>. Note that <strong>parsnip</strong> constrains the outcome column of a classification models to be encoded as a <em>factor</em>; using binary numeric values will result in an error. If a model can only create one type of model, such as linear regression, the mode is already set.</p></li>
</ol>
<p>These specifications are built <em>without referencing the data</em>. For example, for the three cases above:</p>
<div class="sourceCode" id="cb1039"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1039-1"><a href="model-choice.html#cb1039-1"></a><span class="kw">linear_reg</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1039-2"><a href="model-choice.html#cb1039-2"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">"lm"</span>)</span></code></pre></div>
<pre><code>## Linear Regression Model Specification (regression)
## 
## Computational engine: lm</code></pre>
<div class="sourceCode" id="cb1041"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1041-1"><a href="model-choice.html#cb1041-1"></a><span class="kw">linear_reg</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1041-2"><a href="model-choice.html#cb1041-2"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">"glmnet"</span>) </span></code></pre></div>
<pre><code>## Linear Regression Model Specification (regression)
## 
## Computational engine: glmnet</code></pre>
<div class="sourceCode" id="cb1043"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1043-1"><a href="model-choice.html#cb1043-1"></a><span class="kw">linear_reg</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1043-2"><a href="model-choice.html#cb1043-2"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">"stan"</span>)</span></code></pre></div>
<pre><code>## Linear Regression Model Specification (regression)
## 
## Computational engine: stan</code></pre>
<p>Once the details of the model have been specified, the model estimation can be done using a formula with the <code>fit()</code> function. The <strong>parsnip</strong> package allows the user to be indifferent to the interface of the underlying model.</p>
<p>Let’s walk through how to predict the sale price of houses in the Ames data as a function of only longitude and latitude.</p>
<div class="sourceCode" id="cb1045"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1045-1"><a href="model-choice.html#cb1045-1"></a>lm_model &lt;-<span class="st"> </span></span>
<span id="cb1045-2"><a href="model-choice.html#cb1045-2"></a><span class="st">  </span><span class="kw">linear_reg</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1045-3"><a href="model-choice.html#cb1045-3"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">"lm"</span>)</span>
<span id="cb1045-4"><a href="model-choice.html#cb1045-4"></a></span>
<span id="cb1045-5"><a href="model-choice.html#cb1045-5"></a><span class="co"># Recall that Sale_Price has been pre-logged</span></span>
<span id="cb1045-6"><a href="model-choice.html#cb1045-6"></a></span>
<span id="cb1045-7"><a href="model-choice.html#cb1045-7"></a>lm_form_fit &lt;-<span class="st"> </span></span>
<span id="cb1045-8"><a href="model-choice.html#cb1045-8"></a><span class="st">  </span>lm_model <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1045-9"><a href="model-choice.html#cb1045-9"></a><span class="st">  </span><span class="kw">fit</span>(Sale_Price <span class="op">~</span><span class="st"> </span>Longitude <span class="op">+</span><span class="st"> </span>Latitude, <span class="dt">data =</span> ames_train)</span>
<span id="cb1045-10"><a href="model-choice.html#cb1045-10"></a>    </span>
<span id="cb1045-11"><a href="model-choice.html#cb1045-11"></a>lm_form_fit</span></code></pre></div>
<pre><code>## parsnip model object
## 
## Fit time:  10ms 
## 
## Call:
## stats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)
## 
## Coefficients:
## (Intercept)    Longitude     Latitude  
##     -316.37        -2.08         3.01</code></pre>
<p>Not only does <strong>parsnip</strong> enable a consistent model interface for different packages, it also provides consistency in the <em>model arguments</em>. Modeling functions in <strong>parsnip</strong> separate model arguments into two categories:</p>
<ul>
<li><p><em>Main arguments</em> are more commonly used and tend to be available across engines.</p></li>
<li><p><em>Engine arguments</em> are either specific to a particular engine or used more rarely.</p></li>
</ul>
</div>
<div id="use-the-model-results" class="section level3">
<h3>
<span class="header-section-number">10.4.2</span> Use the model results</h3>
<!-- DK: Delete this section? -->
<p>Once the model is created and fit, we can use the results in a variety of ways; we might want to plot, print, or otherwise examine the model output. Several quantities are stored in a <strong>parsnip</strong> model object, including the fitted model. This can be found in an element called <code>fit</code>, which can be returned using the <code>purrr::pluck()</code> function:</p>
<!-- DK: Investigate pluck nonsense. -->
<div class="sourceCode" id="cb1047"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1047-1"><a href="model-choice.html#cb1047-1"></a>lm_form_fit <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1047-2"><a href="model-choice.html#cb1047-2"></a><span class="st">  </span>purrr<span class="op">::</span><span class="kw">pluck</span>(<span class="st">"fit"</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## stats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)
## 
## Coefficients:
## (Intercept)    Longitude     Latitude  
##     -316.37        -2.08         3.01</code></pre>
<p>Normal methods can be applied to this object, such as printing, plotting, and so on.</p>
<p>One issue with some existing methods in base R is that the results are stored in a manner that may not be the most useful. As a solution, the <strong>broom</strong> package has methods to convert many types of model objects to a tidy structure. For example, using the <code>tidy()</code> method on the linear model produces:</p>
<div class="sourceCode" id="cb1049"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1049-1"><a href="model-choice.html#cb1049-1"></a><span class="kw">tidy</span>(lm_form_fit)</span></code></pre></div>
<pre><code>## # A tibble: 3 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)  -316.      14.9       -21.2 3.04e-91
## 2 Longitude      -2.08     0.133     -15.6 3.21e-52
## 3 Latitude        3.01     0.185      16.3 2.58e-56</code></pre>
<p>The column names are standardized across models and do not contain any additional data (such as the type of statistical test). The data previously contained in the row names are now in a column called <code>terms</code> and so on. One important principle in the tidymodels ecosystem is that a function should return values that are <em>predictable, consistent, and unsurprising</em>.</p>
</div>
<div id="parsnip-predictions" class="section level3">
<h3>
<span class="header-section-number">10.4.3</span> Make predictions</h3>
<p>Another area where <strong>parsnip</strong> diverges from conventional R modeling functions is the format of values returned from <code>predict()</code>. For predictions, <strong>parsnip</strong> always conforms to the following rules:</p>
<ol style="list-style-type: decimal">
<li>The results are always a tibble.</li>
<li>The column names of the tibble are always predictable.</li>
<li>There are always as many rows in the tibble as there are in the input data set.</li>
</ol>
<p>For example, when numeric data are predicted:</p>
<div class="sourceCode" id="cb1051"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1051-1"><a href="model-choice.html#cb1051-1"></a>ames_test_small &lt;-<span class="st"> </span>ames_test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>)</span>
<span id="cb1051-2"><a href="model-choice.html#cb1051-2"></a><span class="kw">predict</span>(lm_form_fit, <span class="dt">new_data =</span> ames_test_small)</span></code></pre></div>
<pre><code>## # A tibble: 5 x 1
##   .pred
##   &lt;dbl&gt;
## 1  5.22
## 2  5.29
## 3  5.28
## 4  5.26
## 5  5.24</code></pre>
<p>The row order of the predictions are always the same as the original data. Why is there a leading dot in some of the column names? Some tidyverse and tidymodels functions have arguments and return values which contain periods. This is to protect against merging data with duplicate names. There are some data sets that contain predictors named <code>pred</code>!</p>
<p>These three rules make it easier to merge predictions with the original data:</p>
<div class="sourceCode" id="cb1053"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1053-1"><a href="model-choice.html#cb1053-1"></a>ames_test_small <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1053-2"><a href="model-choice.html#cb1053-2"></a><span class="st">  </span><span class="kw">select</span>(Sale_Price) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1053-3"><a href="model-choice.html#cb1053-3"></a><span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">predict</span>(lm_form_fit, ames_test_small)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1053-4"><a href="model-choice.html#cb1053-4"></a><span class="st">  </span></span>
<span id="cb1053-5"><a href="model-choice.html#cb1053-5"></a><span class="st">  </span><span class="co"># Add 95% prediction intervals to the results:</span></span>
<span id="cb1053-6"><a href="model-choice.html#cb1053-6"></a><span class="st">  </span></span>
<span id="cb1053-7"><a href="model-choice.html#cb1053-7"></a><span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">predict</span>(lm_form_fit, ames_test_small, <span class="dt">type =</span> <span class="st">"pred_int"</span>)) </span></code></pre></div>
<pre><code>## # A tibble: 5 x 4
##   Sale_Price .pred .pred_lower .pred_upper
##        &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
## 1       5.39  5.22        4.90        5.53
## 2       5.28  5.29        4.97        5.60
## 3       5.27  5.28        4.96        5.59
## 4       5.60  5.26        4.95        5.58
## 5       5.02  5.24        4.93        5.55</code></pre>
<p>For the second tidymodels prediction rule, the predictable column names for different types of predictions are:</p>
<pre><code>## # A tibble: 5 x 2
##   `type value` `column name(s)`            
##   &lt;chr&gt;        &lt;chr&gt;                       
## 1 `numeric`    `.pred`                     
## 2 `class`      `.pred_class`               
## 3 `prob`       `.pred_{class levels}`      
## 4 `conf_int`   `.pred_lower`, `.pred_upper`
## 5 `pred_int`   `.pred_lower`, `.pred_upper`</code></pre>
<p>The third rule regarding the number of rows in the output is critical. For example, if any rows of the new data contain missing values, the output will be padded with missing results for those rows.</p>
<p>A main advantage of standardizing the model interface and prediction types in <strong>parsnip</strong> is that, when different models are used, the syntax is identical. Suppose that we used a decision tree to model the Ames data. Outside of the model specification, there are no significant differences in the code pipeline:</p>
<div class="sourceCode" id="cb1056"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1056-1"><a href="model-choice.html#cb1056-1"></a>tree_model &lt;-<span class="st"> </span></span>
<span id="cb1056-2"><a href="model-choice.html#cb1056-2"></a><span class="st">  </span><span class="kw">decision_tree</span>(<span class="dt">min_n =</span> <span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1056-3"><a href="model-choice.html#cb1056-3"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">"rpart"</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1056-4"><a href="model-choice.html#cb1056-4"></a><span class="st">  </span><span class="kw">set_mode</span>(<span class="st">"regression"</span>)</span>
<span id="cb1056-5"><a href="model-choice.html#cb1056-5"></a></span>
<span id="cb1056-6"><a href="model-choice.html#cb1056-6"></a>tree_fit &lt;-<span class="st"> </span></span>
<span id="cb1056-7"><a href="model-choice.html#cb1056-7"></a><span class="st">  </span>tree_model <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1056-8"><a href="model-choice.html#cb1056-8"></a><span class="st">  </span><span class="kw">fit</span>(Sale_Price <span class="op">~</span><span class="st"> </span>Longitude <span class="op">+</span><span class="st"> </span>Latitude, <span class="dt">data =</span> ames_train)</span>
<span id="cb1056-9"><a href="model-choice.html#cb1056-9"></a></span>
<span id="cb1056-10"><a href="model-choice.html#cb1056-10"></a>ames_test_small <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1056-11"><a href="model-choice.html#cb1056-11"></a><span class="st">  </span><span class="kw">select</span>(Sale_Price) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1056-12"><a href="model-choice.html#cb1056-12"></a><span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">predict</span>(tree_fit, ames_test_small))</span></code></pre></div>
<pre><code>## # A tibble: 5 x 2
##   Sale_Price .pred
##        &lt;dbl&gt; &lt;dbl&gt;
## 1       5.39  5.16
## 2       5.28  5.31
## 3       5.27  5.31
## 4       5.60  5.31
## 5       5.02  5.16</code></pre>
<p>This demonstrates the benefit of homogenizing the data analysis process and syntax across different models. It enables users to spend their time on the results and interpretation rather than having to focus on the syntactical differences between R packages.</p>
</div>
</div>
<div id="workflows" class="section level2">
<h2>
<span class="header-section-number">10.5</span> A model workflow</h2>
<p>In the previous two sections, we discussed the <strong>recipes</strong> and <strong>parsnip</strong> packages. These packages can be used to prepare the data for analysis and fitting the model. This chapter introduces a new object called a <em>model workflow</em>. The purpose of this object is to encapsulate the major pieces of the modeling <em>process</em>. The workflow is important in two ways. First, using a workflow object encourages good methodology since it is a single point of entry to the estimation components of a data analysis. Second, it enables the user to better organize their projects.</p>
<div id="begin-model-end" class="section level3">
<h3>
<span class="header-section-number">10.5.1</span> Where does the model begin and end?</h3>
<p>So far, when we have used the term “the model”, we have meant a structural equation that relates some predictors to one or more outcomes. Let’s consider again linear regression as an example. The outcome data are denoted as <span class="math inline">\(y_i\)</span>, where there are <span class="math inline">\(i = 1 \ldots n\)</span> samples in the training set. Suppose that there are <span class="math inline">\(p\)</span> predictors <span class="math inline">\(x_{i1}, \ldots, x_{ip}\)</span> that are used in the model. Linear regression produces a model equation of</p>
<p><span class="math display">\[ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_{i1} + \ldots + \hat{\beta}_px_{ip} \]</span></p>
<p>While this is a <em>linear</em> model, it is only linear in the parameters. The predictors could be nonlinear terms like <span class="math inline">\(log(x_i)\)</span>.</p>
<p>For some data sets that are straightforward in nature, fitting the model itself may be the entire process. However, there are a variety of choices and additional steps that often occur <em>before</em> the model is fit:</p>
<ul>
<li><p>While our example model has <span class="math inline">\(p\)</span> predictors, it is common to start with more than <span class="math inline">\(p\)</span> candidate predictors. Through exploratory data analysis or using domain knowledge, some of the predictors may be excluded from the analysis. In other cases, a feature selection algorithm may be used to make a data-driven choice about which predictors to include.</p></li>
<li><p>There are times when the value of an important predictor is missing. Rather than eliminating this sample from the data set, the missing value could be <em>imputed</em> using other values in the data. For example, if <span class="math inline">\(x_1\)</span> were missing but was correlated with predictors <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span>, an imputation method could estimate the missing <span class="math inline">\(x_1\)</span> observation from the values of <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span>.</p></li>
<li><p>It may be beneficial to transform the scale of a predictor. If there is not <em>a priori</em> information on what the new scale should be, we can estimate the proper scale using a statistical transformation technique, the existing data, and some optimization criterion. Other transformations, such as PCA, take groups of predictors and transform them into new features that are used as the predictors.</p></li>
</ul>
<p>While the examples above are related to steps that occur before the model fit, there may also be operations that occur <em>after</em> the model is created. When a classification model is created where the outcome is binary (e.g., <code>event</code> and <code>non-event</code>), it is customary to use a 50% probability cutoff to create a discrete class prediction, also known as a “hard prediction”. For example, a classification model might estimate that the probability of an event was 62%. Using the typical default, the hard prediction would be <code>event</code>. However, the model may need to be more focused on reducing false positive results (i.e., where true non-events are classified as events). One way to do this is to raise the cutoff from 50% to some greater value. This increases the level of evidence required to call a new sample an event. While this reduces the true positive rate (which is bad), it may have a more dramatic effect on reducing false positives. The choice of the cutoff value should be optimized using data. This is an example of a <em>post-processing</em> step that has a significant effect on how well the model works, even though it is not contained in the model fitting step.</p>
<p>It is important to focus on the broader <em>modeling process</em>, instead of only fitting the specific model used to estimate parameters. This broader process includes any preprocessing steps, the model fit itself, as well as potential post-processing activities. In this book, we will refer to this broader process as the <strong>model workflow</strong> and include in it any data-driven activities that are used to produce a final model equation.</p>
<p>In other software, such as Python or Spark, similar collections of steps are called <em>pipelines</em>. In tidymodels, the term “pipeline” already connotes a sequence of operations chained together with a pipe operator (such as <code>%&gt;%</code>). Rather than using ambiguous terminology in this context, we call the sequence of computational operations related to modeling <strong>workflows</strong>.</p>
<p>Binding together the analytical components of a data analysis is important for another reason. We will learn how to accurately measure performance, as well as how to optimize structural parameters (i.e. model tuning). To correctly quantify model performance on the training set, we recommend using <em>resampling</em> methods. To do this properly, no data-driven parts of the analysis should be excluded from validation. To this end, the workflow must include all significant estimation steps.</p>
<p>To illustrate, consider PCA signal extraction. This is a way to replace correlated predictors with new artificial features that are uncorrelated and capture most of the information in the original set. The new features would be used as the predictors and least squares regression could be used to estimate the model parameters.</p>
<p>There are two ways of thinking about the model workflow. The <em>incorrect</em> method would be to think of the PCA preprocessing step as <em>not being part of the modeling process</em>:</p>
<p><img src="10-model-choice/images/bad-workflow.svg"></p>
<p>The fallacy here is that, although PCA does significant computations to produce the components, its operations are assumed to have no uncertainty associated with them. The PCA components are treated as <em>known</em> and, if not included in the model workflow, the effect of PCA could not be adequately measured.</p>
<p>An <em>appropriate</em> approach would be:</p>
<p><img src="10-model-choice/images/proper-workflow.svg"></p>
<p>In this way, the PCA preprocessing is considered part of the modeling process.</p>
</div>
<div id="workflow-basics" class="section level3">
<h3>
<span class="header-section-number">10.5.2</span> Workflow basics</h3>
<p>The <strong>workflows</strong> package allows the user to bind modeling and preprocessing objects together. Let’s start again with the Ames data and a simple linear model:</p>
<div class="sourceCode" id="cb1058"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1058-1"><a href="model-choice.html#cb1058-1"></a><span class="kw">library</span>(tidymodels)  <span class="co"># Includes the workflows package</span></span>
<span id="cb1058-2"><a href="model-choice.html#cb1058-2"></a></span>
<span id="cb1058-3"><a href="model-choice.html#cb1058-3"></a>lm_model &lt;-<span class="st"> </span></span>
<span id="cb1058-4"><a href="model-choice.html#cb1058-4"></a><span class="st">  </span><span class="kw">linear_reg</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1058-5"><a href="model-choice.html#cb1058-5"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">"lm"</span>)</span></code></pre></div>
<p>A workflow always requires a <strong>parsnip</strong> model object:</p>
<div class="sourceCode" id="cb1059"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1059-1"><a href="model-choice.html#cb1059-1"></a>lm_wflow &lt;-<span class="st"> </span></span>
<span id="cb1059-2"><a href="model-choice.html#cb1059-2"></a><span class="st">  </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1059-3"><a href="model-choice.html#cb1059-3"></a><span class="st">  </span><span class="kw">add_model</span>(lm_model)</span>
<span id="cb1059-4"><a href="model-choice.html#cb1059-4"></a></span>
<span id="cb1059-5"><a href="model-choice.html#cb1059-5"></a>lm_wflow</span></code></pre></div>
<pre><code>## ══ Workflow ════════════════════════════════════════════════════════════════
## Preprocessor: None
## Model: linear_reg()
## 
## ── Model ───────────────────────────────────────────────────────────────────
## Linear Regression Model Specification (regression)
## 
## Computational engine: lm</code></pre>
<p>Notice that we have not yet specified how this workflow should preprocess the data: <code>Preprocessor: None</code>.</p>
<p>If our model were very simple, a standard R formula can be used as a preprocessor:</p>
<div class="sourceCode" id="cb1061"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1061-1"><a href="model-choice.html#cb1061-1"></a>lm_wflow &lt;-<span class="st"> </span></span>
<span id="cb1061-2"><a href="model-choice.html#cb1061-2"></a><span class="st">  </span>lm_wflow <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1061-3"><a href="model-choice.html#cb1061-3"></a><span class="st">  </span><span class="kw">add_formula</span>(Sale_Price <span class="op">~</span><span class="st"> </span>Longitude <span class="op">+</span><span class="st"> </span>Latitude)</span>
<span id="cb1061-4"><a href="model-choice.html#cb1061-4"></a></span>
<span id="cb1061-5"><a href="model-choice.html#cb1061-5"></a>lm_wflow</span></code></pre></div>
<pre><code>## ══ Workflow ════════════════════════════════════════════════════════════════
## Preprocessor: Formula
## Model: linear_reg()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────
## Sale_Price ~ Longitude + Latitude
## 
## ── Model ───────────────────────────────────────────────────────────────────
## Linear Regression Model Specification (regression)
## 
## Computational engine: lm</code></pre>
<p>Workflows have a <code>fit()</code> method that can be used to create the model. Using the objects created previously:</p>
<div class="sourceCode" id="cb1063"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1063-1"><a href="model-choice.html#cb1063-1"></a>lm_fit &lt;-<span class="st"> </span><span class="kw">fit</span>(lm_wflow, ames_train)</span>
<span id="cb1063-2"><a href="model-choice.html#cb1063-2"></a>lm_fit</span></code></pre></div>
<pre><code>## ══ Workflow [trained] ══════════════════════════════════════════════════════
## Preprocessor: Formula
## Model: linear_reg()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────
## Sale_Price ~ Longitude + Latitude
## 
## ── Model ───────────────────────────────────────────────────────────────────
## 
## Call:
## stats::lm(formula = ..y ~ ., data = data)
## 
## Coefficients:
## (Intercept)    Longitude     Latitude  
##     -316.37        -2.08         3.01</code></pre>
<p>We can also <code>predict()</code> on the fitted workflow:</p>
<div class="sourceCode" id="cb1065"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1065-1"><a href="model-choice.html#cb1065-1"></a><span class="kw">predict</span>(lm_fit, ames_test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>))</span></code></pre></div>
<pre><code>## # A tibble: 3 x 1
##   .pred
##   &lt;dbl&gt;
## 1  5.22
## 2  5.29
## 3  5.28</code></pre>
<p>The <code>predict()</code> method follows all of the same rules and naming conventions that we described for the <strong>parsnip</strong> package.</p>
</div>
<div id="workflows-and-recipes" class="section level3">
<h3>
<span class="header-section-number">10.5.3</span> Workflows and recipes</h3>
<!-- DK: Simplify this. No need to teach update() stuff trickery. -->
<p>Instead of using model formulas, recipe objects can also be used to preprocess data for modeling. Previously, we summarized a recipe that specified several preprocessing and feature engineering steps. These are encapsulated inside the object <code>ames_rec</code> and are attached to the workflow:</p>
<div class="sourceCode" id="cb1067"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1067-1"><a href="model-choice.html#cb1067-1"></a>lm_wflow <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1067-2"><a href="model-choice.html#cb1067-2"></a><span class="st">  </span><span class="kw">add_recipe</span>(ames_rec)</span></code></pre></div>
<pre><code>## Error: A recipe cannot be added when a formula already exists.</code></pre>
<p>That did not work! We can only have one preprocessing method at a time, so we need to remove the formula before adding the recipe.</p>
<div class="sourceCode" id="cb1069"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1069-1"><a href="model-choice.html#cb1069-1"></a>lm_wflow &lt;-<span class="st"> </span></span>
<span id="cb1069-2"><a href="model-choice.html#cb1069-2"></a><span class="st">  </span>lm_wflow <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1069-3"><a href="model-choice.html#cb1069-3"></a><span class="st">  </span><span class="kw">remove_formula</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1069-4"><a href="model-choice.html#cb1069-4"></a><span class="st">  </span><span class="kw">add_recipe</span>(ames_rec)</span>
<span id="cb1069-5"><a href="model-choice.html#cb1069-5"></a>lm_wflow</span></code></pre></div>
<pre><code>## ══ Workflow ════════════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: linear_reg()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────
## 4 Recipe Steps
## 
## ● step_log()
## ● step_other()
## ● step_dummy()
## ● step_interact()
## 
## ── Model ───────────────────────────────────────────────────────────────────
## Linear Regression Model Specification (regression)
## 
## Computational engine: lm</code></pre>
<p>Use <code>fit()</code> to complete the model and <code>predict()</code> to create predictions from the fitted model.</p>
<div class="sourceCode" id="cb1071"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1071-1"><a href="model-choice.html#cb1071-1"></a>lm_fit &lt;-<span class="st"> </span><span class="kw">fit</span>(lm_wflow, ames_train)</span>
<span id="cb1071-2"><a href="model-choice.html#cb1071-2"></a></span>
<span id="cb1071-3"><a href="model-choice.html#cb1071-3"></a><span class="kw">predict</span>(lm_fit, ames_test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>))</span></code></pre></div>
<pre><code>## # A tibble: 3 x 1
##   .pred
##   &lt;dbl&gt;
## 1  5.33
## 2  5.31
## 3  5.17</code></pre>
<p>We have learned that the modeling process encompasses more than just estimating the parameters of an algorithm that connects predictors to an outcome. This process also includes preprocessing steps and operations taken after a model is fit. We introduced a concept called a <strong>model workflow</strong> that can capture the important components of the modeling process.</p>
<p>For the Ames data, the updated code:</p>
<div class="sourceCode" id="cb1073"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1073-1"><a href="model-choice.html#cb1073-1"></a><span class="kw">library</span>(tidymodels)</span>
<span id="cb1073-2"><a href="model-choice.html#cb1073-2"></a><span class="kw">data</span>(ames)</span>
<span id="cb1073-3"><a href="model-choice.html#cb1073-3"></a></span>
<span id="cb1073-4"><a href="model-choice.html#cb1073-4"></a>ames &lt;-<span class="st"> </span><span class="kw">mutate</span>(ames, <span class="dt">Sale_Price =</span> <span class="kw">log10</span>(Sale_Price))</span>
<span id="cb1073-5"><a href="model-choice.html#cb1073-5"></a></span>
<span id="cb1073-6"><a href="model-choice.html#cb1073-6"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1073-7"><a href="model-choice.html#cb1073-7"></a>ames_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(ames, <span class="dt">prob =</span> <span class="fl">0.80</span>, <span class="dt">strata =</span> Sale_Price)</span>
<span id="cb1073-8"><a href="model-choice.html#cb1073-8"></a>ames_train &lt;-<span class="st"> </span><span class="kw">training</span>(ames_split)</span>
<span id="cb1073-9"><a href="model-choice.html#cb1073-9"></a>ames_test  &lt;-<span class="st">  </span><span class="kw">testing</span>(ames_split)</span>
<span id="cb1073-10"><a href="model-choice.html#cb1073-10"></a></span>
<span id="cb1073-11"><a href="model-choice.html#cb1073-11"></a>ames_rec &lt;-<span class="st"> </span></span>
<span id="cb1073-12"><a href="model-choice.html#cb1073-12"></a><span class="st">  </span><span class="kw">recipe</span>(Sale_Price <span class="op">~</span><span class="st"> </span>Neighborhood <span class="op">+</span><span class="st"> </span>Gr_Liv_Area <span class="op">+</span><span class="st"> </span>Year_Built <span class="op">+</span><span class="st"> </span>Bldg_Type <span class="op">+</span><span class="st"> </span></span>
<span id="cb1073-13"><a href="model-choice.html#cb1073-13"></a><span class="st">           </span>Latitude <span class="op">+</span><span class="st"> </span>Longitude, <span class="dt">data =</span> ames_train) <span class="op">%&gt;%</span></span>
<span id="cb1073-14"><a href="model-choice.html#cb1073-14"></a><span class="st">  </span><span class="kw">step_log</span>(Gr_Liv_Area, <span class="dt">base =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1073-15"><a href="model-choice.html#cb1073-15"></a><span class="st">  </span><span class="kw">step_other</span>(Neighborhood, <span class="dt">threshold =</span> <span class="fl">0.01</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1073-16"><a href="model-choice.html#cb1073-16"></a><span class="st">  </span><span class="kw">step_dummy</span>(<span class="kw">all_nominal</span>()) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1073-17"><a href="model-choice.html#cb1073-17"></a><span class="st">  </span><span class="kw">step_interact</span>( <span class="op">~</span><span class="st"> </span>Gr_Liv_Area<span class="op">:</span><span class="kw">starts_with</span>(<span class="st">"Bldg_Type_"</span>) )</span>
<span id="cb1073-18"><a href="model-choice.html#cb1073-18"></a></span>
<span id="cb1073-19"><a href="model-choice.html#cb1073-19"></a>lm_model &lt;-<span class="st"> </span><span class="kw">linear_reg</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">set_engine</span>(<span class="st">"lm"</span>)</span>
<span id="cb1073-20"><a href="model-choice.html#cb1073-20"></a></span>
<span id="cb1073-21"><a href="model-choice.html#cb1073-21"></a>lm_wflow &lt;-<span class="st"> </span></span>
<span id="cb1073-22"><a href="model-choice.html#cb1073-22"></a><span class="st">  </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1073-23"><a href="model-choice.html#cb1073-23"></a><span class="st">  </span><span class="kw">add_model</span>(lm_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1073-24"><a href="model-choice.html#cb1073-24"></a><span class="st">  </span><span class="kw">add_recipe</span>(ames_rec)</span>
<span id="cb1073-25"><a href="model-choice.html#cb1073-25"></a></span>
<span id="cb1073-26"><a href="model-choice.html#cb1073-26"></a>lm_fit &lt;-<span class="st"> </span><span class="kw">fit</span>(lm_wflow, ames_train)</span></code></pre></div>
</div>
</div>
<div id="performance" class="section level2">
<h2>
<span class="header-section-number">10.6</span> Judging model effectiveness</h2>
<p>Once we have a model, we need to know how well it works. A quantitative approach for estimating effectiveness allows us to understand the model, to compare different models, or to tweak the model to improve performance. Our focus in tidymodels is on <em>empirical validation</em>; this usually means using data that were not used to create the model as the substrate on which we measure effectiveness.</p>
<p>The choice of which metrics to examine can be critical. For example, two common metrics for regression models are the root mean squared error (RMSE) and the coefficient of determination (a.k.a. <span class="math inline">\(R^2\)</span>). The former measures <em>accuracy</em> while the latter measures <em>correlation</em>. These are not necessarily the same thing. This figure demonstrates the difference between the two:</p>
<p><img src="book_temp_files/figure-html/unnamed-chunk-847-1.png" width="672"></p>
<p>A model optimized for RMSE has more variability but has relatively uniform accuracy across the range of the outcome. The right panel shows that there is a tighter correlation between the observed and predicted values but this model performs poorly in the tails.</p>
<p>This section will largely focus on the <strong>yardstick</strong> package. Before illustrating syntax, let’s explore whether empirical validation using performance metrics is worthwhile when a model is focused on causation rather than prediction.</p>
<div id="performance-metrics-and-causal-models" class="section level3">
<h3>
<span class="header-section-number">10.6.1</span> Performance metrics and causal models</h3>
<!-- DK: We could put these sections at the start of their respective chapters. -->
<p>The effectiveness of any given model depends on how the model will be used. A causal model is used primarily to understand relationships, and typically is discussed with a strong focus on the choice (and validity) of probabilistic distributions and other generative qualities that define the model. For a model used primarily for prediction, by contrast, predictive strength is primary and concerns about underlying statistical qualities may be less important. Predictive strength is usually focused on how close our predictions come to the observed data, i.e., fidelity of the model predictions to the actual results. This section focuses on functions that can be used to measure predictive strength. However, our advice for those developing causal models is to use these techniques <em>even when the model will not be used with the primary goal of prediction</em>.</p>
<p>A longstanding issue with the practice of statistics is that, with a focus purely on causation, it is difficult to assess the credibility of a model. For example, consider Alzheimer’s disease data used to study the factors that influence cognitive impairment. Assume that we build a model, using the standard statistical approach. We would, normally, not even be required to calculate how closely this model fits the actual data.</p>
<!-- DK: This argument is important but awkwardly done. -->
<p>Using resampling methods, we can estimate the accuracy of this model to be about 73.3%. Accuracy is often a poor measure of model performance; we use it here because it is commonly understood. If the model has 73.3% fidelity to the data, should we trust the conclusions produced by the model? We might think so until we realize that the baseline rate of non-impaired patients in the data is 72.7%. This means that, despite our statistical analysis, a fancy model appears to be <em>only 0.6% better than a simple heuristic that always predicts patients to be unimpaired</em>, irregardless of the observed data.</p>
<!-- DK: Never seen this sort of object before. -->
<div class="rmdnote">
<p>
The point of this analysis is to demonstrate the idea that <strong>optimization of statistical characteristics of the model does not imply that the model fits the data well.</strong> Even for purely inferential models, some measure of fidelity to the data should accompany the inferential results. Using this, the consumers of the analyses can calibrate their expectations of the results of the statistical analysis.
</p>
</div>
<!-- DK: Next edition: Bring back multi-classification -->
<p>In the remainder of this section, general approaches for evaluating models via empirical validation are discussed. These approaches are grouped by the nature of the outcome data: purely numeric (Chapter <a href="continuous-response.html#continuous-response">11</a>) and binary classes (Chapter <a href="discrete-response.html#discrete-response">12</a>).</p>
</div>
<div id="regression-metrics" class="section level3">
<h3>
<span class="header-section-number">10.6.2</span> Regression metrics</h3>
<p>Recall from Section <a href="model-choice.html#parsnip-predictions">10.4.3</a> that tidymodels prediction functions produce tibbles with columns for the predicted values. These columns have consistent names, and the functions in the <strong>yardstick</strong> package that produce performance metrics have consistent interfaces. The functions are data frame-based, as opposed to vector-based, with the general syntax of:</p>
<div class="sourceCode" id="cb1074"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1074-1"><a href="model-choice.html#cb1074-1"></a><span class="cf">function</span>(data, truth, ...)</span></code></pre></div>
<p>where <code>data</code> is a data frame or tibble and <code>truth</code> is the column with the observed outcome values. The ellipses or other arguments are used to specify the column(s) containing the predictions.</p>
<p>To illustrate, let’s evaluate the model we have constructed for the Ames house price data. Recall that the <code>lm_fit</code> object was a linear regression model whose predictor set was supplemented with an interaction term. It was created from a training set (named <code>ames_train</code>). Although we do not advise using the test set at this juncture of the modeling process, it will be used to illustrate functionality and syntax. The data frame <code>ames_test</code> consists of 731 properties. To start, let’s produce predictions:</p>
<!-- DK: Is this correct? The pipe within new_data? -->
<div class="sourceCode" id="cb1075"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1075-1"><a href="model-choice.html#cb1075-1"></a>ames_test_res &lt;-<span class="st"> </span><span class="kw">predict</span>(lm_fit, <span class="dt">new_data =</span> ames_test <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1075-2"><a href="model-choice.html#cb1075-2"></a><span class="st">                           </span><span class="kw">select</span>(<span class="op">-</span>Sale_Price))</span>
<span id="cb1075-3"><a href="model-choice.html#cb1075-3"></a>ames_test_res</span></code></pre></div>
<pre><code>## # A tibble: 731 x 1
##    .pred
##    &lt;dbl&gt;
##  1  5.33
##  2  5.31
##  3  5.17
##  4  5.51
##  5  5.09
##  6  5.48
##  7  5.52
##  8  5.44
##  9  5.55
## 10  5.24
## # … with 721 more rows</code></pre>
<p>The predicted numeric outcome from the regression model is named <code>.pred</code>. Let’s match the predicted values with their corresponding observed outcome values:</p>
<div class="sourceCode" id="cb1077"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1077-1"><a href="model-choice.html#cb1077-1"></a>ames_test_res &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(ames_test_res, ames_test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(Sale_Price))</span>
<span id="cb1077-2"><a href="model-choice.html#cb1077-2"></a>ames_test_res</span></code></pre></div>
<pre><code>## # A tibble: 731 x 2
##    .pred Sale_Price
##    &lt;dbl&gt;      &lt;dbl&gt;
##  1  5.33       5.39
##  2  5.31       5.28
##  3  5.17       5.27
##  4  5.51       5.60
##  5  5.09       5.02
##  6  5.48       5.49
##  7  5.52       5.60
##  8  5.44       5.34
##  9  5.55       5.51
## 10  5.24       5.30
## # … with 721 more rows</code></pre>
<p>Note that both the predicted and observed outcomes are in log10 units. It is best practice to analyze the predictions on the transformed scale (if one were used) even if the predictions are reported using the original units.</p>
<p>Let’s plot the data before computing metrics:</p>
<div class="sourceCode" id="cb1079"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1079-1"><a href="model-choice.html#cb1079-1"></a><span class="kw">ggplot</span>(ames_test_res, <span class="kw">aes</span>(<span class="dt">x =</span> Sale_Price, <span class="dt">y =</span> .pred)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1079-2"><a href="model-choice.html#cb1079-2"></a><span class="st">  </span><span class="co"># Create a diagonal line:</span></span>
<span id="cb1079-3"><a href="model-choice.html#cb1079-3"></a><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">lty =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1079-4"><a href="model-choice.html#cb1079-4"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1079-5"><a href="model-choice.html#cb1079-5"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">"Predicted Sale Price (log10)"</span>, <span class="dt">x =</span> <span class="st">"Sale Price (log10)"</span>) <span class="op">+</span></span>
<span id="cb1079-6"><a href="model-choice.html#cb1079-6"></a><span class="st">  </span><span class="co"># Scale and size the x- and y-axis uniformly:</span></span>
<span id="cb1079-7"><a href="model-choice.html#cb1079-7"></a><span class="st">  </span><span class="kw">coord_obs_pred</span>()</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-851-1.png" width="672"></p>
<p>There is one property that is substantially over-predicted.</p>
<p>Let’s compute the root mean squared error for this model using the <code>rmse()</code> function:</p>
<div class="sourceCode" id="cb1080"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1080-1"><a href="model-choice.html#cb1080-1"></a><span class="kw">rmse</span>(ames_test_res, <span class="dt">truth =</span> Sale_Price, <span class="dt">estimate =</span> .pred)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard      0.0833</code></pre>
<p>The output above shows the standard format of the output of <strong>yardstick</strong> functions. Metrics for numeric outcomes usually have a value of “standard” for the <code>.estimator</code> column. Examples with different values for this column are shown below.</p>
<p>To compute multiple metrics at once, we can create a <em>metric set</em>. Let’s add <span class="math inline">\(R^2\)</span> and the mean absolute error:</p>
<div class="sourceCode" id="cb1082"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1082-1"><a href="model-choice.html#cb1082-1"></a>ames_metrics &lt;-<span class="st"> </span><span class="kw">metric_set</span>(rmse, rsq, mae)</span>
<span id="cb1082-2"><a href="model-choice.html#cb1082-2"></a><span class="kw">ames_metrics</span>(ames_test_res, <span class="dt">truth =</span> Sale_Price, <span class="dt">estimate =</span> .pred)</span></code></pre></div>
<pre><code>## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard      0.0833
## 2 rsq     standard      0.782 
## 3 mae     standard      0.0576</code></pre>
<p>This tidy data format stacks the metrics vertically.</p>
</div>
<div id="resampling" class="section level3">
<h3>
<span class="header-section-number">10.6.3</span> Resampling for evaluating performance</h3>
<p>We usually need to understand the effectiveness of the model <em>before using the test set</em>. In this section, we describe an approach called resampling that can fill this gap. Resampling estimates of performance can generalize to new data. The next chapter complements this one by demonstrating statistical methods that compare resampling results.</p>
<p>To motivate this chapter, the next section demonstrates how naive performance estimates can often fail.</p>
</div>
<div id="resampling-resubstition" class="section level3">
<h3>
<span class="header-section-number">10.6.4</span> The resubstitution approach</h3>
<p>Use the <code>ames</code> data, we have a recipe object named <code>ames_rec</code>, a linear model, and a workflow using that recipe and a model called <code>lm_wflow</code>. This workflow was fit on the training set, resulting in <code>lm_fit</code>.</p>
<p>For a comparison to this linear model, we can also fit a different type of model. <em>Random forests</em> are a tree ensemble method that operate by creating a large number of decision trees from slightly different versions of the training set. This collection of trees makes up the ensemble. When predicting a new sample, each ensemble member makes a separate prediction. These are averaged to create the final ensemble prediction for the new data point.</p>
<p>Random forest models are very powerful and they can emulate the underlying data patterns very closely. While this model can be computationally intensive, it is very low-maintenance. Very little preprocessing is required.</p>
<p>Using the same predictor set as the linear model (without the extra preprocessing steps), we can fit a random forest model to the training set using via the <strong>ranger</strong> package. This model requires no preprocessing so a simple formula can be used:</p>
<div class="sourceCode" id="cb1084"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1084-1"><a href="model-choice.html#cb1084-1"></a>rf_model &lt;-<span class="st"> </span></span>
<span id="cb1084-2"><a href="model-choice.html#cb1084-2"></a><span class="st">  </span><span class="kw">rand_forest</span>(<span class="dt">trees =</span> <span class="dv">1000</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1084-3"><a href="model-choice.html#cb1084-3"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">"ranger"</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1084-4"><a href="model-choice.html#cb1084-4"></a><span class="st">  </span><span class="kw">set_mode</span>(<span class="st">"regression"</span>)</span>
<span id="cb1084-5"><a href="model-choice.html#cb1084-5"></a></span>
<span id="cb1084-6"><a href="model-choice.html#cb1084-6"></a>rf_wflow &lt;-<span class="st"> </span></span>
<span id="cb1084-7"><a href="model-choice.html#cb1084-7"></a><span class="st">  </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1084-8"><a href="model-choice.html#cb1084-8"></a><span class="st">  </span><span class="kw">add_formula</span>(</span>
<span id="cb1084-9"><a href="model-choice.html#cb1084-9"></a>    Sale_Price <span class="op">~</span><span class="st"> </span>Neighborhood <span class="op">+</span><span class="st"> </span>Gr_Liv_Area <span class="op">+</span><span class="st"> </span>Year_Built <span class="op">+</span><span class="st"> </span>Bldg_Type <span class="op">+</span><span class="st"> </span></span>
<span id="cb1084-10"><a href="model-choice.html#cb1084-10"></a><span class="st">      </span>Latitude <span class="op">+</span><span class="st"> </span>Longitude) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1084-11"><a href="model-choice.html#cb1084-11"></a><span class="st">  </span><span class="kw">add_model</span>(rf_model) </span>
<span id="cb1084-12"><a href="model-choice.html#cb1084-12"></a></span>
<span id="cb1084-13"><a href="model-choice.html#cb1084-13"></a>rf_fit &lt;-<span class="st"> </span>rf_wflow <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="dt">data =</span> ames_train)</span></code></pre></div>
<p>How should the two models be compared? For demonstration, we will predict the training set to produce what is known as the “apparent error rate” or the “resubstitution error rate”. This function creates predictions and formats the results:</p>
<div class="sourceCode" id="cb1085"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1085-1"><a href="model-choice.html#cb1085-1"></a>estimate_perf &lt;-<span class="st"> </span><span class="cf">function</span>(model, dat) {</span>
<span id="cb1085-2"><a href="model-choice.html#cb1085-2"></a>  <span class="co"># Capture the names of the objects used</span></span>
<span id="cb1085-3"><a href="model-choice.html#cb1085-3"></a>  cl &lt;-<span class="st"> </span><span class="kw">match.call</span>()</span>
<span id="cb1085-4"><a href="model-choice.html#cb1085-4"></a>  obj_name &lt;-<span class="st"> </span><span class="kw">as.character</span>(cl<span class="op">$</span>model)</span>
<span id="cb1085-5"><a href="model-choice.html#cb1085-5"></a>  data_name &lt;-<span class="st"> </span><span class="kw">as.character</span>(cl<span class="op">$</span>dat)</span>
<span id="cb1085-6"><a href="model-choice.html#cb1085-6"></a>  data_name &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">"ames_"</span>, <span class="st">""</span>, data_name)</span>
<span id="cb1085-7"><a href="model-choice.html#cb1085-7"></a>  </span>
<span id="cb1085-8"><a href="model-choice.html#cb1085-8"></a>  <span class="co"># Estimate these metrics:</span></span>
<span id="cb1085-9"><a href="model-choice.html#cb1085-9"></a>  reg_metrics &lt;-<span class="st"> </span><span class="kw">metric_set</span>(rmse, rsq)</span>
<span id="cb1085-10"><a href="model-choice.html#cb1085-10"></a>  </span>
<span id="cb1085-11"><a href="model-choice.html#cb1085-11"></a>  model <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1085-12"><a href="model-choice.html#cb1085-12"></a><span class="st">    </span><span class="kw">predict</span>(dat) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1085-13"><a href="model-choice.html#cb1085-13"></a><span class="st">    </span><span class="kw">bind_cols</span>(dat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(Sale_Price)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1085-14"><a href="model-choice.html#cb1085-14"></a><span class="st">    </span><span class="kw">reg_metrics</span>(Sale_Price, .pred) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1085-15"><a href="model-choice.html#cb1085-15"></a><span class="st">    </span><span class="kw">select</span>(<span class="op">-</span>.estimator) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1085-16"><a href="model-choice.html#cb1085-16"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">object =</span> obj_name, <span class="dt">data =</span> data_name)</span>
<span id="cb1085-17"><a href="model-choice.html#cb1085-17"></a>}</span></code></pre></div>
<p>Both RMSE and R<sup>2</sup> are computed. The resubstitution statistics are:</p>
<div class="sourceCode" id="cb1086"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1086-1"><a href="model-choice.html#cb1086-1"></a><span class="kw">estimate_perf</span>(rf_fit, ames_train)</span></code></pre></div>
<pre><code>## # A tibble: 2 x 4
##   .metric .estimate object data 
##   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;
## 1 rmse       0.0349 rf_fit train
## 2 rsq        0.964  rf_fit train</code></pre>
<div class="sourceCode" id="cb1088"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1088-1"><a href="model-choice.html#cb1088-1"></a><span class="kw">estimate_perf</span>(lm_fit, ames_train)</span></code></pre></div>
<pre><code>## # A tibble: 2 x 4
##   .metric .estimate object data 
##   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;
## 1 rmse       0.0771 lm_fit train
## 2 rsq        0.809  lm_fit train</code></pre>
<p>Based on these results, the random forest is much more capable of predicting the sale prices; the RMSE estimate is 2.21-fold better than linear regression. If these two models were under consideration for this prediction problem, the random forest would probably be chosen. The next step applies the random forest model to the test set for final verification:</p>
<div class="sourceCode" id="cb1090"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1090-1"><a href="model-choice.html#cb1090-1"></a><span class="kw">estimate_perf</span>(rf_fit, ames_test)</span></code></pre></div>
<pre><code>## # A tibble: 2 x 4
##   .metric .estimate object data 
##   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;
## 1 rmse       0.0747 rf_fit test 
## 2 rsq        0.827  rf_fit test</code></pre>
<p>The test set RMSE estimate, 0.07, is <strong>much worse than the training set</strong> value of 0.03! Why did this happen?</p>
<p>Many predictive models are capable of learning complex trends from the data. In statistics, these are commonly referred to as <em>low bias models</em>.</p>
<p>For a low-bias model, the high degree of predictive capacity can sometimes result in the model nearly memorizing the training set data. As an obvious example, consider a 1-nearest neighbor model. It will always provide perfect predictions for the training set no matter how well it truly works for other data sets. Random forest models are similar; re-predicting the training set will always result in an artificially optimistic estimate of performance.</p>
<p>For both models, this table summarizes the RMSE estimate for the training and test sets:</p>
<pre><code>## # A tibble: 2 x 3
##   object  train   test
##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1 lm_fit 0.0771 0.0833
## 2 rf_fit 0.0349 0.0747</code></pre>
<p>Notice that the linear regression model is consistent between training and testing, because of its limited complexity.The main take-away from this example is that re-predicting the training set is a <strong>bad idea</strong> for most models.</p>
<p>If the test set should not be used immediately, and re-predicting the training set is a bad idea, what should be done? <em>Resampling methods</em>, such as cross-validation or validation sets, are the solution.</p>
</div>
<div id="resampling-methods" class="section level3">
<h3>
<span class="header-section-number">10.6.5</span> Resampling methods</h3>
<p>Resampling methods are empirical simulation systems that emulate the process of using some data for modeling and different data for evaluation. Most resampling methods are iterative, meaning that this process is repeated multiple times. This diagram illustrates how resampling methods generally operate:</p>
<p><img src="10-model-choice/images/resampling.svg"></p>
<p><strong>Resampling is only conducted on the training set</strong>. The test set is not involved. For each iteration of resampling, the data are partitioned into two subsamples:</p>
<ul>
<li><p>The model is fit with the <strong>analysis set</strong>.</p></li>
<li><p>The model is evaluated with the <strong>assessment set</strong>.</p></li>
</ul>
<p>These are somewhat analogous to training and test sets. Our language of <em>analysis</em> and <em>assessment</em> avoids confusion with initial split of the data. These data sets are mutually exclusive. The partitioning scheme used to create the analysis and assessment sets is usually the defining characteristic of the method.</p>
<p>Suppose twenty iterations of resampling are conducted. This means that twenty separate models are fit on the analysis sets and the corresponding assessment sets produce twenty sets of performance statistics. The final estimate of performance for a model is the average of the twenty replicates of the statistics. This average has very good generalization properties and is far better than the resubstituion estimates.</p>
<p>The next section defines several commonly used methods and discusses their pros and cons.</p>
<div id="cv" class="section level4">
<h4>
<span class="header-section-number">10.6.5.1</span> Cross-validation</h4>
<p>Cross-validation is a well established resampling method. While there are a number of variations, the most common cross-validation method is <em>V</em>-fold cross-validation. The data are randomly partitioned into <em>V</em> sets of roughly equal size (called the “folds”). For illustration, <em>V</em> = 3 is shown below for a data set of thirty training set points with random fold allocations. The number inside the symbols is the sample number:</p>
<p><img src="10-model-choice/images/three-CV.svg"></p>
<p>The color of the symbols represent their randomly assigned folds. Stratified sampling is also an option for assigning folds.</p>
<p>For 3-fold cross-validation, the three iterations of resampling are illustrated below. For each iteration, one fold is held out for assessment statistics and the remaining folds are substrate for the model. This process continues for each fold so that three models produce three sets of performance statistics.</p>
<p><img src="10-model-choice/images/three-CV-iter.svg"></p>
<p>When <em>V</em> = 3, the analysis sets are 2/3 of the training set and each assessment set is a distinct 1/3. The final resampling estimate of performance averages each of the <em>V</em> replicates.</p>
<p>Using <em>V</em> = 3 is a good choice to illustrate cross-validation but is a poor choice in practice. Values of <em>V</em> are most often 5 or 10; we generally prefer 10-fold cross-validation as a default.</p>
<p>The primary input is the training set data frame as well as the number of folds (defaulting to 10):</p>
<div class="sourceCode" id="cb1093"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1093-1"><a href="model-choice.html#cb1093-1"></a><span class="kw">set.seed</span>(<span class="dv">55</span>)</span>
<span id="cb1093-2"><a href="model-choice.html#cb1093-2"></a>ames_folds &lt;-<span class="st"> </span><span class="kw">vfold_cv</span>(ames_train, <span class="dt">v =</span> <span class="dv">10</span>)</span>
<span id="cb1093-3"><a href="model-choice.html#cb1093-3"></a>ames_folds</span></code></pre></div>
<pre><code>## #  10-fold cross-validation 
## # A tibble: 10 x 2
##    splits           id    
##    &lt;list&gt;           &lt;chr&gt; 
##  1 &lt;split [2K/220]&gt; Fold01
##  2 &lt;split [2K/220]&gt; Fold02
##  3 &lt;split [2K/220]&gt; Fold03
##  4 &lt;split [2K/220]&gt; Fold04
##  5 &lt;split [2K/220]&gt; Fold05
##  6 &lt;split [2K/220]&gt; Fold06
##  7 &lt;split [2K/220]&gt; Fold07
##  8 &lt;split [2K/220]&gt; Fold08
##  9 &lt;split [2K/220]&gt; Fold09
## 10 &lt;split [2K/219]&gt; Fold10</code></pre>
<p>The column named <code>splits</code> contains the information on how to split the data (similar to the object used to create the initial training/test partition). While each row of <code>splits</code> has an embedded copy of the entire training set, R is smart enough not to make copies of the data in memory<label for="tufte-sn-12" class="margin-toggle sidenote-number">12</label><input type="checkbox" id="tufte-sn-12" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">12</span> To see this for yourself, try executing <code>lobstr::obj_size(ames_folds)</code> and <code>lobstr::obj_size(ames_train)</code>. The size of the resample object is much less than ten times the size of the original data.</span>. The print method inside of the tibble shows the frequency of each: <code>[2K/220]</code> indicates that roughly two thousand samples are in the analysis set and 220 are in that particular assessment set.</p>
<p>To manually retrieve the partitioned data, the <code>analysis()</code> and <code>assessment()</code> functions return the corresponding data frames:</p>
<div class="sourceCode" id="cb1095"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1095-1"><a href="model-choice.html#cb1095-1"></a><span class="co"># For the first fold:</span></span>
<span id="cb1095-2"><a href="model-choice.html#cb1095-2"></a>ames_folds<span class="op">$</span>splits[[<span class="dv">1</span>]] <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1095-3"><a href="model-choice.html#cb1095-3"></a><span class="st">  </span><span class="kw">analysis</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1095-4"><a href="model-choice.html#cb1095-4"></a><span class="st">  </span><span class="kw">dim</span>()</span></code></pre></div>
<pre><code>## [1] 1979   74</code></pre>
<p>The <strong>tidymodels</strong> packages, such as <strong>tune</strong>, contain high-level user interfaces so that functions like <code>analysis()</code> are not generally needed for day-to-day work. Section <a href="model-choice.html#resampling-performance">10.6.6</a> demonstrates a function to fit a model over these resamples.</p>
<p>These <strong>rsample</strong> objects also always contain a character column called <code>id</code> that labels the partition. Some resampling methods require multiple <code>id</code> fields.</p>
</div>
<div id="repeated-cross-validation" class="section level4 unnumbered">
<h4>Repeated cross-validation</h4>
<p>There are a variety of variations on cross-validation. The most important is <em>repeated</em> <em>V</em>-fold cross-validation. Depending on the size or other characteristics of the data, the resampling estimate produced by <em>V</em>-fold cross-validation may be excessively noisy. As with many statistical problems, one way to reduce noise is to gather more data. For cross-validation, this means averaging more than <em>V</em> statistics.</p>
<p>To create <em>R</em> repeats of <em>V</em>-fold cross-validation, the same fold generation process is done <em>R</em> times to generate <em>R</em> collections of <em>V</em> partitions. Now, instead of averaging <em>V</em> statistics, <span class="math inline">\(V \times R\)</span> statistics produce the final resampling estimate. Due to the Central Limit Theorem, the summary statistics from each model tend toward a normal distribution.</p>
<p>Consider the Ames data. On average, 10-fold cross-validation uses assessment sets that contain roughly 219 properties. If RMSE is the statistic of choice, we can denote that estimate’s standard deviation as <span class="math inline">\(\sigma\)</span>. With simple 10-fold cross-validation, the standard error of the mean RMSE is <span class="math inline">\(\sigma/\sqrt{10}\)</span>. If this is too noisy, repeats reduce the standard error to <span class="math inline">\(\sigma/\sqrt{10R}\)</span>. For 10-fold cross-validation with <span class="math inline">\(R\)</span> replicates, the plot below shows how quickly the standard error<label for="tufte-sn-13" class="margin-toggle sidenote-number">13</label><input type="checkbox" id="tufte-sn-13" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">13</span> These are <em>approximate</em> standard errors. As will be discussed in the next chapter, there is a within-replicate correlation that is typical of resampled results. By ignoring this extra component of variation, the simple calculations shown in this plot are overestimates of the reduction in noise in the standard errors.</span> decreases with replicates:</p>
<p><img src="book_temp_files/figure-html/unnamed-chunk-857-1.png" width="672"></p>
<p>Larger number of replicates tend to have less impact on the standard error. However, if the baseline value of <span class="math inline">\(\sigma\)</span> is impractically large, the diminishing returns on replication may still be worth the extra computational costs.</p>
<p>To create repeats, invoke <code>vfold_cv()</code> with an additional argument <code>repeats</code>:</p>
<div class="sourceCode" id="cb1097"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1097-1"><a href="model-choice.html#cb1097-1"></a><span class="kw">vfold_cv</span>(ames_train, <span class="dt">v =</span> <span class="dv">10</span>, <span class="dt">repeats =</span> <span class="dv">5</span>)</span></code></pre></div>
<pre><code>## #  10-fold cross-validation repeated 5 times 
## # A tibble: 50 x 3
##    splits           id      id2   
##    &lt;list&gt;           &lt;chr&gt;   &lt;chr&gt; 
##  1 &lt;split [2K/220]&gt; Repeat1 Fold01
##  2 &lt;split [2K/220]&gt; Repeat1 Fold02
##  3 &lt;split [2K/220]&gt; Repeat1 Fold03
##  4 &lt;split [2K/220]&gt; Repeat1 Fold04
##  5 &lt;split [2K/220]&gt; Repeat1 Fold05
##  6 &lt;split [2K/220]&gt; Repeat1 Fold06
##  7 &lt;split [2K/220]&gt; Repeat1 Fold07
##  8 &lt;split [2K/220]&gt; Repeat1 Fold08
##  9 &lt;split [2K/220]&gt; Repeat1 Fold09
## 10 &lt;split [2K/219]&gt; Repeat1 Fold10
## # … with 40 more rows</code></pre>
</div>
<div id="leave-one-out-cross-validation" class="section level4 unnumbered">
<h4>Leave-one-out cross-validation</h4>
<p>One early variation of cross-validation was leave-one-out (LOO) cross-validation where <em>V</em> is the number of data points in the training set. If there are <span class="math inline">\(n\)</span> training set samples, <span class="math inline">\(n\)</span> models are fit using <span class="math inline">\(n-1\)</span> rows of the training set. Each model predicts the single excluded data point. At the end of resampling, the <span class="math inline">\(n\)</span> predictions are pooled to produce a single performance statistic.</p>
<p>Leave-one-out methods are deficient compared to almost any other method. For anything but pathologically small samples, LOO is computationally excessive and it may not have good statistical properties. Although <strong>rsample</strong> contains a <code>loo_cv()</code> function, these objects are not generally integrated into the broader tidymodels frameworks.</p>
</div>
<div id="monte-carlo-cross-validation" class="section level4 unnumbered">
<h4>
<em>Monte Carlo</em> cross-validation</h4>
<p>Finally, another variant of <em>V</em>-fold cross-validation is <em>Monte Carlo</em> cross-validation. Like <em>V</em>-fold cross-validation, it allocates a fixed proportion of data to the assessment sets. The difference is that, for MCCV, this proportion of the data is randomly selected each time. This results in assessment sets that are not mutually exclusive. To create these resampling objects:</p>
<div class="sourceCode" id="cb1099"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1099-1"><a href="model-choice.html#cb1099-1"></a><span class="kw">mc_cv</span>(ames_train, <span class="dt">prop =</span> <span class="dv">9</span><span class="op">/</span><span class="dv">10</span>, <span class="dt">times =</span> <span class="dv">20</span>)</span></code></pre></div>
<pre><code>## # Monte Carlo cross-validation (0.9/0.1) with 20 resamples  
## # A tibble: 20 x 2
##    splits           id        
##    &lt;list&gt;           &lt;chr&gt;     
##  1 &lt;split [2K/219]&gt; Resample01
##  2 &lt;split [2K/219]&gt; Resample02
##  3 &lt;split [2K/219]&gt; Resample03
##  4 &lt;split [2K/219]&gt; Resample04
##  5 &lt;split [2K/219]&gt; Resample05
##  6 &lt;split [2K/219]&gt; Resample06
##  7 &lt;split [2K/219]&gt; Resample07
##  8 &lt;split [2K/219]&gt; Resample08
##  9 &lt;split [2K/219]&gt; Resample09
## 10 &lt;split [2K/219]&gt; Resample10
## 11 &lt;split [2K/219]&gt; Resample11
## 12 &lt;split [2K/219]&gt; Resample12
## 13 &lt;split [2K/219]&gt; Resample13
## 14 &lt;split [2K/219]&gt; Resample14
## 15 &lt;split [2K/219]&gt; Resample15
## 16 &lt;split [2K/219]&gt; Resample16
## 17 &lt;split [2K/219]&gt; Resample17
## 18 &lt;split [2K/219]&gt; Resample18
## 19 &lt;split [2K/219]&gt; Resample19
## 20 &lt;split [2K/219]&gt; Resample20</code></pre>
</div>
<div id="validation" class="section level4">
<h4>
<span class="header-section-number">10.6.5.2</span> Validation sets</h4>
<p>Previously mentioned in Section <a href="#what-about-a-validation-set"><strong>??</strong></a>, a validation set is a single partition that is set aside to estimate performance, before using the test set:</p>
<p><img src="10-model-choice/images/validation.svg"></p>
<p>Validation sets are often used when the original pool of data is very large. In this case, a single large partition may be adequate to characterize model performance without having to do multiple iterations of resampling.</p>
<p>With <strong>rsample</strong>, a validation set is like any other resampling object; this type is different only in that it has a single iteration<label for="tufte-sn-14" class="margin-toggle sidenote-number">14</label><input type="checkbox" id="tufte-sn-14" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">14</span> In essence, a validation set can be considered a single iteration of Monte Carlo cross-validation.</span>:</p>
<p><img src="10-model-choice/images/validation-alt.svg"></p>
<p>To create a validation set object that uses 3/4 of the data for model fitting:</p>
<div class="sourceCode" id="cb1101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1101-1"><a href="model-choice.html#cb1101-1"></a><span class="kw">set.seed</span>(<span class="dv">12</span>)</span>
<span id="cb1101-2"><a href="model-choice.html#cb1101-2"></a>val_set &lt;-<span class="st"> </span><span class="kw">validation_split</span>(ames_train, <span class="dt">prop =</span> <span class="dv">3</span><span class="op">/</span><span class="dv">4</span>)</span>
<span id="cb1101-3"><a href="model-choice.html#cb1101-3"></a>val_set</span></code></pre></div>
<pre><code>## # Validation Set Split (0.75/0.25)  
## # A tibble: 1 x 2
##   splits             id        
##   &lt;list&gt;             &lt;chr&gt;     
## 1 &lt;split [1.6K/549]&gt; validation</code></pre>
</div>
<div id="bootstrap" class="section level4">
<h4>
<span class="header-section-number">10.6.5.3</span> Bootstrapping</h4>
<p>Bootstrap resampling was originally invented as a method for approximating the sampling distribution of statistics whose theoretical properties are intractable. Using it to estimate model performance is a secondary application of the method.</p>
<p>A bootstrap sample of the training set is a sample that is the same size as the training set but is drawn <em>with replacement</em>. This means that some training set data points are selected multiple times for the analysis set. Each data point has a 63.2% chance of inclusion in the training set <em>at least once</em>. The assessment set contains all of the training set samples that were not selected for the analysis set (on average, with 36.8% of the training set). When bootstrapping, the assessment set is often called the “out-of-bag” sample.</p>
<p>For a training set of 30 samples, a schematic of three bootstrap samples is:</p>
<p><img src="10-model-choice/images/bootstraps.svg">
Note that the sizes of the assessment sets vary.</p>
<p>Using <strong>rsample</strong>:</p>
<div class="sourceCode" id="cb1103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1103-1"><a href="model-choice.html#cb1103-1"></a><span class="kw">bootstraps</span>(ames_train, <span class="dt">times =</span> <span class="dv">5</span>)</span></code></pre></div>
<pre><code>## # Bootstrap sampling 
## # A tibble: 5 x 2
##   splits             id        
##   &lt;list&gt;             &lt;chr&gt;     
## 1 &lt;split [2.2K/790]&gt; Bootstrap1
## 2 &lt;split [2.2K/814]&gt; Bootstrap2
## 3 &lt;split [2.2K/797]&gt; Bootstrap3
## 4 &lt;split [2.2K/827]&gt; Bootstrap4
## 5 &lt;split [2.2K/794]&gt; Bootstrap5</code></pre>
<p>Bootstrap samples produce performance estimates that have very low variance (unlike cross-validation) but have significant pessimistic bias. This means that, if the true accuracy of a model is 90%, the bootstrap would tend to estimate the value to be less than 90%. The amount of bias cannot be empirically determined with sufficient accuracy. Additionally, the amount of bias changes over the scale of the performance metric. For example, the bias is likely to be different when the accuracy is 90% versus when it is 70%.</p>
<p>The bootstrap is also used inside of many models. For example, the random forest model mentioned earlier contained 1,000 individual decision trees. Each tree was the product of a different bootstrap sample of the training set.</p>
</div>
<div id="rolling" class="section level4">
<h4>
<span class="header-section-number">10.6.5.4</span> Rolling forecasting origin resampling</h4>
<p>When the data have a strong time component, a resampling method should support modeling to estimate seasonal and other temporal trends within the data. A technique that randomly samples values from the training set can disrupt the model’s ability to estimate these patterns.</p>
<p>Rolling forecast origin resampling provides a method that emulates how time series data is often partitioned in practice, estimating the model with historical data and evaluating it with the most recent data. For this type of resampling, the size of the initial analysis and assessment sets are specified. The first iteration of resampling uses these sizes, starting from the beginning of the series. The second iteration uses the same data sizes but shifts over by a set number of samples.</p>
<p>To illustrate, a training set of fifteen samples was resampled with an analysis size of eight samples and an assessment set size of three. The second iteration discards the first training set sample and both data sets shift forward by one. This configuration results in five resamples:</p>
<p><img src="10-model-choice/images/rolling.svg"></p>
<p>There are a few different configurations of this method:</p>
<ul>
<li><p>The analysis set can cumulatively grow (as opposed to remaining the same size). After the first initial analysis set, new samples can accrue without discarding the earlier data.</p></li>
<li><p>The resamples need not increment by one. For example, for large data sets, the incremental block could be a week or month instead of a day.</p></li>
</ul>
<p>For a year’s worth of data, suppose that six sets of 30-day blocks define the analysis set. For assessment sets of 30 days with a 29 day skip, the <strong>rsample</strong> code is:</p>
<div class="sourceCode" id="cb1105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1105-1"><a href="model-choice.html#cb1105-1"></a>time_slices &lt;-<span class="st"> </span></span>
<span id="cb1105-2"><a href="model-choice.html#cb1105-2"></a><span class="st">  </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">365</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1105-3"><a href="model-choice.html#cb1105-3"></a><span class="st">  </span><span class="kw">rolling_origin</span>(<span class="dt">initial =</span> <span class="dv">6</span> <span class="op">*</span><span class="st"> </span><span class="dv">30</span>, <span class="dt">assess =</span> <span class="dv">30</span>, <span class="dt">skip =</span> <span class="dv">29</span>, <span class="dt">cumulative =</span> <span class="ot">FALSE</span>)</span>
<span id="cb1105-4"><a href="model-choice.html#cb1105-4"></a></span>
<span id="cb1105-5"><a href="model-choice.html#cb1105-5"></a>data_range &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</span>
<span id="cb1105-6"><a href="model-choice.html#cb1105-6"></a>  <span class="kw">summarize</span>(x, <span class="dt">first =</span> <span class="kw">min</span>(x), <span class="dt">last =</span> <span class="kw">max</span>(x))</span>
<span id="cb1105-7"><a href="model-choice.html#cb1105-7"></a>}</span>
<span id="cb1105-8"><a href="model-choice.html#cb1105-8"></a></span>
<span id="cb1105-9"><a href="model-choice.html#cb1105-9"></a><span class="kw">map_dfr</span>(time_slices<span class="op">$</span>splits, <span class="op">~</span><span class="st">   </span><span class="kw">analysis</span>(.x) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">data_range</span>())</span></code></pre></div>
<pre><code>## # A tibble: 6 x 2
##   first  last
##   &lt;int&gt; &lt;int&gt;
## 1     1   180
## 2    31   210
## 3    61   240
## 4    91   270
## 5   121   300
## 6   151   330</code></pre>
<div class="sourceCode" id="cb1107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1107-1"><a href="model-choice.html#cb1107-1"></a><span class="kw">map_dfr</span>(time_slices<span class="op">$</span>splits, <span class="op">~</span><span class="st"> </span><span class="kw">assessment</span>(.x) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">data_range</span>())</span></code></pre></div>
<pre><code>## # A tibble: 6 x 2
##   first  last
##   &lt;int&gt; &lt;int&gt;
## 1   181   210
## 2   211   240
## 3   241   270
## 4   271   300
## 5   301   330
## 6   331   360</code></pre>
</div>
</div>
<div id="resampling-performance" class="section level3">
<h3>
<span class="header-section-number">10.6.6</span> Estimating performance</h3>
<p>Any of these resampling methods can be used to evaluate the modeling process (including preprocessing, model fitting, etc). These methods are effective because different groups of data are used to train the model and assess the model. To reiterate the process:</p>
<ol style="list-style-type: decimal">
<li><p>During resampling, the analysis set is used to preprocess the data, apply the preprocessing to itself, and use these processed data to fit the model.</p></li>
<li><p>The preprocessing statistics produced by the analysis set are applied to the assessment set. The predictions from the assessment set estimate performance.</p></li>
</ol>
<p>This sequence repeats for every resample. If there are <em>B</em> resamples, there are <em>B</em> replicates of each of the performance metrics. The final resampling estimate is the average of these <em>B</em> statistics. If <em>B</em> = 1, as with a validation set, the individual statistics represent overall performance.</p>
<p>Let’s reconsider the previous random forest model contained in the <code>rf_wflow</code> object. The <code>fit_resamples()</code> function is analogous to <code>fit()</code>, but instead of having a <code>data</code> argument, <code>fit_resamples()</code> has <code>resamples</code> which expects an <code>rset</code> object like the ones shown above. The possible interfaces to the function are:</p>
<div class="sourceCode" id="cb1109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1109-1"><a href="model-choice.html#cb1109-1"></a>model_spec <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit_resamples</span>(formula,  resamples, ...)</span>
<span id="cb1109-2"><a href="model-choice.html#cb1109-2"></a>model_spec <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit_resamples</span>(recipe,   resamples, ...)</span>
<span id="cb1109-3"><a href="model-choice.html#cb1109-3"></a>workflow   <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit_resamples</span>(          resamples, ...)</span></code></pre></div>
<p>There are a number of other optional arguments, such as:</p>
<ul>
<li><p><code>metrics</code>: A metric set of performance statistics to compute. By default, regression models use RMSE and R<sup>2</sup> while classification models compute the area under the ROC curve and overall accuracy. Note that this choice also defines what predictions are produced during the evaluation of the model. For classification, if only accuracy is requested, class probability estimates are not generated for the assessment set (since they are not needed).</p></li>
<li><p><code>control</code>: A list created by <code>control_resamples()</code> with various options.</p></li>
</ul>
<p>The control arguments include:</p>
<ul>
<li><p><code>verbose</code>: A logical for printing logging.</p></li>
<li><p><code>extract</code>: A function for retaining objects from each model iteration (discussed below).</p></li>
<li><p><code>save_pred</code>: A logical for saving the assessment set predictions.</p></li>
</ul>
<p>For our example, let’s save the predictions in order to visualize the model fit and residuals:</p>
<div class="sourceCode" id="cb1110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1110-1"><a href="model-choice.html#cb1110-1"></a>keep_pred &lt;-<span class="st"> </span><span class="kw">control_resamples</span>(<span class="dt">save_pred =</span> <span class="ot">TRUE</span>)</span>
<span id="cb1110-2"><a href="model-choice.html#cb1110-2"></a></span>
<span id="cb1110-3"><a href="model-choice.html#cb1110-3"></a><span class="kw">set.seed</span>(<span class="dv">130</span>)</span>
<span id="cb1110-4"><a href="model-choice.html#cb1110-4"></a>rf_res &lt;-<span class="st"> </span></span>
<span id="cb1110-5"><a href="model-choice.html#cb1110-5"></a><span class="st">  </span>rf_wflow <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1110-6"><a href="model-choice.html#cb1110-6"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">resamples =</span> ames_folds, <span class="dt">control =</span> keep_pred)</span>
<span id="cb1110-7"><a href="model-choice.html#cb1110-7"></a>rf_res</span></code></pre></div>
<pre><code>## # Resampling results
## # 10-fold cross-validation 
## # A tibble: 10 x 5
##    splits           id     .metrics         .notes           .predictions      
##    &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;            
##  1 &lt;split [2K/220]&gt; Fold01 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [220 × 3]&gt;
##  2 &lt;split [2K/220]&gt; Fold02 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [220 × 3]&gt;
##  3 &lt;split [2K/220]&gt; Fold03 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [220 × 3]&gt;
##  4 &lt;split [2K/220]&gt; Fold04 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [220 × 3]&gt;
##  5 &lt;split [2K/220]&gt; Fold05 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [220 × 3]&gt;
##  6 &lt;split [2K/220]&gt; Fold06 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [220 × 3]&gt;
##  7 &lt;split [2K/220]&gt; Fold07 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [220 × 3]&gt;
##  8 &lt;split [2K/220]&gt; Fold08 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [220 × 3]&gt;
##  9 &lt;split [2K/220]&gt; Fold09 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [220 × 3]&gt;
## 10 &lt;split [2K/219]&gt; Fold10 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [219 × 3]&gt;</code></pre>
<!-- DK: Need to deal with this save() failure. -->
<p>The return value is a tibble similar to the input resamples, along with some extra columns:</p>
<ul>
<li><p><code>.metrics</code> is a list column of tibbles containing the assessment set performance statistics.</p></li>
<li><p><code>.notes</code> is another list column of tibbles cataloging any warnings or errors generated during resampling. Note that errors will not stop subsequent execution of resampling.</p></li>
<li><p><code>.predictions</code> is present when <code>save_pred = TRUE</code>. This list column contains tibbles with the out-of-sample predictions.</p></li>
</ul>
<p>While these list columns may look daunting, they can be easily reconfigured using <strong>tidyr</strong> or with convenience functions that tidymodels provides. For example, to return the performance metrics in a more usable format:</p>
<div class="sourceCode" id="cb1112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1112-1"><a href="model-choice.html#cb1112-1"></a><span class="kw">collect_metrics</span>(rf_res)</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric .estimator   mean     n std_err
##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 rmse    standard   0.0708    10 0.00176
## 2 rsq     standard   0.840     10 0.00829</code></pre>
<p>These are the resampling estimates averaged over the individual replicates. To get the metrics for each resample, use the option <code>summarize = FALSE</code></p>
<p>Notice how much more realistic the performance estimates are than the resubstitution estimates from Section <a href="model-choice.html#resampling-resubstition">10.6.4</a>!</p>
<p>To obtain the assessment set predictions:</p>
<div class="sourceCode" id="cb1114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1114-1"><a href="model-choice.html#cb1114-1"></a>assess_res &lt;-<span class="st"> </span><span class="kw">collect_predictions</span>(rf_res)</span>
<span id="cb1114-2"><a href="model-choice.html#cb1114-2"></a>assess_res</span></code></pre></div>
<pre><code>## # A tibble: 2,199 x 4
##    id     .pred  .row Sale_Price
##    &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;
##  1 Fold01  5.24    11       5.23
##  2 Fold01  5.36    47       5.35
##  3 Fold01  5.42    52       5.46
##  4 Fold01  5.15    94       5.15
##  5 Fold01  5.16   111       5.15
##  6 Fold01  5.13   115       5.04
##  7 Fold01  5.11   117       5.16
##  8 Fold01  5.10   118       5.11
##  9 Fold01  5.15   134       5.03
## 10 Fold01  5.06   140       5.18
## # … with 2,189 more rows</code></pre>
<p>The prediction column names follow the conventions discussed for <strong>parsnip</strong> models. The observed outcome column always uses the original column name from the source data. The <code>.row</code> column is an integer that matches the row of the original training set so that these results can be properly arranged and joined with the original data.</p>
<p>Since this analysis used 10-fold cross-validation, there is one unique prediction for each training set sample. These data can generate helpful plots of the model to understand where it potentially failed. For example, let’s compare the observed and predicted values:</p>
<div class="sourceCode" id="cb1116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1116-1"><a href="model-choice.html#cb1116-1"></a>assess_res <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1116-2"><a href="model-choice.html#cb1116-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Sale_Price, <span class="dt">y =</span> .pred)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1116-3"><a href="model-choice.html#cb1116-3"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">.15</span>) <span class="op">+</span></span>
<span id="cb1116-4"><a href="model-choice.html#cb1116-4"></a><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">col =</span> <span class="st">"red"</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb1116-5"><a href="model-choice.html#cb1116-5"></a><span class="st">  </span><span class="kw">coord_obs_pred</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb1116-6"><a href="model-choice.html#cb1116-6"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">"Predicted"</span>)</span></code></pre></div>
<p><img src="book_temp_files/figure-html/unnamed-chunk-862-1.png" width="672"></p>
<p>There was one house in the training set with a low observed sale price that is significantly overpredicted by the model. Which house was that?</p>
<div class="sourceCode" id="cb1117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1117-1"><a href="model-choice.html#cb1117-1"></a>over_predicted &lt;-<span class="st"> </span></span>
<span id="cb1117-2"><a href="model-choice.html#cb1117-2"></a><span class="st">  </span>assess_res <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1117-3"><a href="model-choice.html#cb1117-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">residual =</span> Sale_Price <span class="op">-</span><span class="st"> </span>.pred) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1117-4"><a href="model-choice.html#cb1117-4"></a><span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(<span class="kw">abs</span>(residual))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1117-5"><a href="model-choice.html#cb1117-5"></a><span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span>)</span>
<span id="cb1117-6"><a href="model-choice.html#cb1117-6"></a>over_predicted</span></code></pre></div>
<pre><code>## # A tibble: 1 x 5
##   id     .pred  .row Sale_Price residual
##   &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt;
## 1 Fold03  4.94  1165       4.12   -0.822</code></pre>
<div class="sourceCode" id="cb1119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1119-1"><a href="model-choice.html#cb1119-1"></a>ames_train <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1119-2"><a href="model-choice.html#cb1119-2"></a><span class="st">  </span><span class="kw">slice</span>(over_predicted<span class="op">$</span>.row) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1119-3"><a href="model-choice.html#cb1119-3"></a><span class="st">  </span><span class="kw">select</span>(Gr_Liv_Area, Neighborhood, Year_Built, Bedroom_AbvGr, Full_Bath)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 5
##   Gr_Liv_Area Neighborhood           Year_Built Bedroom_AbvGr Full_Bath
##         &lt;int&gt; &lt;fct&gt;                       &lt;int&gt;         &lt;int&gt;     &lt;int&gt;
## 1         733 Iowa_DOT_and_Rail_Road       1952             2         1</code></pre>
<p>These results can help us investigate why the prediction was poor for this house.</p>
<p>How can we use a validation set instead of cross-validation? From our previous <strong>rsample</strong> object:</p>
<div class="sourceCode" id="cb1121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1121-1"><a href="model-choice.html#cb1121-1"></a>val_res &lt;-<span class="st"> </span>rf_wflow <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit_resamples</span>(<span class="dt">resamples =</span> val_set)</span>
<span id="cb1121-2"><a href="model-choice.html#cb1121-2"></a>val_res</span></code></pre></div>
<pre><code>## # Resampling results
## # Validation Set Split (0.75/0.25)  
## # A tibble: 1 x 4
##   splits             id         .metrics         .notes          
##   &lt;list&gt;             &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;          
## 1 &lt;split [1.6K/549]&gt; validation &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;</code></pre>
<div class="sourceCode" id="cb1123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1123-1"><a href="model-choice.html#cb1123-1"></a><span class="kw">collect_metrics</span>(val_res)</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric .estimator   mean     n std_err
##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 rmse    standard   0.0671     1      NA
## 2 rsq     standard   0.858      1      NA</code></pre>
<p>These results are also much closer to the test set results than the resubstitution estimates of performance.</p>
<p>Our code so far:</p>
<div class="sourceCode" id="cb1125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1125-1"><a href="model-choice.html#cb1125-1"></a><span class="kw">library</span>(tidymodels)</span>
<span id="cb1125-2"><a href="model-choice.html#cb1125-2"></a><span class="kw">data</span>(ames)</span>
<span id="cb1125-3"><a href="model-choice.html#cb1125-3"></a>ames &lt;-<span class="st"> </span><span class="kw">mutate</span>(ames, <span class="dt">Sale_Price =</span> <span class="kw">log10</span>(Sale_Price))</span>
<span id="cb1125-4"><a href="model-choice.html#cb1125-4"></a></span>
<span id="cb1125-5"><a href="model-choice.html#cb1125-5"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1125-6"><a href="model-choice.html#cb1125-6"></a>ames_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(ames, <span class="dt">prob =</span> <span class="fl">0.80</span>, <span class="dt">strata =</span> Sale_Price)</span>
<span id="cb1125-7"><a href="model-choice.html#cb1125-7"></a>ames_train &lt;-<span class="st"> </span><span class="kw">training</span>(ames_split)</span>
<span id="cb1125-8"><a href="model-choice.html#cb1125-8"></a>ames_test  &lt;-<span class="st">  </span><span class="kw">testing</span>(ames_split)</span>
<span id="cb1125-9"><a href="model-choice.html#cb1125-9"></a></span>
<span id="cb1125-10"><a href="model-choice.html#cb1125-10"></a>ames_rec &lt;-<span class="st"> </span></span>
<span id="cb1125-11"><a href="model-choice.html#cb1125-11"></a><span class="st">  </span><span class="kw">recipe</span>(Sale_Price <span class="op">~</span><span class="st"> </span>Neighborhood <span class="op">+</span><span class="st"> </span>Gr_Liv_Area <span class="op">+</span><span class="st"> </span>Year_Built <span class="op">+</span><span class="st"> </span>Bldg_Type <span class="op">+</span><span class="st"> </span></span>
<span id="cb1125-12"><a href="model-choice.html#cb1125-12"></a><span class="st">           </span>Latitude <span class="op">+</span><span class="st"> </span>Longitude, <span class="dt">data =</span> ames_train) <span class="op">%&gt;%</span></span>
<span id="cb1125-13"><a href="model-choice.html#cb1125-13"></a><span class="st">  </span><span class="kw">step_log</span>(Gr_Liv_Area, <span class="dt">base =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1125-14"><a href="model-choice.html#cb1125-14"></a><span class="st">  </span><span class="kw">step_other</span>(Neighborhood, <span class="dt">threshold =</span> <span class="fl">0.01</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1125-15"><a href="model-choice.html#cb1125-15"></a><span class="st">  </span><span class="kw">step_dummy</span>(<span class="kw">all_nominal</span>()) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1125-16"><a href="model-choice.html#cb1125-16"></a><span class="st">  </span><span class="kw">step_interact</span>( <span class="op">~</span><span class="st"> </span>Gr_Liv_Area<span class="op">:</span><span class="kw">starts_with</span>(<span class="st">"Bldg_Type_"</span>) ) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1125-17"><a href="model-choice.html#cb1125-17"></a><span class="st">  </span><span class="kw">step_ns</span>(Latitude, Longitude, <span class="dt">deg_free =</span> <span class="dv">20</span>)</span>
<span id="cb1125-18"><a href="model-choice.html#cb1125-18"></a></span>
<span id="cb1125-19"><a href="model-choice.html#cb1125-19"></a>lm_model &lt;-<span class="st"> </span><span class="kw">linear_reg</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">set_engine</span>(<span class="st">"lm"</span>)</span>
<span id="cb1125-20"><a href="model-choice.html#cb1125-20"></a></span>
<span id="cb1125-21"><a href="model-choice.html#cb1125-21"></a>lm_wflow &lt;-<span class="st"> </span></span>
<span id="cb1125-22"><a href="model-choice.html#cb1125-22"></a><span class="st">  </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1125-23"><a href="model-choice.html#cb1125-23"></a><span class="st">  </span><span class="kw">add_model</span>(lm_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1125-24"><a href="model-choice.html#cb1125-24"></a><span class="st">  </span><span class="kw">add_recipe</span>(ames_rec)</span>
<span id="cb1125-25"><a href="model-choice.html#cb1125-25"></a></span>
<span id="cb1125-26"><a href="model-choice.html#cb1125-26"></a>lm_fit &lt;-<span class="st"> </span><span class="kw">fit</span>(lm_wflow, ames_train)</span>
<span id="cb1125-27"><a href="model-choice.html#cb1125-27"></a></span>
<span id="cb1125-28"><a href="model-choice.html#cb1125-28"></a>rf_model &lt;-<span class="st"> </span></span>
<span id="cb1125-29"><a href="model-choice.html#cb1125-29"></a><span class="st">  </span><span class="kw">rand_forest</span>(<span class="dt">trees =</span> <span class="dv">1000</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1125-30"><a href="model-choice.html#cb1125-30"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">"ranger"</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1125-31"><a href="model-choice.html#cb1125-31"></a><span class="st">  </span><span class="kw">set_mode</span>(<span class="st">"regression"</span>)</span>
<span id="cb1125-32"><a href="model-choice.html#cb1125-32"></a></span>
<span id="cb1125-33"><a href="model-choice.html#cb1125-33"></a>rf_wflow &lt;-<span class="st"> </span></span>
<span id="cb1125-34"><a href="model-choice.html#cb1125-34"></a><span class="st">  </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1125-35"><a href="model-choice.html#cb1125-35"></a><span class="st">  </span><span class="kw">add_formula</span>(</span>
<span id="cb1125-36"><a href="model-choice.html#cb1125-36"></a>    Sale_Price <span class="op">~</span><span class="st"> </span>Neighborhood <span class="op">+</span><span class="st"> </span>Gr_Liv_Area <span class="op">+</span><span class="st"> </span>Year_Built <span class="op">+</span><span class="st"> </span>Bldg_Type <span class="op">+</span><span class="st"> </span></span>
<span id="cb1125-37"><a href="model-choice.html#cb1125-37"></a><span class="st">      </span>Latitude <span class="op">+</span><span class="st"> </span>Longitude) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1125-38"><a href="model-choice.html#cb1125-38"></a><span class="st">  </span><span class="kw">add_model</span>(rf_model) </span>
<span id="cb1125-39"><a href="model-choice.html#cb1125-39"></a></span>
<span id="cb1125-40"><a href="model-choice.html#cb1125-40"></a><span class="kw">set.seed</span>(<span class="dv">55</span>)</span>
<span id="cb1125-41"><a href="model-choice.html#cb1125-41"></a>ames_folds &lt;-<span class="st"> </span><span class="kw">vfold_cv</span>(ames_train, <span class="dt">v =</span> <span class="dv">10</span>)</span>
<span id="cb1125-42"><a href="model-choice.html#cb1125-42"></a></span>
<span id="cb1125-43"><a href="model-choice.html#cb1125-43"></a>keep_pred &lt;-<span class="st"> </span><span class="kw">control_resamples</span>(<span class="dt">save_pred =</span> <span class="ot">TRUE</span>)</span>
<span id="cb1125-44"><a href="model-choice.html#cb1125-44"></a></span>
<span id="cb1125-45"><a href="model-choice.html#cb1125-45"></a><span class="kw">set.seed</span>(<span class="dv">130</span>)</span>
<span id="cb1125-46"><a href="model-choice.html#cb1125-46"></a>rf_res &lt;-<span class="st"> </span>rf_wflow <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit_resamples</span>(<span class="dt">resamples =</span> ames_folds, <span class="dt">control =</span> keep_pred)</span></code></pre></div>
</div>
</div>
<div id="compare" class="section level2">
<h2>
<span class="header-section-number">10.7</span> Comparing models with resampling</h2>
<p>Once we create two or more models, the next step is to compare them. In some cases, comparisons might be <em>within-model</em>, where the same model might be evaluated with different features or preprocessing methods. Alternatively, <em>between-model</em> comparisons, such as when we compared linear regression and random forest models in Section <a href="model-choice.html#resampling">10.6.3</a>, are the more common scenario.</p>
<p>In either case, the result is a collection of resampled summary statistics (e.g. RMSE, accuracy, etc.) for each model. The first section of this chapter discusses important aspects of these statistics. Two additional sections follow describing how to formally compare models.</p>
<div id="resampled-stats" class="section level3">
<h3>
<span class="header-section-number">10.7.1</span> Resampled performance statistics</h3>
<p>In Chapter <a href="model-choice.html#resampling">10.6.3</a>, a random forest model for the Ames data was resampled with 10-fold cross-validation. These results were saved in the object <code>rf_res</code>.</p>
<p>We’ll estimate two additional models with the same resamples. First, the previous linear regression model, using the preprocessing defined in the <code>ames_rec</code> recipe, is resampled:</p>
<div class="sourceCode" id="cb1126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1126-1"><a href="model-choice.html#cb1126-1"></a>lm_with_splines_res &lt;-</span>
<span id="cb1126-2"><a href="model-choice.html#cb1126-2"></a><span class="st">  </span>lm_wflow <span class="op">%&gt;%</span></span>
<span id="cb1126-3"><a href="model-choice.html#cb1126-3"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">resamples =</span> ames_folds, </span>
<span id="cb1126-4"><a href="model-choice.html#cb1126-4"></a>                <span class="dt">control =</span> <span class="kw">control_resamples</span>(<span class="dt">save_pred =</span> <span class="ot">TRUE</span>))</span></code></pre></div>
<p>Secondly, a less complex recipe without the interaction terms is evaluated to determine if the extra complexity is worth it. Once added to a workflow, it is resampled:</p>
<div class="sourceCode" id="cb1127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1127-1"><a href="model-choice.html#cb1127-1"></a>lm_with_inter_res &lt;-</span>
<span id="cb1127-2"><a href="model-choice.html#cb1127-2"></a><span class="st">  </span>lm_wflow <span class="op">%&gt;%</span></span>
<span id="cb1127-3"><a href="model-choice.html#cb1127-3"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">resamples =</span> ames_folds, </span>
<span id="cb1127-4"><a href="model-choice.html#cb1127-4"></a>                <span class="dt">control =</span> <span class="kw">control_resamples</span>(<span class="dt">save_pred =</span> <span class="ot">TRUE</span>))</span>
<span id="cb1127-5"><a href="model-choice.html#cb1127-5"></a></span>
<span id="cb1127-6"><a href="model-choice.html#cb1127-6"></a>no_inter_rec &lt;-<span class="st"> </span></span>
<span id="cb1127-7"><a href="model-choice.html#cb1127-7"></a><span class="st">  </span><span class="kw">recipe</span>(Sale_Price <span class="op">~</span><span class="st"> </span>Neighborhood <span class="op">+</span><span class="st"> </span>Gr_Liv_Area <span class="op">+</span><span class="st"> </span>Year_Built <span class="op">+</span><span class="st"> </span>Bldg_Type <span class="op">+</span><span class="st"> </span></span>
<span id="cb1127-8"><a href="model-choice.html#cb1127-8"></a><span class="st">           </span>Latitude <span class="op">+</span><span class="st"> </span>Longitude, <span class="dt">data =</span> ames_train) <span class="op">%&gt;%</span></span>
<span id="cb1127-9"><a href="model-choice.html#cb1127-9"></a><span class="st">  </span><span class="kw">step_log</span>(Gr_Liv_Area, <span class="dt">base =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1127-10"><a href="model-choice.html#cb1127-10"></a><span class="st">  </span><span class="kw">step_other</span>(Neighborhood, <span class="dt">threshold =</span> <span class="fl">0.01</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1127-11"><a href="model-choice.html#cb1127-11"></a><span class="st">  </span><span class="kw">step_dummy</span>(<span class="kw">all_nominal</span>()) </span>
<span id="cb1127-12"><a href="model-choice.html#cb1127-12"></a></span>
<span id="cb1127-13"><a href="model-choice.html#cb1127-13"></a>lm_no_inter_res &lt;-<span class="st"> </span></span>
<span id="cb1127-14"><a href="model-choice.html#cb1127-14"></a><span class="st">  </span>lm_wflow <span class="op">%&gt;%</span></span>
<span id="cb1127-15"><a href="model-choice.html#cb1127-15"></a><span class="st">  </span><span class="kw">remove_recipe</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1127-16"><a href="model-choice.html#cb1127-16"></a><span class="st">  </span><span class="kw">add_recipe</span>(no_inter_rec) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1127-17"><a href="model-choice.html#cb1127-17"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">resamples =</span> ames_folds, </span>
<span id="cb1127-18"><a href="model-choice.html#cb1127-18"></a>                <span class="dt">control =</span> <span class="kw">control_resamples</span>(<span class="dt">save_pred =</span> <span class="ot">TRUE</span>))</span></code></pre></div>
<p>The difference in performance appears relatively minor:</p>
<div class="sourceCode" id="cb1128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1128-1"><a href="model-choice.html#cb1128-1"></a><span class="kw">collect_metrics</span>(lm_no_inter_res)</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric .estimator   mean     n std_err
##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 rmse    standard   0.0788    10 0.00187
## 2 rsq     standard   0.799     10 0.0113</code></pre>
<div class="sourceCode" id="cb1130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1130-1"><a href="model-choice.html#cb1130-1"></a><span class="kw">collect_metrics</span>(lm_with_inter_res)</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric .estimator   mean     n std_err
##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 rmse    standard   0.0782    10 0.00192
## 2 rsq     standard   0.802     10 0.0113</code></pre>
<p>Considering these results, it appears that the additional terms do not profoundly improve the <em>mean</em> RMSE or R<sup>2</sup> statistics. The difference is small, but it might be larger than the experimental noise in the system.</p>
<p>Before making model comparisons or looking at the resampling results, it can be helpful to define a relevant <em>practical effect size</em>. Since these analyses focus on the R<sup>2</sup> statistics, the practical effect size is the change in R<sup>2</sup> that we would consider to be a realistic difference that matters. For example, we might think that two models are not practically different if their R<sup>2</sup> values are within <span class="math inline">\(\pm 2\)</span>%. If this were the case, differences smaller than 2% are not deemed important even if they are statistically significant.</p>
<p>Practical significance is subjective; two people can have very different ideas on the threshold for importance. However, this consideration can be very helpful when deciding between models.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>
<span class="header-section-number">10.8</span> Conclusion</h2>
<p>The <strong>tidymodels</strong> package provides the most popular and sophisticated tools for making models in R. This chapter has provided a whirlwind tour. Don’t worry if you did not understand every detail! We will spend the last two chapters of the book practicing.</p>

</div>
</div></body></html>

<p style="text-align: center;">
<a href="n-parameters.html"><button class="btn btn-default">Previous</button></a>
<a href="continuous-response.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-10-30
</p>
</div>
</div>



</body>
</html>
