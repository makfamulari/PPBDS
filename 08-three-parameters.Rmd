---
output_yaml: output.yml
---

<!-- BG: test commit take 2 -->

# Three Parameters {#three-parameters}

*This chapter is still very much a draft.* Come back in a few weeks for a better version.







<!--   2.1) Wisdom -->

<!--   This section can be rather short. There are two aspects to be discussed: Whether our data is appropriate for answering our research question and whether there are ethical concerns. Stick to the points outlined by Gelman. Begin by discussing possible problems with the data set on hand, in particular the 'age' and 'party' variables. Also consider how other variables might impact the usefulness of these two. Make sure to address both content-related (does this variables tell us what we're looking for?) and methodological problems (is the assignment mechanism problematic, can the scale lead to problems?). This was already discussed on C3, so have a look at this before doing all this from scratch. -->


<!--   2.2) Justice -->

<!--   This section consists of two parts. First, we need to become aware of the purpose of our our model - in this section, it is about making a prediction. Explain why that is, and how this shows in the Preceptor able. Start with and infinite one, then drop any irrelevant rows and columns to arrive at the ideal Preceptor table. Replace unknown rows with question marks to get the actual table. Again, have a look at C3 before doing this. -->
<!--   Second, show the mathematical model. Start by repeating the generic regression equation shown in the math section of C3. Then, write the actual regression equation we will use to create our model in this chapter. Instead of just stopping here like in previous chapters, we will now explain what it actually means. Again, no need to talk about the Bayesian-specific stuff like priors. Start with a scatterplot, then add a regression line. Explain how the stuff in the plot corresponds to the equation. Give some examples. -->


<!--   2.3) Courage -->

<!--   This section consists of two subparts. First, we need to explain the fitted model. Estimate the model shown before using stan_glm().  Then, create a scatterplot with the estimated regression line. Explain the output by referring to the plot.  -->
<!--   Second, explain the unmodeled variation. It is, on average, zero - without this assumption, our model would be useless. Explain why this does not hold for (almost) any individual observation, i.e. what residuals are. Give some examples. Finally, create a histogram each for the distribution of y hat, the two regression coefficients, and the residuals. Arrange them in this very order, and possibly put the regression equation below to show how this relates to the model. -->


<!--   2.4) Temperance -->

<!--   We have three sections here. First, make some predictions using predict(), posterior_predict(), and posterior linpred. Explain what the output means.  -->
<!--   Third, explain why null tests are stupid (why are they?). -->







Models have parameters. In Chapter \@ref(one-parameter) we created models with a single parameter $p$, the proportion of red beads in an urn. In Chapter \@ref(two-parameters) , we used models with two parameters: $\mu$ (the average height in the population, generically known as a model "intercept") and $\sigma$ (the variation in height in the population). Here --- can you guess where this is going? --- we will build models with three parameters: $\sigma$ (which serves the same role throughout the book) and two intercepts: $\beta_1$ and $\beta_2$. All this notation is confusing, not least because different academic fields use inconsistent schemes. The key is to just follow the cardinal virtues and tackle your problem step by step.




## EDA for `trains`

<!-- DK: Still need to standardize the method we use for providing references. -->

<!-- DK: Fix Enos reference. -->

As always, it makes sense to start with some exploratory data analysis (EDA). To demonstrate modeling with three parameters, we will use the `trains` data set from the **PPBDS.data** package. Recall the discussion from Chapter \@ref(rubin-causal-model). Enos (2014) randomly placed Spanish-speaking confederates on nine train platforms around Boston, Massachusetts. Exposure to Spanish-speakers -- the `treatment` -- influenced attitudes toward immigration. These reactions were measured through changes in answers to three survey questions. Let's load the libraries we will need in this chapter, all of which we have used before, and look at the data.

```{r, message=FALSE}
library(PPBDS.data)
library(rstanarm)
library(broom.mixed)
library(skimr)
library(tidyverse)
```

```{r}
glimpse(trains)
```

Here, we can see variables that indicate each respondent's gender, political affiliations, age, and income. Additionally, we have variables that indicate whether a subject was in the control or treatment group, and their attitudes toward immigration both before (`att_start`) and after (`att_end`) the experiment. You can type `?trains` to read the help page for more information about each variable. Let's restrict attention to a subset of the variables.

```{r}
ch8 <- trains %>% 
  select(age, att_end, party, treatment)
```

It is always smart to look at a some random samples of the data:

```{r}
ch8 %>% 
  sample_n(5)
```

`att_end` is a measure of person's attitude toward immigration, a higher number means more conservative, i.e., a more exclusionary stance on immigration into the United States. Running `glimpse()` is another way of exploring a data set.

```{r}
ch8 %>% 
  glimpse()
```

Pay attention to the variable types. Do they make sense? Perhaps. But there are certainly grounds for suspicion. Why are `age` and `att_end` doubles rather than integers? All the values in the data appear to be integers, so there is no benefit is having these variables be doubles. Why is `party` a character variable and `treatment` a factor variable? It could be that these are intentional choices made by the creator of the tibble, i.e., us. These could be mistakes. Or, most likely, these choices are a mixture of sensible and arbitrary. Regardless, it is your responsibility to notice them. You can't make a good model without looking closely at the data which you are using.

`skim` from the **skimr** package is the best way to get an overview of a tibble.

```{r}
ch8 %>% 
  skim()
```

`skim()` shows us what the different values of `treatment` are because it is a factor. Unfortunately, it does not do the same for character variables like `party`. The ranges for `age` and `att_end` seem reasonable. Recall that participants were asked three questions about immigration issues, each of which allowed for an answer indicated strength of agreement on a scale form 1 to 5, with higher values indicating more agreement with conservative viewpoints. `att_end` is the sum of the responses to the three questions, so the most liberal possible value is 3 and the most conservative is 15.

Always plot your data.

```{r}
ch8 %>%
  ggplot(aes(x = party, y = age)) + 
  geom_jitter(width = 0.1, height = 0) + 
  labs(title = "Age by Party Affiliation in Trains Dataset",
       subtitle = "Where are the old Republicans?",
       x = "Party",
       y = "Age")
```

From this plot, we can gather that there are many more Democrats in this dataset than Republicans. Democrats also span a wider range of ages than Republicans. The mode age for Democrats appears to be 50.

```{r}
ch8 %>%
  ggplot(aes(x = treatment, y = att_end)) + 
  geom_boxplot() + 
  labs(title = "Attitude End by Treatment in Trains Dataset",
       subtitle = "Did the treatment make people more conservative?",
       x = "Treatment",
       y = "Attitude After Experiment")
```

On a boxplot, the top and bottom borders of the box denotes the 75th and 25th percentiles, respectively. The line inside the box denotes the mean of the data. Treated individuals have a higher mean `att_end` than the control group, and a higher distribution in general, with its 25th percentile lining up with the mean of the control group. The control group has one outlier, while the treatment group has none. 



## `age` as a function of `party`


We want to build a model and then use that model to draw conclusions about the world. What is the probability that, if a Democrat shows up at the train station, he will be over 50 years old? In a group of three Democrats and three Republicans, what age difference should we expect betweeb the oldest Democrat and the youngest Republican? We can answer these and similar questions by creating a model that uses party affiliation to predict age

### Wisdom

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Wisdom"}
knitr::include_graphics("other/images/Wisdom.jpg")
```

In data science, there are two main aspects to Wisdom: representativeness and ethics. The data that we have are very limited. There are only 115 observations, all from 2012 and involving train commuters to Boston. How useful will this data be today, or for other populations around Boston, or for other cities in the US? Only your judgment, along with advice from your colleagues, can guide you.

The ethical issues are, fortunately, not fraught. Estimating someone's age is (universally?) viewed as OK. Estimating someone's health or income or criminal record are far dicier. 


### Justice

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Justice"}
knitr::include_graphics("other/images/Justice.jpg")
```

Justice, in data science, consists of three topics: predictive versus causal modeling, the Preceptor Table and a mathematical formulation of the model. 


Any model with age as its dependent variable will be predictive, not causal, for the simple reason that nothing, other than time, can change our age. You are X years old. It would not matter if you changed your party registration from Democrat to Republican or vice versa. Your age is your age.

In terms of the Preceptor Table, this fact means that there is only one potential outcome, i.e., one outcome. There is not a potential outcome if you are Democrat and a different potential outcome if you are a Republican.

When dealing with a non-causal model, the focus is on predicting things. The underlying mechanism which connects age with party is less important than the brute statistical fact that there is a connection.  *Predictive models care little about causality.*

<!-- DK: Really need a Preceptor Table! -->

#### The mathematical model

We now know that we are working with a predictive model. Recall:

$$outcome = model + not\ in\ the\ model$$

In words, any event depends on our explicitly described model as well as on influences unknown to us. Everything that happens in the world is the result of various factors, and we can only ever consider a part of them in our model (because we do not know about some influences, or because we have no data about them).  

So far we have only treated the equation above conceptually, but in fact it works just like any other equation. Let's be a bit more concrete. Our model would formally look like this:

<!-- DK: Should standardize the notation across the book. -->

$$ \underbrace{y_i}_{outcome} = \underbrace{\beta_1 x_{r,i} + \beta_2 x_{d,i}}_{model} + \underbrace{\epsilon_i}_{not\ in\ the\ model}$$

where \n
$$x_{r,i}, x_{d,i} \in \{0,1\}$$ \n
$$x_{r,i} +  x_{d,i} = 1$$ \n
$$\epsilon_i \sim N(0, \sigma^2)$$   

Don't panic dear poets and philosophers, the whole thing is easier than it looks at first sight. 

* On the left-hand side we have the outcome, $y_i$, which is the variable to be explained. In our case, this is the age in years of a person. 

* On the right-hand side we first have the part contained in the model, consisting of two similar looking terms. The two terms stand for Republicans and Democrats and work as follows. Each term consists of a parameter and a data point. The betas are our two parameters; $\beta_1$ is the average age of Republicans in the population and $\beta_2$ is the average age of  Democrats in the population. The x's are our explanatory variables and take the values 1 or 0. If someone is a Republican we have $x_{r,i} = 1$ and $x_{d,i} = 0$, if someone is a Democrat we have $x_{r,i} = 0$ and $x_{d,i} = 1$. In other words, the x's are binary variables and are mutually exclusive (if you are a Democrat, you cannot also be a Republican).

* The last part, $\epsilon_i$ (“epsilon”), represents the unexplained part and is called the error term. It is simply the difference between the outcome and our model predictions. In our particular case, this includes all factors that have an influence on someone's age but are not explained by party affiliation. We assume that this error follows a normal distribution with an expected value of 0, i.e., it is 0 on average.

* The small i's are an index to number observations in our data set. It is equivalent to the "ID" column in our Preceptor Table and simply states that the outcome for person i is explained by the modeled and non-modelled factors for person i. The betas are numbered with 1, 2, 3, etc. The corresponding x's have an "r" or "d" subscript instead, for Republicans or Democrats.



### Courage

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Courage"}
knitr::include_graphics("other/images/Courage.jpg")
```

Courage allows us to translate model to code.



#### The fitted model

To get posterior distributions for our three parameters, we will again use the `stan_glm()` function from Chapter \@ref(two-parameters). If we take a look at the formula, we can see that it is similar to the equation from before. 

* The variable before the tilde, `age`, is our outcome. 

* The only explanatory variable is `party`. This variable has only two values, 'Democrat' and 'Republican'. 

* We have also added `- 1` at the end of the equation, indicating that we do not want an intercept, which is otherwise added by default. Running this code in R, we get the following output:

```{r}
fit_obj <- stan_glm(age ~ party - 1, data = trains, refresh = 0)

fit_obj
```


"partyDemocrat" corresponds to $\beta_1$, the average age of Democrats in the population. "partyRepublican" corresponds to $\beta_2$, the average age of Republicans in the population. Since we don't really care about the posterior distribution for $\sigma$, we won't discuss it here. Graphically,

```{r}

```




To continue our discussion of these values, we'll bring back our Preceptor table (the actual one). For your reference, it looked like this:



Recall that when creating a model, our goal is to replace the question marks in that table. To keep it simple, the table will use now displays only a sample of eight people from the `trains` data set instead of all N commuters. We have also added two more columns to show the prediction and the error term for each person. The first one in the table, Age, is the *actual* outcome variable ($y_i$). The second one, called "fitted value", is our *prediction* of the outcome variable ($\hat{y_i}$). The third one, called "residual", is our error term ($\epsilon_i,\ also\ r_i$). The actual age of each person is the addition of the model prediction and the unmodeled variation ($y_i = \hat{y_i} + r_i$).

```{r, include = FALSE}
margin_note("There are two notations for the error term, one with epsilon and one with an r. Strictly speaking, the epsilon is only used in connection with the real model. This corresponds to the equation where the betas are based on the (unknown) population values. The r is used in connection with the estimated model, like right now. This can be useful to know, but we will not ask you about it in the exam.")

margin_note("To be precise, this is no longer considered a Preceptor Table by our terminology. A Preceptor Table must account for all of the observations in the real-world population that the model is.")
```

```{r, echo = FALSE}

tibble(subject = c("4", "9", "32", "41", "74", "90", "100", "106"),
       age = c(45, 54, 37, 45, 42, 22, 60, 52),
       fitted = c(41.1, 41.1, 42.6, 41.1, 42.6, 41.1, 41.1, 42.6),
       party = c("Democrat", "Democrat", "Republican", "Democrat", "Republican", "Democrat", "Democrat", "Republican")) %>%
  mutate(noise = round(age - fitted, 1)) %>%
  select(subject, age, fitted, noise, party) %>% 
  gt() %>%
  cols_label(subject = md("**ID**"),
             age = md("**Age**"),
             fitted = md("**Fitted**"),
             noise = md("**Residual**"),
             party = md("**Party**")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_header("8 Observations from Trains Dataset") %>%
  cols_align(align = "center", columns = TRUE) %>%
  fmt_markdown(columns = TRUE)

```

The fitted values are the same for all Republicans and for all Democrats, as the fitted value is one of the two estimates output by our model. However, almost all of the residuals are different due to each observation's own variation from the fitted value estimate. The only observations with the same residual are 4 and 41. This is because they are both Democrats that are the same age: with the same $y_i$ and $\hat{y_i}$, the calculation of their residuals is identical. This table shows how just a sample of 8 individuals captures a wide range of residuals, making it difficult to predict the age of a new individual who walks in the room even using our model. 

We can get a better picture of the unmodeled variation in our sample if we plot these three values for all individuals in our data set. The following three histograms show the actual outcomes, fitted values and residuals of all people in `trains`:

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}

trains_both <- trains %>% 
              select(party, age) %>% 
              mutate("fitted" = ifelse(party == "Democrat", 41.1, 42.6),
                     "residual" = age - fitted)



# Actual outcomes (both)

act_both <- ggplot(trains_both, aes(x = age)) +
  geom_histogram(color = "white", fill = "blueviolet", binwidth = 2, boundary = 20) +
  scale_x_continuous(expand = c(0, 0), limits = c(18, 70)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 10), breaks = c(2, 4, 6, 8, 10)) +
  labs(
    x = "Age",
    y = "Count") +
  theme_linedraw()


# Predicted outcomes (both)

pred_both <- ggplot(trains_both, aes(x = fitted)) +
  geom_histogram(color = "white", fill = "blueviolet", binwidth = 1.5) +
  scale_x_continuous(expand = c(0, 0), limits = c(18, 70)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 100)) +
  labs(
    x = "Fitted Values",
    y = "") +
  theme_linedraw()


# Residuals (both)

res_both <- ggplot(trains_both, aes(x = residual)) +
  geom_histogram(color = "white", fill = "blueviolet", binwidth = 2) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 10), breaks = c(2, 4, 6, 8, 10)) +
  labs(
    x = "Residuals",
    y = "") +
  theme_linedraw()

act_both + pred_both + res_both + 
  plot_annotation(title = "Results for Democrats and Republicans")

```

The three plots are structured like our equation and table above, i.e. a value in the left plot is the addition of one value in the middle and one in the right plot. The actual age distribution looks like a normal distribution, centered around 43 and with a standard deviation of about 12 years (the distribution could be random, but the exact reasons are not important here). The plot for the fitted values shows only two adjacent spikes, which represent the estimates for Democrats and Republicans. Since the residuals represent the difference between the two plots, their distribution looks like a mirror image of the first plot around the value 43. Indeed, our model seems to be quite inaccurate.

We already learned about another method to reveal the distribution of the residuals. When we estimate a model with stan_glm() we automatically get the standard deviation of the residuals, or *sigma*. Recall that sigma is the third parameter of our model, but we have ignored it so far since it was not useful for any of the analysis above. A look at the output from before shows a sigma of 12.3, indicating that the actual age within $\pm$ is 12.3 years around our predicted value for about 68% of the people in `trains`, and $\pm$ 2*12.3 = 24.6 years around our prediction for about 95% of the people.

The main reason and the solution for this strong deviation can be seen by looking at the histograms again. A bar on the left is always the result of the bars in the middle and right. The better our prediction is, i.e. the more the fitted values match the real ones, the more the middle histogram looks like the one on the left. Only the part that does not coincide is "moved" to the right plot. Apparently we need more bars in the middle, that is, we need a model that can predict more than just two values. We can achieve this by adding more (meaningful) variables to our model. An example would be a variable that shows whether the person has ever used Windows 95; if so, we would probably add a few more years to our estimate. But don't worry about that now, we will learn in the next chapter how to create models with more variables.

<!-- Are separate plots for Democrats and Republicans useful?

```{r, echo=FALSE, warning=FALSE, message=FALSE}

trains_dems <- trains %>% 
                select(party, age) %>% 
                filter(party == "Democrat") %>% 
                mutate("fitted" = 41.1,
                     "residual" = age - fitted)



# Actual outcomes (Dems)

act_dems <- ggplot(trains_dems, aes(x = age)) +
  geom_histogram(color = "white", fill = "dodgerblue2", binwidth = 2, boundary = 20) +
  scale_x_continuous(expand = c(0, 0), limits = c(18, 70)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 10), breaks = c(2, 4, 6, 8, 10)) +
  labs(
    x = "Age",
    y = "Count") +
  theme_linedraw()


# Predicted outcomes (Dems)

pred_dems <- ggplot(trains_dems, aes(x = fitted)) +
  geom_histogram(color = "white", fill = "dodgerblue2", binwidth = 1.5) +
  scale_x_continuous(expand = c(0, 0), limits = c(18, 70)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 100)) +
  labs(
    x = "Predicted Age",
    y = "") +
  theme_linedraw()


# Residuals (Dems)

res_dems <- ggplot(trains_dems, aes(x = residual)) +
  geom_histogram(color = "white", fill = "dodgerblue2", binwidth = 2) +
  xlim(-30, 30) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 10), breaks = c(2, 4, 6, 8, 10)) +
  labs(
    x = "Residuals",
    y = "") +
  theme_linedraw()

act_dems + pred_dems + res_dems + plot_annotation("Results for Democrats")

```

```{r, echo=FALSE, warning=FALSE, message=FALSE,}

trains_reps <- trains %>% 
                select(party, age) %>% 
                filter(party == "Republican") %>% 
                mutate("fitted" = 42.6,
                     "residual" = age - fitted)



# Actual outcomes (Reps)

act_reps <- ggplot(trains_reps, aes(x = age)) +
  geom_histogram(color = "white", fill = "red2", binwidth = 2, boundary = 20) +
  scale_x_continuous(expand = c(0, 0), limits = c(18, 70)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 10), breaks = c(2, 4, 6, 8, 10)) +
  labs(
    x = "Age",
    y = "Count") +
  theme_linedraw()


# Predicted outcomes (Reps)

pred_reps <- ggplot(trains_reps, aes(x = fitted)) +
  geom_histogram(color = "white", fill = "red2", binwidth = 1.5) +
  scale_x_continuous(expand = c(0, 0), limits = c(18, 70)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 100)) +
  labs(
    x = "Predicted Age",
    y = "") +
  theme_linedraw()


# Residuals (Reps)

res_reps <- ggplot(trains_reps, aes(x = residual)) +
  geom_histogram(color = "white", fill = "red2", binwidth = 2) +
  xlim(-30, 30) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 10), breaks = c(2, 4, 6, 8, 10)) +
  labs(
    x = "Residuals",
    y = "") +
  theme_linedraw()

act_reps + pred_reps + res_reps + plot_annotation("Results for Republicans")

```

-->


<br/><br/>

### Temperance

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Temperance"}
knitr::include_graphics("other/images/Temperance.jpg")
```

Recall the two questions with which we began this section:

* What is the probability that, if a Democrat shows up at the train station, he will be over 50 years old? 

* In a group of three Democrats and three Republicans, what age difference should we expect betweeb the oldest Democrat and the youngest Republican?

So far we have only tried our model on people from our data set whose real age we already knew. This is helpful to understand the model, but our ultimate goal is to understand more about the real world, about people we don't yet know much about. Temperance guides us to make meaningful predictions and to become aware of their (unknown) limitations.


#### Making Predictions

Some questions can be answered with the simple equation we have learned before However, instead of using the two pre-calculated values for Democrats and Republicans returned by `stan_glm`, it can be helpful to directly use simulation results. Recall from chapter 7 that when running `stan_glm`, an algorithm creates thousands of simulations, each of which contains an estimate. The median of these thousands of parameters estimates is then returned as single estimate. This means that behind each of our two estimates for Democrats' and Republicans' age, there are actually thousand of values. The long series of simulation results are especially helpful when we want to make predictions involving probabilities. 

What sounds complicated is actually just a matter of a few extra lines of code. To start with a simple question, what are the chances that a random Democrat is over 50 years old? First, we create a tibble with the desired input for our model. In our case the tibble has a column named "party" which contains a single observation named "Democrat". This is a bit different than before, when we only specified the input to be 0 or 1. Next, we'll use the `posterior_predict` function from chapter 7 to get some simulation results. `posterior_predict` takes two arguments: the model for which the simulations should be run, and a tibble indicating for which and how many parameters we want to run these simulations. In this case, the model is the one from 'Courage' and the tibble is the one we just created in the previous step.

```{r}

# Creating a tibble with a column named "party" and
# a single observation, "Democrat". We will use this to 
# tell R that we want simulation results for a single 
# Democrat.

new <- tibble(party = "Democrat")

# Generating simulation results. The first argument 
# specifies the fitted model to be used, which in our
# case is the one we generated in "Courage". The second
# argument specifies the input, and takes tibbles only.

pp <- posterior_predict(fit_obj, newdata = new)

head(pp, 10)

```

A look at the first few observations shows that we simply get ten different estimates for the age of a person. Since we have defined in our tibble that the value of "party" for this person is "Democrat", these are estimates for a random Democrat. It is important to understand that this is not a concrete person from the `trains` dataset - the algorithm in `posterior_predict` simply uses the existing data of all democrats in `trains` to predict how old a random additional democrat could be in 4000 different scenarios (that's how many rows the tibble has). When the values from all 4000 scenarios are plotted in a histogram, you can see that they are centered around the single estimate of 42.6 that we calculated in "Courage." So, instead of this single like before estimate, we now use the thousands of simulations that lie behind it.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}

as_tibble(pp) %>% 
  rename(age = "1") %>% 
  ggplot(., aes(x = age)) +
    geom_histogram(color = "white", fill = "blueviolet", binwidth = 2) +
   scale_x_continuous(expand = c(0, 0), limits = c(0, 85)) + 
    scale_y_continuous(expand = c(0, 0), limits = c(0, 280)) +
    labs(
      title = "Age of a Random Democrat (4000 Simulations)",
      x = "Age",
      y = "Count") +
    theme_linedraw()


```

Once we have the simulation results for our random democrat, we just have to use our wrangling skills. For this, we proceed in a similar way as in chapter 7:

```{r}

as_tibble(pp) %>% 
  
  # Renaming the column containing
  # the simulation results.
  
  rename(age = "1") %>% 
  
  # Creating a new column that is TRUE if someone 
  # is over 50 years old, and FALSE otherwise.
  
  mutate(ot_50 = ifelse(age > 50, TRUE, FALSE)) %>% 
  
  # Calculating the percentage of TRUEs.
  
  summarize(perc = sum(ot_50)/n() * 100)


```

Therefore we can assume with 27.8% probability that a random democrat is over 50 years old. We can also use simulation results to answer questions involving several persons or groups. Suppose three democrats and three republicans show up. What age difference should we expect between the youngest Republican and the oldest Democrat?

As before we start by creating a tibble with the desired input. Note that the name of the column ("party") and the observations ("Democrat", "Republican") must always be named *exactly* as in the data set. This tibble as well as our model can then be used as arguments for `posterior_predict`:

```{r}

new <- tibble(party = c("Democrat", "Democrat", "Democrat", 
                        "Republican", "Republican","Republican"))

pp <- posterior_predict(fit_obj, newdata = new)

head(pp, 10)

```

A look at the output shows that we now have 6 columns: one for each person. R does not name the columns, but they are arranged in the same order in which we specified the persons in the tibble (D, D, R, R, R). Notice that all values in a row belong to the same scenario, i.e. each row represents a scenario where the 6 people meet. 

To determine the expected age difference, we can then proceed as follows:

```{r}

as_tibble(pp) %>% 
  
  # Using more meaningful names.
  
  rename(dem_1 = "1", dem_2 = "2", dem_3 = "3",
         rep_1 = "4", rep_2 = "5", rep_3 = "6") %>% 

  # Grouping the data by rows.
  
  rowwise() %>% 
  
  # Creating three new columns. The first two are the 
  # highest age among Democrats and the lowest age
  # among Republicans, respectively. The third one is
  # the difference between the first two.
  
  mutate(dems_oldest = max(c(dem_1, dem_2, dem_3)),
         reps_youngest = min(c(rep_1, rep_2, rep_3)),
         age_diff = dems_oldest - reps_youngest) %>% 
  
  # Ungroup to tell R not to perform rowwise operations
  # anymore.
  
  ungroup() %>% 
  
  # Calculate the average age difference.
  
  summarize(avg_diff = mean(age_diff))
  
```

In words, we would expect the oldest Democrat to be about 22 years older than the youngest Republican.


#### Unknown unknowns

Yet another area of uncertainty that might not be immediately obvious is the possibility of unknown unknowns — that is, all the things that might change with time. How do we know that our model — which relies heavily on figures like income and party affiliation — will still be able to draw conclusions about America 50 years from now? Mass migrations, political revolutions, and world wars are just a few of the things that could render all the conclusions we draw from the `trains` dataset moot. And, other than expanding our confidence intervals to the point of uselessness, there's no way to account for the unknown unknowns. 

In short, *while we can accept the model as representative of the United States at or around 2012 (the year in which Enos conducted his study), we cannot reasonably extrapolate that to the far future*.

It is always important to keep the problem of validity in the back of our minds as we investigate data sets and how the data within these sets are collected. Given the many factors — those we are aware of and those we are not aware of — that go into any kind of data collection, there is no way to guarantee 100 percent validity.




<br/><br/>
<br/><br/>

## `att_end` as a function of `treatment`

Above, we created a predictive model: with someone's party affiliation, we can make a better guess as to what their age is. There was nothing causal about that model. Changing someone's party registration can not change their age. In this example, we build a causal model. What is the effect of exposing someone on a train platform to Spanish-speakers  on their attitudes toward immigration? 


### Wisdom

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Wisdom"}
knitr::include_graphics("other/images/Wisdom.jpg")
```


### Justice

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Justice"}
knitr::include_graphics("other/images/Justice.jpg")
```


### Courage

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Courage"}
knitr::include_graphics("other/images/Courage.jpg")
```

```{r}
obj <- stan_glm(att_end ~ treatment - 1, data = trains, refresh = 0)

obj
```

Note that for both Democrats and Republicans, the prediction interval is significantly wider than the confidence interval. This discrepancy illustrates the *unmodeled variation* of the dataset: while the mean values, as indicated by the "fit" column, are still the best guess you can make for any given subject, we are less confident in the value range of that individual subject than we are of the value range of the mean. *The unmodeled variation widens the prediction intervals relative to the confidence intervals.*

Recall our discussion regarding the wider confidence intervals for Republican subjects than Democrat subjects. Although the Republican and Democrat average incomes had vastly different confidence intervals — the Republican confidence intervals were much wider — they have very similar prediction intervals. Although the *parameter uncertainty* of these two parameters were very different, the *unmodeled variation* for both parameters was roughly the same. Even though the data points were equally spread out for both Democrat and Republican income, were were able to more confidently pinpoint a true value for democrat income because we had more data to work with.

In many cases, the most important use of a model is for prediction. We use them to make predictions about data which we have not yet seen.



And we can reconstruct the table by filling in the unknowns with the predictions (note that these are unchanged) and their associated levels of uncertainty (which HAVE changed to accommodate for individual variation within the data):

```{r, echo = FALSE}

tibble(subject = c("7", "6,041", "67", "40,080", "758", "32", "8,809"),
       Control = c("8.45 (2.9-14.0)", "8.45 (2.9-14.0)", "8.45 (2.9-14.0)", "8.45 (2.9-14.0)", "5", "8.45 (2.9-14.0)", "13"),
       Treated = c("11", "10", "5", "11", "10 (4.4-15.6)", "13", "10 (4.4-15.6)")) %>% 
  
  gt() %>%
  cols_label(subject = md("**Subject**"),
                Control = md("**$$Y_c(u)$$**"),
                Treated = md("**$$Y_t(u)$$**")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  cols_align(align = "center", columns = TRUE) %>%
  tab_header(title = "Ending Attitudes on Immigration of 7 Random Respondents")
```





### Temperance

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Temperance"}
knitr::include_graphics("other/images/Temperance.jpg")
```

There are various sources of uncertainties and problems that have addressed in previous chapters and again in this chapter. It's important that you always have these in the back of your mind when working with data:

1) *Validity*. At the surface level, parameter names might not invoke too much thought — but once you really sit down and think about the story the numbers are trying to tell, it's not always the case that the numbers deserve to tell that story. In our earlier discussion regarding `income` and `att_end`, the conclusions we draw from these data might be misleading just because the way they were collected or computed inevitably change these conclusions. Even when there's nothing WRONG with a setup, we need to keep in mind that the smallest differences 

2) When we dive deeper into the data, we run into the issue of *parameter uncertainty*, which is the uncertainty associated with our analysis of summary parameters, such as mean, median, and range. In our above analysis with the `trains` dataset, we account for parameter uncertainty using confidence intervals.

3) As great as confidence intervals are, they don't capture the uncertainty of using summary statistics to predict parameter values of random individuals. We are more sure, for example, about the range of possible incomes for the mean income of American democrats than we are about the range of possible incomes for a single American democrat, whose income could stray far from the mean democrat income. This *unmodeled variation*, or variation between data points that is not fully captured by bootstrapping, deserves much consideration when dealing with data.

4) *Unknown unknowns* are an especially unavoidable source of uncertainty. The world is changing, and people 50 years from now might act differently and have vastly different lifestyles and characteristics; in the case of the `trains` dataset, it wouldn't be unreasonable to expect completely different combinations of income and party affiliation. That would render our mean income, median ratio, and prediction intervals for both those measures useless.





The *fitted value* of att_end is 8.45 for the control group and 10 for the treated group. Because a higher att_end means more conservative towards stances on immigration, it seems that treated individuals had more conservative stances on this issue than the control group by the end of the study. 

### Model Structure

Our predictive model helped us make guesses as to what an individual's age is. However, that's just about all we can do with this relationship. Recall that the trains dataset was originally introduced in the context of the Rubin Causal Model back in Chapter \@ref(rubin-causal-model). That model cannot be applied to the relationship between party and income because these values were independently decided outside the experiment — although we were able to identify correlations between those two variables, the combinations of party and income were predetermined and not randomly assigned in a way that allows us to establish a causal relationship. If an individual suddenly decided to change party affiliations, we wouldn't suddenly expect his or her income to spike or plummet. 

In conclusion, while we can explore correlations or trends between party and income, we cannot label these correlations or trends as causal. However, `treatment` was a variable that was randomly assigned to subjects, and we have a variable that was measured after this random assignment, `att_end`. With this setup, we can calculate the Average Treatment Effect, or the average difference in `att_end` for treated subjects and control subjects. 

```{r, warning = FALSE, message = FALSE}

mean_ends <- trains %>% 
  group_by(treatment) %>% 
  summarize(mean_att_end = mean(att_end))

```

```{r}

ATE <- mean_ends$mean_att_end[2] - mean_ends$mean_att_end[1]

```

Now, let's make a Preceptor Table for this data:

```{r, echo = FALSE}

tibble(subject = c("1", "2", "...", "75", "...", "564", "565", "...", "10,627", "...", "N"),
       treatment = c("treated", "?", "...", "control", "...", "control", "?",  "...", "treated", "...", "?"),
       att_end = c("11", "?", "...", "5", "...", "13", "?",  "...", "11", "...", "?")) %>%
  gt() %>%
  cols_label(subject = md("**ID**"),
             att_end = md("**att_end**"),
             treatment = md("**treatment**")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  cols_align(align = "center", columns = TRUE) %>%
  tab_header("Preceptor Table of att_end ~ treatment") %>%
  fmt_markdown(columns = TRUE)

```

Preceptor Tables aim to capture all of the data such that if the question marks were filled out, one wouldn't need to construct a model. The rows in this Preceptor Table represent the N commuters on which our model is extrapolating, with N being the population of commuters using Boston public transportation. However, we're going to need a different kind of table if we want to display the causal effect of being treated -- more with this soon. 

Next, the second part of model structure, the formula. 

$$ y_i = b_1 x_t + b_2 x_c$$
Once again, we start here. We can make our parameters constants, $\hat{b_1}$ and $\hat{b_1}$, by assigning them to the estimates of the model. Plugging in these numbers outputs an estimated att_end for the observation, or our *fitted value*   $\hat{y_i}$. 

$$ \hat{y_i} = \hat{b_1} x_t + \hat{b_2} x_c  $$
$$ \hat{y_i} = 8.45x_t + 10x_c $$
When the individual is treated ($x_t$=1 and $x_c$=0), their fitted value is 8.45. When the individual is a control ($x_t$=0 and $x_c$=1), their fiited value is 42.6. 

Subtract the recorded att_end by fitted value for the epsilon value...
$$\epsilon_i = y_i - \hat{y_i}$$
...And you can now complete your equation.
$$ y_i = \hat{b_1}x_r + \hat{b_2} x_d + \epsilon_i $$
Note that the math for both the predictive model we worked on earlier and this model is *identical*. It is not the calculations for the residual, fitted value, or parameter estimates that differ between these two model types; rather, it is how one uses and interprets these numbers. While predictive models are simply used for predicting another variable's value (in the previous case, age), we can use these numbers to calculate the effect of one variable (treatment) on another (att_end) for each individual observation. 

Now, we'll create a Rubin Causal Model table of possible outcomes, as we did back in ch3. We'll do this using 7 random observations from the trains dataset. 

```{r}

#Adding question marks where necessary

trains_RCM <- trains %>% 
  pivot_wider(names_from = treatment, values_from = att_end) %>%
  slice(1:7) %>%
  mutate(subject = 1:7) %>%
  replace_na(list(Treated = "?", Control = "?")) %>% 
  select(subject, Control, Treated)

trains_RCM$subject <- c("7", "6,041", "67", "40,080", "758", "32", "8,809")

#Mathematical notation

trains_RCM %>%
  gt() %>%
  cols_label(subject = md("**Subject**"),
                Control = md("**$$Y_c(u)$$**"),
                Treated = md("**$$Y_t(u)$$**")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  cols_align(align = "center", columns = TRUE) %>%
  tab_header(title = "Ending Attitudes on Immigration of 7 Random Respondents")


```

Recall that the Rubin Causal Model centers on filling in the unknowns, which are indicated by the "?"s in the table. There's one way we could fill in the missing values — we could use our approximations we generated with our `lm()` model: that is, for each unknown Treated value, we use 10, and for each unknown Control value, we use 8.45. 

```{r, echo = FALSE}

tibble(subject = c("7", "6,041", "67", "40,080", "758", "32", "8,809"),
       Control = c("8.45", "8.45", "8.45", "8.45", "5", "8.45", "13"),
       Treated = c("11", "10", "5", "11", "10", "13", "10")) %>% 
  
  gt() %>%
  cols_label(subject = md("**Subject**"),
                Control = md("**$$Y_c(u)$$**"),
                Treated = md("**$$Y_t(u)$$**")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  cols_align(align = "center", columns = TRUE) %>%
  tab_header(title = "Ending Attitudes on Immigration of 7 Random Respondents")

```

```{r, include = FALSE}

library(tufte)

margin_note("The minimum att_end is 3 and maximum att_end is 15.")

```

This table looks a bit funky: take a look at subject 67. According to this model, subject 67's attitude under control would be HIGHER than his/her attitude under treatment, which corresponds to a negative treatment effect. This would mean that he/she would become more LIBERAL after undergoing treatment, which flies in the face of the ATE we calculated earlier. Another example of this can be seen with subject 32, whose predicted `att_end` under treatment is lower than his/her `att_end` under control (also signifying a shift towards liberal attitudes regarding immigration). 

Clearly, using the same control and treatment values to fill in the unknowns doesn't capture the entire picture as it does with a predictive model. While using `lm()` to fill in the `att_end` unknowns towards the center of the distribution, it makes less sense for values towards the outskirts of the dataset. So, how can we predict outcomes in a way that equally addresses data towards the center of the distribution, and data that isn't?

One such way is by attaching the ATE, which we previously calculated to be `r ATE`, in the appropriate direction to fill in the unknowns:

```{r, echo = FALSE}

#This is necessary so we can see the offset as caused by treatmentTreated, not treatmentControl.

trains$treatment <- fct_relevel(trains$treatment, "Control", "Treated")

lm(data = trains, att_end ~ treatment) %>% 
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)


tibble(subject = c("7", "6,041", "67", "40,080", "758", "32", "8,809"),
       Control = c("9.45", "8.45", "3.45", "9.45", "5", "11.45", "13"),
       Treated = c("11", "10", "5", "11", "6.55", "13", "14.55")) %>%
  gt() %>%
  cols_label(subject = md("**Subject**"),
                Control = md("**$$Y_c(u)$$**"),
                Treated = md("**$$Y_t(u)$$**")) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  cols_align(align = "center", columns = TRUE) %>%
  tab_header(title = "Ending Attitudes on Immigration of 7 Random Respondents")

```

The usage of this table shows how a causal model is used differently than a predictive model. Unlike before, it wasn't enough to fill in our unknowns with the parameter estimates. This is because we aren't merely predicting their att_end, a variable that, unlike age, could be changed by exposure to a treatment variable. By using the ATE, we were able to make a more informed prediction on any individual's att_end had they been placed in the other group. 

### Parameter Uncertainty

Here, we see that the average treatment effect is `r ATE`; this is the average change in att_end caused by treatment (if this statistic looks unfamiliar, refer back to chapter 3 for a refresher on ATE). But how confident are we in this ATE? We can use `lm()` again to generate confidence intervals for our parameters and for this value:

```{r}
#This is necessary so we can see the offset as caused by treatmentTreated, not treatmentControl.

trains$treatment <-fct_relevel(trains$treatment, "Control", "Treated")

lm(data = trains, att_end ~ treatment - 1) %>% 
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)

summary(lm(data = trains, att_end ~ treatment - 1))

```

<!-- MB: Is this true  -->

Our measure of confidence for the ATE is known as the residual standard error, which in this case is 2.78. The residual standard error show our confidence in the average treatment effect of the `trains` dataset being representative of the true average treatment effect on Boston commuters.

### Uncertainties





