---
output_yaml:
  - _output.yml
---

<!-- DK: Sure would be nice to do one multi-category model. Once we get rid of all the logistic regression stuff, we have some extra space/time. -->

<!-- For Becca: -->

<!--   * Note the use of cache=TRUE at the end. -->

<!--   * Edit anything, in any way. You now own the whole chapter, including, for example the descriptions of the model types at the start of each section. Are they bad? Fix them. -->

  <!-- * The binary classification exercise is 80% too long. I just copy/pasted it from the other book. We don't care about all that detail. Just explain accuracy, and roc_accuracy, since they are the two things that appear by default. You can also mention that there are further details to explore. -->

  <!-- *Let's use the random forest model for doing predictions. Note how it has the best accuracy. -->



# Discrete Response {#discrete-response}


Packages:

```{r, message=FALSE}
library(tidyverse)
library(skimr)
library(PPBDS.data)
library(tidymodels)
```


**Binary responses** take on only two values: success ($Y=1$) or failure ($Y=0$), yes ($Y=1$) or no ($Y=0$), et cetera. Binary responses are one of the most common types of data that statisticians encounter.  We are often interested in modeling the probability of success, $p$, based on a set of covariates. As with regression, there are two broad categories of problems: *modeling for prediction* and *modeling for causation*. Although terminology varies across fields, "regression" is generally used for situations in which our *dependent variable* is continuous, as in Chapter \@ref(continuous-response). "Classification" applies to cases in which the dependent variable takes on discrete values, the simplest of which is the binary case.

In this chapter, we will look at three common techniques of **classification** of binary data.We use the **tidymodels** tools for all examples. At the end, we will compare the performances of all three models.

* logistic regression
* classification and regression trees (CART)
* random forests



<!-- DK: Finish with a question. -->

## Binary classification metrics

<!-- DK: Get rid of most of this. -->

Recall the discussion in Chapter \@ref(model-choice) about how to measure model accuracy. Classification models require different measures than those we used in Chapter \@ref(continuous-response) when considering continuous outcome measures.

The **modeldata** package contains example predictions from a test data set with two classes ("Class1" and "Class2"):

```{r performance-two-class-example}
data(two_class_example)
str(two_class_example)
```

The second and third columns are the predicted class probabilities for the test set while `predicted` are the discrete predictions. 

For the hard class predictions, there are a variety of **yardstick** functions that are helpful: 

```{r performance-class-metrics}
# A confusion matrix: 
conf_mat(two_class_example, truth = truth, estimate = predicted)

accuracy(two_class_example, truth = truth, estimate = predicted)

# Matthews correlation coefficient:
mcc(two_class_example, truth, predicted)

# F1 metric:
f_meas(two_class_example, truth, predicted)
```

For binary classification data sets, these functions have a standard argument called `event_level`. The _default_ is that the **first** level of the outcome factor is the event of interest. 

```{block, type = "rmdnote"}
There is some heterogeneity in R functions in this regard; some use the first level and others the second to denote the event of interest. We consider it more intuitive that the first level is the most important. The second level logic is borne of encoding the outcome as 0/1 (in which case the second value is the event) and unfortunately remains in some packages. However, tidymodels (along with many other R packages) _require_ a categorical outcome to be encoded as a factor and, for this reason, the legacy justification for the second level as the event becomes irrelevant.  
```

As an example where the second class is the event: 

```{r}
f_meas(two_class_example, truth, predicted, event_level = "second")
```

In the output above, the `.estimator` value of "binary" indicates that the standard formula for binary classes will be used. 

There are numerous classification metrics that use the predicted probabilities as inputs rather than the hard class predictions. For example, the receiver operating characteristic (ROC) curve computes the sensitivity and specificity over a continuum of different event thresholds. The predicted class column is not used. There are two **yardstick** functions for this method: `roc_curve()` computes the data points that make up the ROC curve and `roc_auc()` computes the area under the curve. 

The interfaces to these types of metric functions use the `...` argument placeholder to pass in the appropriate class probability column. For two-class problems, the probability column for the event of interest is passed into the function:

```{r}
two_class_curve <- roc_curve(two_class_example, truth, Class1)
two_class_curve

roc_auc(two_class_example, truth, Class1)
```

The `two_class_curve` object can be used in a `ggplot` call to visualize the curve. There is an `autoplot()` method that will take care of the details:

```{r performance-2class-roc-curve}
autoplot(two_class_curve)
```

There are a number of other functions that use probability estimates, including `gain_curve()`, `lift_curve()`, and `pr_curve()`. 



## Exploratory Data Analysis (EDA)

We begin with our usual libraries:

```{r, message=FALSE}
library(tidyverse)
library(broom)
library(skimr)
library(PPBDS.data)
library(tidymodels)
```

It's always a good idea to perform some exploratory analysis on the dataset before we start modeling. In this chapter, we'll be working with, cces. cces stands for the Cooperative Congressional Election Study, a study regarding the approval rating of individual voters to their sitting president. Each row captures one voter, some of their demographic information, and how highly they approve (or disapprove) of the president. 

```{r}
glimpse(cces)
```

We will tweak the data by only looking at observations recorded in the year 2018 so that all the responses are about the same president. We'll also select the variables that are currently of interest to us. Finally, because this chapter will be dealing with logistic regressions, we want to convert the numeric `approval` variable into a binary variable. `approval` is a numeric variable from 1-5 with 5 representing the highest approval of the president. In order to do this, we have to turn approval into a binary variable. 1-2 will be coded to 0 to signify disapproval and 3-5 will be coded to 1 for approval. We will also cast `approval` as a factor variable rather than a number, which is useful information for models.

```{r}
ch12 <- cces %>%
  filter(year == 2018) %>%
  select(state, age, gender, race, education, ideology, approval) %>%
  mutate(approval = as.factor(case_when(
    approval == 1 ~ 0,
    approval == 2 ~ 0,
    approval == 3 ~ 1,
    approval == 4 ~ 1,
    approval == 5 ~ 1)))
```

From this, we can gather that there are 16 variables. Notably, there are 60,000 observations even after filtering only for the year 2018. 

Let's also display a random sample of 5 rows of the 60,000 rows. 

```{r}
ch12 %>%
  sample_n(5)
```

Now, let’s compute summary statistics. Let’s use the `skim()` function from the `skimr` package.

```{r}
ch12 %>% 
  skim()
```

You'll notice that we are missing data for our ideology and approval variables. The `complete_rate` column tells us that approval has 3% missing observations and ideology has 0.7% missing observations. Let's use the function `drop_na()` to get rid of these missing observations so they don't interfere with our models later in the chapter:

```{r}
ch12 <- cces %>%
  filter(year == 2018) %>%
  select(state, age, gender, race, education, ideology, approval) %>%
  mutate(approval = as.factor(case_when(
    approval == 1 ~ 0,
    approval == 2 ~ 0,
    approval == 3 ~ 1,
    approval == 4 ~ 1,
    approval == 5 ~ 1))) %>% 
  drop_na()
```

To complete our exploratory data analysis, let's create some data visualizations. 

The primary response variable left in our dataset is `approval`, a (newly) binary variable with 0 representing disapproval of the President and 1 representing approval. So, let's start by looking at the overall distribution of `approval`.

```{r}
ch12 %>%
  ggplot(aes(x = approval)) +
    geom_bar() + 
    labs(y = "Count",
         x = "Presidential Approval",
         title = "Presidential Approval in 2018") 
```

According to this graph, there are roughly 10,000 more voters who disapprove of Trump. To make things more interesting, let's look at `approval` across gender.

```{r}
ch12 %>%
  ggplot(aes(x = approval, fill = gender)) +
  geom_bar() + 
  labs(y = "Count",
       x = "Presidential Approval",
       title = "Presidential Approval in 2018 by Gender") +
  facet_wrap(~ gender) + 
  theme(legend.position = "none")
```

It seems that females have higher rates of disapproval of the President than males have.

```{r}
ch12 %>%
  ggplot(aes(fill = approval, x = race, y = age)) +
  geom_bar(position="fill", stat="identity") + 
  labs(y = "Percentage",
       x = "Presidential Approval",
       title = "Presidential Approval in 2018 by Race")
```

Let's add race into the mix. This segmented bar graph shows us the percentage of each race that approved of the president. We can see that the disapproving majority in the overall data is present across most races.

Now, let's use our state variable. Let's create a scatterplot with `approval` to see how the rate of approval varied across states.

```{r, message = FALSE}
ch12 %>%
  mutate(approval = as.integer(approval)) %>%
  mutate(approval = case_when(
    approval == 1 ~ 0,
    approval == 2 ~ 1)) %>%
  group_by(state) %>%
  summarise(avg_approval = sum(approval)/n()) %>%
  ggplot(aes(x = avg_approval, y = reorder(state, avg_approval))) + 
  geom_point() + 
  labs(y = "State",
       x = "Approval Rate of President",
       title = "Presidential Approval by State")
```

We are now ready to complete the first step for a proper data science project. Just as we have done in previous chapters, we will split our data set into training and testing samples, then create cross-validations from the training data.

```{r}
set.seed(10)
ch12_split <- initial_split(ch12, prob = 0.80)
ch12_train <- training(ch12_split)
ch12_test  <- testing(ch12_split)
ch12_folds <- vfold_cv(ch12_train, v = 10)
```

Now let's use the split data to explore three different models.


## Logistic regression

Our first way of modeling binary/discrete data: logistic regressions. 

Figure \@ref(fig:OLSlogistic) illustrates a data set with a binary (0 or 1) response ($Y$) and a single continuous predictor ($X$).  The blue line is a linear regression to model the probability of a success ($Y=1$) for a given value of $X$. With a binary response, the linear regression has an obvious problem: it can produce predicted probabilities below 0 and above 1. Probabilities can only range from 0 up to and including 1 as these represent a 0% and 100% chance of an event happening, respectively.

The red curve is the *logistic regression* curve.  Note that its characteristic "S" shape always produces predicted probabilities between 0 and 1.  Here is the formula for a logistic regression:

Where $p$ is the probability of a "yes" or "success" for a given set of predictors $X$.

<!-- Revisit nomenclature after chapter 5 -->

```{r, OLSlogistic, fig.align="center", out.width="60%", fig.cap='Linear vs. logistic regression models for binary response data.', echo=FALSE, warning=FALSE, message=FALSE}

set.seed(0)
dat <- tibble(x=runif(200, -5, 10),
                  p=exp(-2+1*x)/(1+exp(-2+1*x)),
                  y=rbinom(200, 1, p),
                  logit=log(p/(1-p)))

ggplot(dat, aes(x = x)) +
  geom_point(aes(y = y)) +
  geom_smooth(aes(y = y, color = "blue"), method = "lm", se=FALSE) +
  geom_line(aes(y = p, color = "red")) +
  scale_color_manual(name = 'Regression model', 
         values = c('blue', 'red'), 
         labels = c('Linear', 'Logistic'), guide = 'legend') +
  theme_minimal()
```

<!-- DK: How does the math work here? log(p/1-p) seems, to me, to map 0,1 to 0,infinity. -->

The mathematical function $log\left(\frac{p}{1 - p}\right)$ is called the *logit function* and it transforms variables from the space $(0, 1)$ (like probabilities) to $(-\infty, \infty)$.  The inverse of that function, the *standard logistic function*, is $\frac{1}{1 + e^{-x}}$ and transforms variables from the space $(-\infty, \infty)$ to $(0, 1)$.  From that latter function's name we get the terminology of *logistic regression*.


The process begins with the creation of a workflow object with our model engine.

```{r}
glm_wfl <- workflow() %>% 
   add_model(logistic_reg() %>%
            set_engine("glm") %>%
            set_mode("classification"))
```

Add a recipe.

```{r}
glm_wfl <- workflow() %>% 
  add_model(logistic_reg() %>%
            set_engine("glm") %>%
            set_mode("classification")) %>% 
  add_recipe(recipe(approval ~ age + gender + race,
                    data = ch12_train) %>% 
               step_dummy(gender, race) %>% 
               step_interact(~age:starts_with("race_"))
             )
```

Examine performance on the cross-validation samples.

```{r}
glm_metrics <- glm_wfl %>% 
  fit_resamples(resamples = ch12_folds) %>% 
  collect_metrics()

glm_metrics
```

Check the predictions against the actual values.

```{r}
glm_wfl %>% 
  fit(data = ch12_train) %>% 
  predict(new_data = ch12_train) %>% 
  bind_cols(ch12_train %>% select(approval)) %>% 
  ggplot(aes(y = approval, x = `.pred_class`)) +
    geom_jitter(height = 0.2, alpha = 0.01) +
    labs(title = "Predicting Approval",
         subtitle = "Using demographic predictors",
         x = "Predicted Approval",
         y = "Approval"
         )
```


## Classification and regression trees (CART)

CART is another approach to model binary responses, which we'll learn about in this section.

A **tree** is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as _nodes_. Decision trees predict an outcome variable $Y$ by *partitioning* the predictors.

**Classification trees**, or decision trees, are used in prediction problems where the outcome is categorical.  When the outcome is numerical, they are called **regression trees**; hence the acronym **CART**, standing for Classification and Regression Trees.  The general idea here is to build a decision tree and, at the end of each _node_, obtain a predictor $\hat{y}$. In this case, $\hat{y}$ would identify the likelihood of a voter in that node approving of the President.  

But how do we decide on which partitions to make  ($R_1, R_2, \ldots, R_J$) and how do we choose $J$, the total number of partitions? Here is where the algorithm gets a bit complicated.

Classification trees create partitions recursively. We start the algorithm with one partition in which every observation is classified as either 0 or 1. But after the first step we will have two partitions. After the second step, we will split one of these partitions into two and will have three partitions, then four, then five, and so on.

Now, after we define the new partitions $R_1$ and $R_2$, and we decide to stop the partitioning process, we compute predictors by taking the most common category of all the observations $y$ for which the associated $\mathbf{x}$ is in $R_1$ and $R_2$. We refer to these two as $\hat{y}_{R_1}$ and $\hat{y}_{R_2}$ respectively. 


Once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region. 


Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough).  Finally, they can model human decision processes. However, in terms of accuracy, they are rarely the best performing method since they are not very flexible. Random forests, explained in the next section, improve on some of the shortcomings of classification trees.

One limitation of CART is its lack of fitted values. Unlike a `glm()`, you can't clean predicted probabilities or point estimates from CART. Rather, you simply get a prediction of 0 or 1 for whatever observation you pass through the tree.  

<!-- DK: Previous version had a nice graphic showing what a tree looks like. Find another example. -->

First create a workflow object with our model engine.

```{r}
cart_wfl <- workflow() %>% 
  add_model(decision_tree() %>%
            set_engine("rpart",
             model = TRUE) %>%
            set_mode("classification"))
```

Add a recipe.

```{r}
cart_wfl <- workflow() %>% 
  add_model(decision_tree() %>%
            set_engine("rpart",
             model = TRUE) %>%
            set_mode("classification")) %>% 
 add_recipe(recipe(approval ~ age + gender + race,
                    data = ch12_train) %>% 
               step_dummy(gender, race) %>% 
               step_interact(~age:starts_with("race_") )
             )
```

Examine performance on the cross-validation samples.

```{r}
cart_metrics <- cart_wfl %>% 
  fit_resamples(resamples = ch12_folds) %>% 
  collect_metrics()

cart_metrics
```

Check the predictions against the actual values.

```{r}
cart_wfl %>% 
  fit(data = ch12_train) %>% 
  predict(new_data = ch12_train) %>% 
  bind_cols(ch12_train %>% select(approval)) %>% 
  ggplot(aes(y = approval, x = `.pred_class`)) +
    geom_jitter(height = 0.2, alpha = 0.01) +
    labs(title = "Predicting Approval",
        subtitle = "Using demographic predictors",
         x = "Predicted Approval",
         y = "Approval"
         )
```



## Random forests

Random forests are a very popular machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by _averaging_ multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.

The first step is _bootstrap aggregation_ or _bagging_. The general idea is to generate many predictors, each using classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees **randomly** different, and the combination of trees is the **forest**. 

Our steps.

1. Build decision trees using a portion of the data called the training set. We refer to the fitted models as $T_1, T_2, \dots, T_B$. We later explain how we ensure they are different.

2. For every observation in the test set, form a prediction $\hat{y}_j$ using tree $T_j$.

3. For categorical data classification, predict $\hat{y}$ with majority vote (most frequent class among $\hat{y}_1, \dots, \hat{y}_T$).
     
So how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let $N$ be the number of observations in the training set. To create $T_j, \, j=1,\ldots,B$ from the training set we do the following:

4. Create a bootstrap training set by sampling $N$ observations from the training set **with replacement**. This is the first way to induce randomness. 
    
5. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy. 

First create a workflow object with our model engine.

```{r}
rf_wfl <-workflow() %>% 
  add_model(rand_forest() %>%
            set_engine("randomForest") %>%
             set_mode("classification"))
```

Add a recipe.

```{r}
rf_wfl <- workflow() %>% 
  add_model(rand_forest() %>%
            set_engine("randomForest") %>%
             set_mode("classification")) %>%  
  add_recipe(recipe(approval ~ age + gender + race,
                    data = ch12_train) %>% 
               step_dummy(gender, race) %>% 
               step_interact(~age:starts_with("race_") )
             )
  
```

Examine performance on the cross-validation samples.

```{r, cache=TRUE}
rf_metrics <- rf_wfl %>% 
  fit_resamples(resamples = ch12_folds) %>% 
  collect_metrics()

rf_metrics
```


Check the predictions against the actual values.

```{r, cache=TRUE}
rf_wfl %>% 
  fit(data = ch12_train) %>% 
  predict(new_data = ch12_train) %>% 
  bind_cols(ch12_train %>% select(approval)) %>% 
  ggplot(aes(y = approval, x = `.pred_class`)) +
    geom_jitter(height = 0.2, alpha = 0.01) +
    labs(title = "Predicting Approval",
         subtitle = "Using demographic predictors",
         x = "Predicted Approval",
         y = "Approval"
         )
```

## Cardinal Virtues
<!-- DK: Make this aprt like Chapter 11. Put model comparison under courage. Once we choose a model then, under Temperance, we use all our data to fit it. Then we use the resulting _fit object to answer our question. -->

_Wisdom_ tells us not to use the data if the data won’t help. Why build a model that is not relevant to the problem you face? The answer is, we don't. However, we also want to consider that it is impossible to account for all the variables we care about. A perfect world does not exist, so we must use caution when drawing conclusions.

_Justice_ checks in with us to make sure the models we are building are as unbiased as possible. We determine whether the model is predictive or causal. Then, we proceed to describe the math of the model, but in the simplest possible way.

_Courage_ is what we need to implement our model into code. By doing so, we can fit models and then choose which model is the most meaningful for our purposes. Recall that we should only select the more complex models or formulas if the additional complexity is justified. Does the linear model have better accuracy than the logistic regression model? Well, let's find out in the section below.

### Comparing Models

Recall that the most common method for deciding which model to choose is to look at which one does the best predicting out of sample. Below is a summary of how well our three different models performed.

```{r}
tibble(model = rep(c("Logistic Regression", 
                     "Classification and Regression Tree", 
                     "Random Forest"), each = 2)) %>% 
  bind_cols(bind_rows(glm_metrics, cart_metrics, rf_metrics)) 
```

As you can see, the logistic model has the best accuracy. Therefore, we choose it as our model. The next step is to check performance using the test data. Recall we never use the test data until the final prediction and the resulting model performance calculations. Do the predictions seem sensible? Reminder that we still use the training data to fit the model.


```{r}
glm_wfl %>% 
  fit(data = ch12_train) %>% 
  predict(new_data = ch12_test) %>% 
  bind_cols(ch12_test %>% select(approval)) %>% 
  metrics(truth = approval, estimate = .pred_class)
```
 
What does this tell us? Well, going forward, the accuracy of our model would be 62% accurate with data it has never seen. 
 
 
_Temperance_ guides us to the finish line. Now that we have chosen our model and tested out of sample, we once again use caution. it is difficult to apply any model to the real world because the world is constantly changing around us. 


Let's answer the following question.


































