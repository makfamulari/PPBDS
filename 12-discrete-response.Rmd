---
output_yaml:
  - _output.yml
---

<!-- DK: Sure would be nice to do one multi-category model. Once we get rid of all the logistic regression stuff, we have some extra space/time. -->

# Discrete Response {#discrete-response}


Packages:

```{r, message=FALSE}
library(tidyverse)
library(skimr)
library(PPBDS.data)
library(tidymodels)
```


**Binary responses** take on only two values: success ($Y=1$) or failure ($Y=0$), yes ($Y=1$) or no ($Y=0$), et cetera. Binary responses are one of the most common types of data that statisticians encounter.  We are often interested in modeling the probability of success, $p$, based on a set of covariates. As with regression, there are two broad categories of problems: *modeling for prediction* and *modeling for causation*. Although terminology varies across fields, "regression" is generally used for situations in which our *dependent variable* is continuous, as in Chapter \@ref(continuous-response). "Classification" applies to cases in which the dependent variable takes on discrete values, the simplest of which is the binary case.

In this chapter, we will look at three common techniques of **classification** of binary data.  First, we will consider logistic regression.  Second, we will consider classification and regression trees (CART).  Third, we will discuss random forests. We use the **tidymodels** tools for all examples. At the end, we will compare the performances of all three models.

<!-- DK: Finish with a question. -->

## Binary classification metrics

<!-- DK: Get rid of most of this. -->

Recall the discussion in Chapter \@ref(model-choice) about how to measure model accuracy. Classification models require different measures than those we used in Chapter \@ref(continuous-response) when considering continuous outcome measures.

The **modeldata** package contains example predictions from a test data set with two classes ("Class1" and "Class2"):

```{r performance-two-class-example}
data(two_class_example)
str(two_class_example)
```

The second and third columns are the predicted class probabilities for the test set while `predicted` are the discrete predictions. 

For the hard class predictions, there are a variety of **yardstick** functions that are helpful: 

```{r performance-class-metrics}
# A confusion matrix: 
conf_mat(two_class_example, truth = truth, estimate = predicted)

accuracy(two_class_example, truth = truth, estimate = predicted)

# Matthews correlation coefficient:
mcc(two_class_example, truth, predicted)

# F1 metric:
f_meas(two_class_example, truth, predicted)
```

For binary classification data sets, these functions have a standard argument called `event_level`. The _default_ is that the **first** level of the outcome factor is the event of interest. 

```{block, type = "rmdnote"}
There is some heterogeneity in R functions in this regard; some use the first level and others the second to denote the event of interest. We consider it more intuitive that the first level is the most important. The second level logic is borne of encoding the outcome as 0/1 (in which case the second value is the event) and unfortunately remains in some packages. However, tidymodels (along with many other R packages) _require_ a categorical outcome to be encoded as a factor and, for this reason, the legacy justification for the second level as the event becomes irrelevant.  
```

As an example where the second class is the event: 

```{r}
f_meas(two_class_example, truth, predicted, event_level = "second")
```

In the output above, the `.estimator` value of "binary" indicates that the standard formula for binary classes will be used. 

There are numerous classification metrics that use the predicted probabilities as inputs rather than the hard class predictions. For example, the receiver operating characteristic (ROC) curve computes the sensitivity and specificity over a continuum of different event thresholds. The predicted class column is not used. There are two **yardstick** functions for this method: `roc_curve()` computes the data points that make up the ROC curve and `roc_auc()` computes the area under the curve. 

The interfaces to these types of metric functions use the `...` argument placeholder to pass in the appropriate class probability column. For two-class problems, the probability column for the event of interest is passed into the function:

```{r}
two_class_curve <- roc_curve(two_class_example, truth, Class1)
two_class_curve

roc_auc(two_class_example, truth, Class1)
```

The `two_class_curve` object can be used in a `ggplot` call to visualize the curve. There is an `autoplot()` method that will take care of the details:

```{r performance-2class-roc-curve}
autoplot(two_class_curve)
```

There are a number of other functions that use probability estimates, including `gain_curve()`, `lift_curve()`, and `pr_curve()`. 



## Exploratory Data Analysis (EDA)

Begin with our usual libraries:

```{r, message=FALSE}
library(tidyverse)
library(broom)
library(skimr)
library(PPBDS.data)
library(tidymodels)
```

Before we start modeling, let's perform some exploratory analysis on the dataset we'll be working with, cces. Cces stands for the Cooperative Congressional Election Study, a study regarding the approval rating of individual voters to their sitting president. Each row captures one voter, some of their demographic information, and how highly they approve (or disaprprove) of the president. 

```{r}
glimpse(cces)
```

We will tweak the data by only looking at observations recorded in the year 2018 so that all the responses are about the same president. We'll also select the variables that are currently of interest to us. Finally, because this chapter will be dealing with logistic regressions, we want to convert the numeric `approval` variable into a binary variable. `approval` is a numeric variable from 1-5 with 5 representing the highest approval of the president. In order to do this, we have to turn approval into a binary variable. 1-2 will be coded to 0 to signify disapproval and 3-5 will be coded to 1 for approval. We will also cast `approval` as a factor variable rather than a number, which is useful information for models.

```{r}
ch12 <- cces %>%
  filter(year == 2018) %>%
  select(state, age, gender, race, education, ideology, approval) %>%
  mutate(approval = as.factor(case_when(
    approval == 1 ~ 0,
    approval == 2 ~ 0,
    approval == 3 ~ 1,
    approval == 4 ~ 1,
    approval == 5 ~ 1)))
```

From this, we can gather that there are 16 variables. Notably, there are 60,000 observations even after filtering only for the year 2018. 

Let's also display a random sample of 5 rows of the 60,000 rows. 

```{r}
ch12 %>%
  sample_n(5)
```

Now, let’s compute summary statistics. Let’s use the `skim()` function from the `skimr` package.

```{r}
ch12 %>% 
  skim()
```

You'll notice that we are missing data for our ideology and approval variables. The `complete_rate` column tells us that approval has 3% missing observations and ideology has 0.7% missing observations. Let's use the function `drop_na()` to get rid of these missing observations so they don't interfere with our models later in the chapter:

```{r}
ch12 <- cces %>%
  filter(year == 2018) %>%
  select(state, age, gender, race, education, ideology, approval) %>%
  mutate(approval = as.factor(case_when(
    approval == 1 ~ 0,
    approval == 2 ~ 0,
    approval == 3 ~ 1,
    approval == 4 ~ 1,
    approval == 5 ~ 1))) %>% 
  drop_na()
```

To complete our exploratory data analysis, let's create some data visualizations. 

The primary response variable left in our dataset is `approval`, a (newly) binary variable with 0 representing disapproval of the President and 1 representing approval. So, let's start by looking at the overall distribution of `approval`.

```{r}
ch12 %>%
  ggplot(aes(x = approval)) +
    geom_bar() + 
    labs(y = "Count",
         x = "Presidential Approval",
         title = "Presidential Approval in 2018") 
```

According to this graph, there are roughly 10,000 more voters who disapprove of Trump. To make things more interesting, let's look at `approval` across gender and then race.

```{r}
ch12 %>%
  ggplot(aes(x = approval, fill = gender)) +
  geom_bar() + 
  labs(y = "Count",
       x = "Presidential Approval",
       title = "Presidential Approval in 2018 by Gender") +
  facet_wrap(~ gender) + 
  theme(legend.position = "none")
```

It seems that females have higher rates of disapproval of the President than males have.

```{r}
ch12 %>%
  ggplot(aes(fill = approval, x = race, y = age)) +
  geom_bar(position="fill", stat="identity") + 
  labs(y = "Percentage",
       x = "Presidential Approval",
       title = "Presidential Approval in 2018 by Race")
```

This segmented bar graph shows us the percentage of each race that approved of the president. We can see that the disapproving majority in the overall data is present across most races.

Now, let's use our state variable. Let's create a scatterplot with `approval` to see how the rate of approval varied across states.

```{r, message = FALSE}
ch12 %>%
  mutate(approval = as.integer(approval)) %>%
  mutate(approval = case_when(
    approval == 1 ~ 0,
    approval == 2 ~ 1)) %>%
  group_by(state) %>%
  summarise(avg_approval = sum(approval)/n()) %>%
  ggplot(aes(x = avg_approval, y = reorder(state, avg_approval))) + 
  geom_point() + 
  labs(y = "State",
       x = "Approval Rate of President",
       title = "Presidential Approval by State")
```

Let's now complete the first step for a proper data science project. We will split our data set into training and testing samples, then create cross-validations from the training data.

```{r}
set.seed(10)
ch12_split <- initial_split(ch12, prob = 0.80)
ch12_train <- training(ch12_split)
ch12_test  <- testing(ch12_split)
ch12_folds <- vfold_cv(ch12_train, v = 10)
```

We can now use the split data to explore different models.



## Logistic regression



Now that we know our dataset a little better, let's begin our first way of modeling binary/discrete data: logistic regressions. 

Figure \@ref(fig:OLSlogistic) illustrates a data set with a binary (0 or 1) response ($Y$) and a single continuous predictor ($X$).  The blue line is a linear regression to model the probability of a success ($Y=1$) for a given value of $X$. With a binary response, the linear regression has an obvious problem: it can produce predicted probabilities below 0 and above 1. Probabilities can only range from 0 up to and including 1 as these represent a 0% and 100% chance of an event happening, respectively.

The red curve is the *logistic regression* curve.  Note that its characteristic "S" shape always produces predicted probabilities between 0 and 1.  Here is the formula for a logistic regression:

Where $p$ is the probability of a "yes" or "success" for a given set of predictors $X$.

<!-- Revisit nomenclature after chapter 5 -->

```{r, OLSlogistic, fig.align="center", out.width="60%", fig.cap='Linear vs. logistic regression models for binary response data.', echo=FALSE, warning=FALSE, message=FALSE}

set.seed(0)
dat <- tibble(x=runif(200, -5, 10),
                  p=exp(-2+1*x)/(1+exp(-2+1*x)),
                  y=rbinom(200, 1, p),
                  logit=log(p/(1-p)))

ggplot(dat, aes(x = x)) +
  geom_point(aes(y = y)) +
  geom_smooth(aes(y = y, color = "blue"), method = "lm", se=FALSE) +
  geom_line(aes(y = p, color = "red")) +
  scale_color_manual(name = 'Regression model', 
         values = c('blue', 'red'), 
         labels = c('Linear', 'Logistic'), guide = 'legend') +
  theme_minimal()
```

<!-- DK: How does the math work here? log(p/1-p) seems, to me, to map 0,1 to 0,infinity. -->

The mathematical function $log\left(\frac{p}{1 - p}\right)$ is called the *logit function* and it transforms variables from the space $(0, 1)$ (like probabilities) to $(-\infty, \infty)$.  The inverse of that function, the *standard logistic function*, is $\frac{1}{1 + e^{-x}}$ and transforms variables from the space $(-\infty, \infty)$ to $(0, 1)$.  From that latter function's name we get the terminology of *logistic regression*.


The process begins with the creation of a workflow object with our model engine.

```{r}
glm_wfl <- workflow() %>% 
   add_model(logistic_reg() %>%
            set_engine("glm") %>%
            set_mode("classification"))
```

Add a recipe.

```{r}
glm_wfl <- workflow() %>% 
  add_model(logistic_reg() %>%
            set_engine("glm") %>%
            set_mode("classification")) %>% 
  add_recipe(recipe(approval ~ age + gender + race,
                    data = ch12_train) %>% 
               step_dummy(gender, race) %>% 
               step_interact(~age:starts_with("race_"))
             )
```

Examine performance on the cross-validation samples.

```{r}
glm_metrics <- glm_wfl %>% 
  fit_resamples(resamples = ch12_folds) %>% 
  collect_metrics()

glm_metrics
```

Check the predictions against the actual values.

```{r}
glm_wfl %>% 
  fit(data = ch12_train) %>% 
  predict(new_data = ch12_train) %>% 
  bind_cols(ch12_train %>% select(approval)) %>% 
  ggplot(aes(y = approval, x = `.pred_class`)) +
    geom_jitter(height = 0.2, alpha = 0.01) +
    labs(title = "Predicting Approval",
         subtitle = "Using demographic predictors",
         x = "Predicted Approval",
         y = "Approval"
         )
```


## Classification and regression trees (CART)

```{r, echo=FALSE}
img_path <- "images"
```

### What is CART?

Logistic regression is just one of many methods we can use to model binary responses.  CART is another approach, which we'll learn about in this section.

A **tree** is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as _nodes_. Decision trees predict an outcome variable $Y$ by *partitioning* the predictors.

**Classification trees**, or decision trees, are used in prediction problems where the outcome is categorical.  When the outcome is numerical, they are called **regression trees**; hence the acronym **CART**, standing for Classification and Regression Trees.  The general idea here is to build a decision tree and, at the end of each _node_, obtain a predictor $\hat{y}$. In this case, $\hat{y}$ would identify the likelihood of a voter in that node approving of the President.  

But how do we decide on which partitions to make  ($R_1, R_2, \ldots, R_J$) and how do we choose $J$, the total number of partitions? Here is where the algorithm gets a bit complicated.

Classification trees create partitions recursively. We start the algorithm with one partition in which every observation is classified as either 0 or 1. But after the first step we will have two partitions. After the second step we will split one of these partitions into two and will have three partitions, then four, then five, and so on.

Now, after we define the new partitions $R_1$ and $R_2$, and we decide to stop the partitioning process, we compute predictors by taking the most common category of all the observations $y$ for which the associated $\mathbf{x}$ is in $R_1$ and $R_2$. We refer to these two as $\hat{y}_{R_1}$ and $\hat{y}_{R_2}$ respectively. 

<!--MB: make this a margin note?
But how do we pick the predictor $j$ and the value $s$? One of the more popular ways for categorical data is the _Gini Index_. In a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The _Gini Index_ is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define $\hat{p}_{j,k}$ as the proportion of observations in partition $j$ that are of class $k$. The Gini Index is defined as $\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})$ -->
<!-- If you study the formula carefully you will see that it is in fact 0 in the perfect scenario described above, since $\hat{p}_{j,k}(1-\hat{p}_{j,k}) = 0$ for all $k$. -->

Once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region. 

<!-- But when do we stop partitioning?  Every time we split and define two new partitions, the Gini Index improves. This is because with more partitions, our model has more flexibility to adapt to our data.  However, our model may therefore perform worse when exposed to new data (this problem is called *overfitting*). This connects to our discussion of validity and models, as the conditions used to create the model will be too specific to accurately extrapolate to new data points. To avoid this, the algorithm sets a minimum for how much the Gini Index must improve for another partition to be added. This parameter is referred to as the _complexity parameter_ ($c_p$). The measure of fit must improve by a factor of $c_p$ for the new partition to be added. Large values of $c_p$ will therefore force the algorithm to stop earlier which results in fewer nodes. -->

Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough).  Finally, they can model human decision processes. However, in terms of accuracy, they are rarely the best performing method since they are not very flexible. Random forests, explained in the next section, improve on some of the shortcomings of classification trees.

One limitation of CART is its lack of fitted values. Unlike a `glm()`, you can't clean predicted probabilities or point estimates from CART. Rather, you simply get a prediction of 0 or 1 for whatever observation you pass through the tree.  

### Multivariate CART

Before creating any model, let's split our data into a training and testing set, just like we did with our multivariate logistical regression.

```{r}
cart_split <- initial_split(ch12)

cart_training <- cart_split %>% training()
cart_testing <- cart_split %>% testing()
```

To create a CART using multiple variables, we'll use the `decision_tree()` model specification and the `"rpart"` engine.  The syntax is very similar to when we used `logistic_reg()`.  Note that our binary response variable has to be a factor, just like with `logistic_reg()`. 

```{r}
tree_mod <- decision_tree() %>%
  set_engine("rpart",
             model = TRUE) %>%
  set_mode("classification")
```

Note that we added the argument `model = TRUE` to `set_engine()`.  This saves the model frame, which we will need to avoid a warning when we plot the trees later.

The function `set_mode()` wasn't necessary when we did logistic regression.  Here it clarifies that we want a *classification* tree rather than a *regression* tree.

Next, let's create our formula for this tree. We will use the same formula that we used in the logistical regression.

```{r}
cart_formula <- formula(approval ~ age + gender + race)
```

Now that we have the object `tree_mod` and our formula, we can use `fit()` to construct our tree.

```{r}
cart_fit <- fit(tree_mod,
                 cart_formula,
                 cart_training)
```

As you can see, **tidymodels** has the same workflow for creating a CART as it does for fitting a logistic regression, but with our model specification saved in `tree_mod` rather than the model specification we saved in `logistic_mod`.

What was the result of our tree?

```{r}
cart_fit
```

It's not helpful to look at the results of a tree as text.  In order to visualize the tree, we'll use the `prp()` function in the **rpart.plot** package.  Remember that the model object is stored in `cart_fit$fit`.

```{r, message = FALSE}
library(rpart.plot)

cart_fit$fit %>%
  prp(extra = 6, varlen = 0, faclen = 0)
```

The arguments `varlen = 0` and `faclen = 0` ensure that the full variable names and factor levels are printed.  The argument `extra = 6` shows the proportion of "yes" outcomes within a given partition. Since we'll be using these same arguments throughout the chapter, we'll create a new function that calls `prp()` but with these options as defaults:

```{r}
prp_ch12 <- function(x, ...) prp(x, extra = 6, varlen = 0, faclen = 0, ...)

cart_fit$fit %>%
  prp_ch12()
```

So, how do we interpret this tree? We are given two partitions; the first is for race. The observations that identify as any of the named races are classified as disapproving of the President, denoted by the 0 on that node. All other racial identities proceed to the next partition. If these people are younger than 44, they are also classified as disapproving of the president. Voters 44 and older are classsified as approving of the President. 

The decimal numbers within each box identify how many observations in that node approved of the President. This means that only 20% of observations in the first node approved of the President, 38% in the second node, and 54% in the third node.

You may notice that this tree did not factor in gender. 
<!-- MB: Trees cannot handle interaction terms, -->

### K-Fold Cross-Validation

Once again, we can check whether the model we created is accurate by using cross-validation. 

We'll use `vfold_cv()` to create `v = 5` splits in the training set, using the final fold to asses the model's accuracy.

```{r}
cart_folds <- cart_training %>%
  vfold_cv(v = 5)
```

Then, we'll create our recipe and plug it into `fit_resamples()` along with `tree_mod` and `cart_folds`. We'll use `collect_metrics()` to get our accuracy values.

<!-- MB: Added interaction term here, but not in the tree above -->

```{r, message = FALSE, warning = FALSE}
cart_recipe <- recipe(approval ~ age + race + gender, data = cart_training) 

cart_recipe <- cart_recipe %>%
  step_interact(terms = ~ age:race)

fit_resamples(tree_mod,
              cart_recipe,
              cart_folds) %>%
  collect_metrics()
```
<!-- Interpret accuracy values -->

Now that we saw how the training set performed, we'll use our testing set. We do so by applying `cart_testing` to `predict()`.

```{r}
tree_mod %>%
  fit(cart_formula, data = cart_training) %>%
  predict(new_data = cart_testing)
```

Once again, the model predicts whether the observations in testing set would approve or disapprove of the President based on the model created with the training set. 

To extract the rmse, we set the "truth" to `approval` so this function can compare our predicted values to the true values.

```{r}
tree_mod %>%
  fit(cart_formula, data = cart_training) %>%
  predict(new_data = cart_testing) %>%
  bind_cols(cart_testing) %>%
  rmse(truth = as.numeric(approval), estimate = as.numeric(.pred_class))
```

The mean squared error is about 0.616.

## CART Summary

First create a workflow object with our model engine.

```{r}
cart_wfl <- workflow() %>% 
  add_model(decision_tree() %>%
            set_engine("rpart",
             model = TRUE) %>%
            set_mode("classification"))
```

Add a recipe.

```{r}
cart_wfl <- workflow() %>% 
  add_model(decision_tree() %>%
            set_engine("rpart",
             model = TRUE) %>%
            set_mode("classification")) %>% 
 add_recipe(recipe(approval ~ age + gender + race,
                    data = ch12_train) %>% 
               step_dummy(gender, race) %>% 
               step_interact(~age:starts_with("race_") )
             )
```

Examine performance on the cross-validation samples.

```{r}
cart_metrics <- cart_wfl %>% 
  fit_resamples(resamples = ch12_folds) %>% 
  collect_metrics()

cart_metrics
```

Check the predictions against the actual values.

```{r}
cart_wfl %>% 
  fit(data = ch12_train) %>% 
  predict(new_data = ch12_train) %>% 
  bind_cols(ch12_train %>% select(approval)) %>% 
  ggplot(aes(y = approval, x = `.pred_class`)) +
    geom_jitter(height = 0.2, alpha = 0.01) +
    labs(title = "Predicting Approval",
        subtitle = "Using demographic predictors",
         x = "Predicted Approval",
         y = "Approval"
         )
```


### Predicting New Data

Let's now use our tree to make some predictions. When used on trees, the `predict()` function takes in the tree object and new observations containing the explanatory variables. In this case, we'll be passing in a 54-year-old Asian voter, a 41-year-old Black voter, a 56-year-old White voter, and a 70-year-old Native American voter.

```{r}

newdata <- tibble(race = c("Asian", "Black", "White", "Native American"),
                  age = c(54, 41, 56, 70),
                  gender = c("Male", "Female", "Male", "Female"))

predict(cart_fit, new_data = newdata) 

```

As you can see, the tree predicted that our first two new voters would disapprove of the President while our second two would approve based on their age and race.


## Random forests

### What are random forests?

Random forests are a very popular machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by _averaging_ multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.

The first step is _bootstrap aggregation_ or _bagging_. The general idea is to generate many predictors, each using classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees **randomly** different, and the combination of trees is the **forest**. The specific steps are as follows.

1. Build decision trees using a portion of the data called the training set. We refer to the fitted models as $T_1, T_2, \dots, T_B$. We later explain how we ensure they are different.

2. For every observation in the test set, form a prediction $\hat{y}_j$ using tree $T_j$.

3. For categorical data classification, predict $\hat{y}$ with majority vote (most frequent class among $\hat{y}_1, \dots, \hat{y}_T$).
     
So how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let $N$ be the number of observations in the training set. To create $T_j, \, j=1,\ldots,B$ from the training set we do the following:

4. Create a bootstrap training set by sampling $N$ observations from the training set **with replacement**. This is the first way to induce randomness. 
    
5. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy. 

### Fitting random forests

<!-- MB: http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/#evaluate-the-model-on-the-test-set
https://s3.amazonaws.com/assets.datacamp.com/production/course_7215/slides/chapter3.pdf
-->

We will demonstrate by fitting a random forest to the House elections data. Before we even write our model formula, we need to split the data to create a *training set* and a *testing set*. The training dataset will be used to create our model and the testing dataset will be used to test our final model’s performance.

```{r}
forest_split <- initial_split(ch12, 
                                prop = 3/4)

forest_train <- training(forest_split)
forest_test <- testing(forest_split)
```

Next, we will use the `rand_forest()` function to create our model specification, setting the engine to `"randomForest"` and the mode to `"classification"`. 

```{r, message = FALSE}
library(randomForest)

forest_mod <- 
  rand_forest() %>%
  set_engine("randomForest") %>%
  set_mode("classification") 

```

We will now create the formula for our forest.

```{r}
forest_formula <- formula(approval ~ age + gender + race + race:age)
```

Finally, let's fit our model. `fit()` will take in our model specification `logistic_mod`, our formula, and our training set of data. We are reserving our testing set for later.

```{r}
forest_fit <- fit(forest_mod, forest_formula, forest_train)
```

If we graph the "OOB estimate of error rate", we can see that this model settles around an error rate of just under 40% (or, looking at it the other way, an accuracy of 60%). We can see how the error rate of our algorithm changes as we add trees by looking at `forest_fit$fit$err.rate[, "OOB"]`.  By default, `randomForest()` (the engine we specified) grows 500 trees. -->

```{r}
tibble(`Error rate` = forest_fit$fit$err.rate[, "OOB"], Trees = 1:500) %>%
  ggplot(aes(x = Trees, y = `Error rate`)) +
  geom_line() + 
  theme_classic()
```

We can see that in this case, the accuracy improves as we add more trees until about 50 trees where accuracy stabilizes. Random forests often perform better than other methods. However, a disadvantage of random forests is that we lose interpretability---we don't get anything like the coefficients from a logistic regression or the single tree from CART. 

### K-Fold Cross-Validation

Just like with CART and logistic regression, we will use cross-validation to check the accuracy of our random forest. Once again, we'll create the recipe and add our interaction term seperately.

```{r}

forest_recipe <- recipe(approval ~ age + gender + race, 
                        data = forest_train)

forest_recipe <- forest_recipe %>%
  step_interact(terms = ~ age:race)


```


<!-- approval_workflow <- workflow() %>% -->
<!-- add the recipe -->
<!--  add_recipe(approval_recipe) %>% -->
<!-- add the model -->
<!--  add_model(forest_mod) -->


We will also create our folds using `vfold_cv()`.

```{r}
forest_folds <- forest_train %>%
  vfold_cv(v = 5)
```

Now we use `fit_resamples()` to see how it accurately  the model performed on the fifth and final fold of the training set. We will save the output of `fit_resamples()` in forest_metrics to use later.

```{r, message = FALSE, warning = FALSE}

forest_metrics <- fit_resamples(forest_mod,
                                forest_recipe,
                                forest_folds) 

forest_metrics %>% collect_metrics()

```

<!-- MB: Interpret -->

Now that we saw how the training set performed, we'll use our testing set. We do so by applying `forest_test` to `predict()`.


<!-- forest_mod %>% -->
<!--  fit(forest_formula, data = forest_train) %>% -->
<!--  predict(new_data = forest_test) -->


To extract the rmse, we again set the "truth" to `approval` so this function can compare our predicted values to the true values.


<!-- forest_mod %>% -->
<!--  fit(forest_formula, data = forest_train) %>% -->
<!--  predict(new_data = forest_test) %>% -->
<!--  bind_cols(forest_test) %>% -->
<!--  rmse(truth = as.numeric(approval), estimate = as.numeric(.pred_class)) -->


The mean squared error is about 0.619.

<!-- We can use `collect_predictions()` to see exactly how many of the predictions on the test set were correct. It appears that `r 7001+2050` were correct (the addition of the (0,0) and (1,1) cells), while `r 4082+1321` were incorrect. -->

<!-- ```{r} -->

<!-- test_predictions <- forest_metrics %>% collect_predictions() -->
<!-- test_predictions %>%  -->
<!--   conf_mat(truth = approval, estimate = .pred_class) -->

<!-- ``` -->

<!-- We could also plot distributions of the predicted probability distributions for voter. -->

<!-- ```{r} -->
<!-- test_predictions %>% -->
<!--   ggplot() + -->
<!--   geom_density(aes(x = .pred_1, fill = approval),  -->
<!--                alpha = 0.5) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- final_model <- fit(approval_workflow, ch12) -->
<!-- ``` -->

## Random Forest Summary

First create a workflow object with our model engine.

```{r}
rf_wfl <-workflow() %>% 
  add_model(rand_forest() %>%
            set_engine("randomForest") %>%
             set_mode("classification"))
```

Add a recipe.

```{r}
rf_wfl <- workflow() %>% 
  add_model(rand_forest() %>%
            set_engine("randomForest") %>%
             set_mode("classification")) %>%  
  add_recipe(recipe(approval ~ age + gender + race,
                    data = ch12_train) %>% 
               step_dummy(gender, race) %>% 
               step_interact(~age:starts_with("race_") )
             )
  
```

Examine performance on the cross-validation samples.

```{r}
rf_metrics <- rf_wfl %>% 
  fit_resamples(resamples = ch12_folds) %>% 
  collect_metrics()

rf_metrics
```


Check the predictions against the actual values.

```{r}
rf_wfl %>% 
  fit(data = ch12_train) %>% 
  predict(new_data = ch12_train) %>% 
  bind_cols(ch12_train %>% select(approval)) %>% 
  ggplot(aes(y = approval, x = `.pred_class`)) +
    geom_jitter(height = 0.2, alpha = 0.01) +
    labs(title = "Predicting Approval",
         subtitle = "Using demographic predictors",
         x = "Predicted Approval",
         y = "Approval"
         )
```



### Predicting New Data

Let's use our final forest model to make some predictions. We'll be passing in the same observations as with the CART example: a 54-year-old Asian voter, a 41-year-old Black voter, a 56-year-old White voter, and a 70-year-old Native American voter. However, our forest also includes a gender variable. We'll make the first and third observations female.

```{r}
# DK: If the model has been fit with race as a factor, then race in newdata must
# also be a factor and, subtle point!, must have the same levels as the variable
# which was used to fit the model, even if there are no observations with those
# values in the newdata.


newdata <- tibble(race = c("Asian", "Black", "White", "Native American"),
                  age = c(54, 41, 56, 70),
                 gender = c("Female", "Male", "Female", "Male"))

#predict(forest_fit, new_data = newdata) 

```

Our forest predicted that the first three voters would disaprove of the President and that our final voter would approve. These predictions differ from our CART predictions most likely because of the added variable: gender. Because females disapproved of the President at higher rates, the third voter was predicted to disapprove instead.

```{r}
#posterior_linpred(forest_fit, newdata)
```


## Comparing Models

Recall that the most common method for deciding which model to choose is to look at which one does the best predicting out of sample. Below is a summary of how well our three different models performed.

```{r}
tibble(model = rep(c("glm Model", 
                     "CART Model", 
                     "RF Model"), each = 2)) %>% 
  bind_cols(bind_rows(glm_metrics, cart_metrics, rf_metrics)) 
```

<!-- BG: This code returns an error. -->
<!-- BG: once code errors are sorted, talk about results here -->

## The Question Pt. 2

There are an infinite numbers of comparisons to be made within this dataset. When creating a model, it's best to have a specific question in mind. At the end, you can answer that question, using your model to learn more about the world.

Let's return to our original question of how Asians and Whites specifically vary in their support of the president. We'll use our logistic regression first, as that model performed with the highest roc_auc. 

<!-- imprtnt: When comparing blacks and hispanics, the approval was predicted to be 0 for every age. So i switched to white and asian here. Need to go back and fix in earlier work (in The Question section) -->

```{r, echo = FALSE}

# Creating dataset

newdata <- tibble(race = c(rep("Asian", 12),
                           rep("White", 12)),
                  age = rep(c(24, 24, 36, 36, 48, 48, 65, 65, 73, 73, 84, 84), 2),
                  gender = rep(c("Female", "Male"), 12))

```

We've created a dataset of 24 new Asian and White voters so that we can use our model to predict if they will vote for the President. Each voter's race, age, and gender have been included as these are the 3 demographic factors our model uses to predict approval. 

```{r}
newdata
```

Next, we'll use the `predict()` function to see how our model thinks these voters will regard the President. 

```{r}

x <- predict(logistic_fit, new_data = newdata) 
x

```

Well, now what? We have our predictions for each of the new voters, but it isn't helpful to answering our question yet. To see greater trends between White and Asian voters, let's visualize our predictions! The new data had an intentionally even spread of age, race, and gender to make our comparisons easier.

Let's use the same strategy from our exploratory data analysis earlier. We can graph age against approval, facet by gender, and color the observations by race. This way, we can view trends across all three variables in one graph.

```{r}

# Cleaning the data 

y <- x %>% 
  bind_cols(newdata) %>%
  mutate(approval = .pred_class) %>%
  select(-.pred_class)

# Graphing the data

y %>%
  ggplot(aes(x = age, y = approval, color = race)) + 
  geom_jitter(size = 2, width = 8, height = 0) + # Adding width so that the dots don't block eachother
  facet_wrap(~gender) + 
  labs(x = "Age",
       y = "Predicted Approval",
       title = "Predicted Approval of President for White and Asian Voters")

```

So, how do White and Asian voters vary in their support of the President in 2018? As we can see, this is a more complex question than one may think. For example, if one were only considering race, they would guess an White voter to be more likely to approve of the President than an Asian voter. However, a 40-year-old White male, according to our model, is not as likely to approve of the President as a 70-year-old Asian man. This is useful information to consider: when using our model to its full extent, we can make predictions based on more than one variable at a time. 

What else can we learn from this graph? Both races follow the trend of older voters being more likely to approve of the President. However, we see that Asian woman were predicted to *disapprove* of the President at any age, while the older Asian men were predicted to approve of him. The margin was much closer for White voters, with both older men and women being predicted to approve.   
