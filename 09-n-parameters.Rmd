---
output_yaml:
  - _output.yml
---



<!-- Becca: -->

<!-- * Wisdom section -->

<!-- * Everything starting with fit_gov_3. Math, preceptor table, mention its predictive. Courage: meaning fit model, show posteriors, interpret posteriors. Show split into outcome, fitted, residuals. -->

# N Parameters {#n-parameters}


*This chapter is a draft.* Come back in a few weeks for a better version.


Having created models with one parameter in Chapter \@ref(one-parameter), two parameters in Chapter \@ref(two-parameters) and three parameters in Chapter \@ref(three-parameters), you are now ready to make the jump to $N$ parameters. 

In this chapter, we will consider models with multiple parameters and the complexities that arise therefrom. 

<!-- DK: Add some more. What are the key lessons? What do we carry over from previous chapters? What is new? Hit the data science research cycle. -->


## EDA of `governors`

Load packages:

```{r, message=FALSE}
library(PPBDS.data)
library(skimr)
library(rstanarm)
library(tidyverse)
```

<!-- DK: Perhaps we need more detail in this EDA?  Explain how we only have data for people who are already dead. That is an interesting restriction, and effects the Preceptor Table in interesting ways. Or is such a discussion too advanced? -->

<!-- DK: Don't keep around variables other than the ones we end up using. -->

<!-- DK: Provide proper citation to Barfort et al. -->

<!-- DK: Go and change governors data to use better variable names than the ones we have here. Maybe switch to years. Or maybe just clean them up here. -->

We will start off by using a subset of the `governors` data set from the **PPBDS.data** package. This data set looks at the lifespans of candidates for governor in the US. It comes from the paper "Longevity Returns to Political Office" by Barfort, Klemmensen and Larsen (2019), which concludes that winning a gubernatorial election increases a candidate's lifespan.


```{r}
glimpse(governors)
```

There are `r ncol(governors)` variables and `r nrow(governors)` observations. In this Chapter, we will only be looking at the variables `last_name`, `year`, `state`, `sex`, `alive_post`, and `alive_pre`. 

```{r}
ch9_gov <- governors %>% 
  select(last_name, year, state, sex, alive_post, alive_pre)
```

There are a few things to note when looking at this data. First, the data set includes the variables `alive_pre` and `alive_post`, which tell us how many days a candidate lived before the election took place and how many days a candidate lived after the election, respectively. Therefore, only politicians who are already deceased are included in this data set. This means that there are only a handful of observations from elections in the last 20 years. Most candidates from that time period are still alive and are, therefore, excluded.

<!-- DK: Too much information? Not enough? -->

Another caveat is that for a given election, only the top two candidates are included in the data. If a politician did not receive the highest or second-highest number of votes, they are excluded. 

Finally, for some observations, only the birth or death year of a candidate could be determined and not the exact date. In those cases, the date was set as July 1st of that year.

```{r}
sample_n(ch9_gov, 10)
```

`sex` is most often "Male", as we might expect.

<!-- DK: Weird that this automagically creates a "TABLE 9.1: Data summary" side margin note. -->

```{r}
skim(ch9_gov)
```

This output groups the variables together by type (character, logical, numeric, etc.).  We are given histograms of the numerical data. In looking at the histogram for `year`, we see that it is skewed right, with half of the observations from election years between 1945 and 1962. This makes sense logically, because we are only looking at deceased candidates, and candidates from more recent elections are more likely to still be alive.

In using this data set, our left-side variable will be `alive_post`. We are trying to understand/predict how many days a candidate will live after the election. Let's look at some graphs and plots showing the relationships between `alive_post` and some of the other variables in our subset.


```{r, echo=FALSE}
ch9_gov %>%
  ggplot(aes(x = year, y = alive_post)) +
  geom_point() +
  labs(title = "US Gubernatorial Candidate Lifespans",
       subtitle = "Candidates who died more recently can't have lived for long post-election",
       caption = "Data: Barfort, Klemmensen and Larsen (2019)",
       x = "Year",
       y = "Days Lived After Election") +
  scale_y_continuous(labels = scales::label_number()) 
```

Starting with the relationship between `alive_post` and `year`, we can see that the data is skewed right and that there is a rough line above which there are no observations. There are no data points in the top right portion of the graph because it is not possible to have run in 2011, lived 20,000 days after the election took place, and still have died before the data set was created. This line represents, approximately, the most a candidate could have possibly lived --- and still have died --- to be included the data set. The reason this line is slanted downward is because the maximum value for this scenario is greater in earlier years. That is, those candidates who ran for governor in earlier years could live a long time after the election and still have died prior to the data set creation, giving them higher `alive_post` values than those who ran for office in more recent years. 

There are fewer observations in later years because fewer recent candidates have died. 


```{r}
ch9_gov %>%
  ggplot(aes(x = sex, y = alive_post)) +
  geom_boxplot() +
  labs(title = "US Gubernatorial Candidate Lifespans",
       subtitle = "Male candidates live much longer after the election",
       caption = "Data: Barfort, Klemmensen and Larsen (2019)",
       x = "Gender",
       y = "Days Lived After Election") +
  scale_y_continuous(labels = scales::label_number()) 
```

This plot shows that men live much longer, on average, than women after the election. Does that make sense to you?

<!-- DK: Add more discussion here? -->


## Wisdom

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Wisdom"}
knitr::include_graphics("other/images/Wisdom.jpg")
```

<!-- DK: Two paragraphs is plenty. Note this is a change. Topic is: If we want to predict how long someone will live, how relevant is this data set? (We are not interested in the question which the authors explore which is the causal effect of victory on lifespan.) We are making a predictive model, not a causal one. Can we use this model for any new person we meet? Only for people who are candidates? For people who are about as rich/successful/educated as the typical candidate? -->


## Justice and Courage

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Justice"}
knitr::include_graphics("other/images/Justice.jpg")
```

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Courage"}
knitr::include_graphics("other/images/Courage.jpg")
```

Because we will be going through a series of models in this chapter, it is useful to combine the virtues of Justice and Courage. To begin, let's model candidate lifespan after the election as a function of candidate lifespan prior to the election. The data:


```{r, echo=FALSE}
ch9_gov %>% 
  ggplot(aes(x = alive_pre, y = alive_post)) +
    geom_point() +
    labs(title = "Longevity of Gubernatorial Candidates",
         subtitle = "Younger candidates live longer", 
         caption = "Data Source: Barfort, Klemmensen and Larsen (2019)",
         x = "Age in Days",
         y = "Days Lived After Election") +
    scale_x_continuous(labels = scales::label_number()) +
    scale_y_continuous(labels = scales::label_number()) 

```


The math is fairly simple:

$$ alive\_post_i =  \beta_0 + \beta_1 alive\_pre_i + \epsilon_i $$


with $\epsilon_i \sim N(0, \sigma^2)$. $alive\_post_i$ is the number of days lived after the election for candidate $i$. $alive\_pre_i$ is the number of days lived before the election for candidate $i$. $\epsilon_i$ is the "error term," the difference between the days-lived of candidate $i$ and the average days-lived of all candidates.  $\epsilon_i$ is normally distributed with a mean of 0 and a standard deviation of $\sigma$. The key distinction is between:

* *Variables*, always subscripted with $i$, whose values (potentially) vary across individuals.

* *Parameters*,  never subscripted with $i$, whose values are constant across individuals.


$\beta_0$ is the "intercept" of the regression, the average value for the population of $alive\_post$ for those for whom $alive\_pre = 0$. $\beta_1$ is the "coefficient" of  $alive\_pre$. Each one day increase in $alive\_pre$ is associated with a $\beta_1$ change in $alive\_post$. Again, this is the value for the population from which are data is drawn.  

There are three unknown parameters, similar to the situation we faced in Chapter \@ref(three-parameters).

You may recall from middle school algebra that the equation of a line is $y = a + b x$. It is defined by two coefficients $a$ and $b$. The intercept coefficient $a$ is the value of $y$ when $x = 0$. The slope coefficient $b$ for $x$ is the increase in $y$ for every increase of one in $x$. When defining a regression line, we use slightly different notation but the fundamental relationship is the same. 

We can use `geom_smooth()` to create the fitted regression line:

```{r, echo=FALSE}
ch9_gov %>% 
  ggplot(aes(x = alive_pre, y = alive_post)) +
    geom_point() +
    geom_smooth(formula = y ~ x, method = "lm", se = FALSE) +
    labs(title = "Longevity of Gubernatorial Candidates",
         subtitle = "Younger candidates live longer", 
         caption = "Data Source: Barfort, Klemmensen and Larsen (2019)",
         x = "Age in Days",
         y = "Days Lived After Election") +
    scale_x_continuous(labels = scales::label_number()) +
    scale_y_continuous(labels = scales::label_number()) 
```

Consider someone who is about 5,000 says younger than the average candidate. We have a score or more data points for candidates around that age. Some of then only lived for about 5,000 days after the election. Others lived for more than 20,000 days. The world is filled with variation. But the fitted line tells us that, on average, we would expect a candidate that age to live for about 15,000 days after the election. 

<!-- DK: Add a red circle to the graphic to show how we figured that out? -->

This model, with a continuous independent (or "predictor") variable has an infinite number of fitted values, one for each possible value of `alive_pre`. This is very different from the models we saw in Chapter \@ref(three-parameters). Those models only had two possible fitted values because the predictor variable only took two possible values.

<!-- DK: I am nervous that alive_pre, as used here, is a different variable that alive_pre when, later, we transform it.  -->

We can implement this model with `stan_glm()`. 


```{r, cache = TRUE}
fit_gov_1 <- stan_glm(data = ch9_gov,
                      formula = alive_post ~ alive_pre,
                      refresh = 0)
```

As we discussed in Chapter \@ref(three-parameters), the most common term for a model like this is a "regression." We have "regressed" `alive_post`, our dependent variable on `alive_pre`, our one independent variable.

The parameter values:

```{r}
print(fit_gov_1, detail = FALSE)
```

As is almost always the case, $\sigma$ is a nuisance parameter, somethings whose value we are not interested in. This is why `stan_glm()` refers to it as an "Auxiliary" parameter.

The posterior distributions of $\beta_0$ (the intercept) and $\beta_1$ (the coefficient of `alive_pre`), on the other hand, are important. Before looking at the posteriors themselves, let's examine the fitted values:


```{r}
ch9_gov %>% 
  ggplot(aes(x = alive_pre, y = alive_post)) +
    geom_point() +
    geom_line(aes(y = fitted(fit_gov_1)), color = "blue") +
    labs(title = "Longevity of Gubernatorial Candidates",
         subtitle = "Blue line shows fitted values", 
         caption = "Data Source: Barfort, Klemmensen and Larsen (2019)",
         x = "Age in Days",
         y = "Days Lived After Election") +
    scale_x_continuous(labels = scales::label_number()) +
    scale_y_continuous(labels = scales::label_number()) 
```

This code is the same as the code we used above, except that we have replaced `geom_smooth()` with `geom_line()`. Calling `fitted()` on a model returns the set of fitted values, which we have plotted by hand.


We can create a formula for the fitted values by placing the median values of the parameters into the model:


$$ alive\_post_i =  26,524 - 0.9 alive\_pre_i + \epsilon_i$$

Consider the intercept. Since our independent variable is `alive_pre`, the intercept is the `alive_post` value when `alive_pre` is zero. Here, we would interpret this intercept as the average lifespan of a gubernatorial candidate after the election, if the candidate was alive for zero days prior to the election. 

This is, of course, substantively nonsense. No one runs for office on the day they are born. In the next model, we will explore ways of making the intercept more interpretable. In the meantime, the math is the math. 

Consider the coefficient for `alive_pre`, $\beta_1$. The median of the posterior, -0.9, represents the slope of the model. For every unit increase in our independent variable, our dependent variable will change by this coefficient. Putting this slope definition in terms of our model, this means that for every additional day a candidate is alive before an election, their lifespan after the election will be 0.9 days lower, on average. If we are given the number of days a candidate lived before the election and want to estimate how long they will live for after, we will multiply the days they were alive prior by this beta of -0.9, then subtract that from the intercept. 

This is a descriptive model, not a causal model. Remember our motto from Chapter \@ref(rubin-causal-model): *No causation without manipulation.* There is no way, for person $i$, to change the days that she has been alive on Election Day. On the day of this election, she is X days old. There is no way to change that. So, there are not two (or more) potential outcomes. Without more than one potential outcome, there can not be a causal effect.

Given that, it is important to monitor our language. We do not believe that that changes in `alive_pre` "cause" changes in `alive_post`. That is obvious. But there are some words and phrases --- like "associated with" and "change by" --- which are too close to causal. (And which we are guilty of using just a few paragraphs ago!) Be wary of their use. *Always think in terms of comparisons* when using a predictive model. We can't change `alive_pre` from X to Y for an individual candidate. We can only compare two candidates (or two groups of candidates), one with `alive_pre` equal to X and the other with `alive_pre` equal to Y. If our model is correct, such candidates will, on average, differ in `alive_post` by $\beta_1$ times the difference between X and Y.

<!-- DK: THis is a critical point. Maybe give a precise example? Go through the math slowly, like ModernDive does? Highlight this somehow? At the very least, we should emphasize the point for each model going forward. -->

Let's look at the posterior of $\beta_1$, the coefficient of `alive_pre`:

<!-- DK: We should standardize what these plots look like, both in this chapter and throughout the book. -->

```{r}
fit_gov_1 %>% 
  as_tibble() %>% 
  ggplot(aes(alive_pre)) + 
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100, ) +
    labs(title = "Posterior Distribution of the Coefficient of `alive_pre`",
         y = "Probability",
         x = "Coefficient of `alive_pre`")
```

<!-- DK: Add some interpretation. Need to explain how the uncertainty in beta_1 is connected to the uncertainty we will see in alive_post, for a given difference (not "change") in alive_pre. -->

<!-- DK: Priority is to add a graphic which shows the "cloud" of possible regression lines, consistent with the posterior for beta_1.  -->


Centering is a tool that is used when the model's intercept does not make substantive sense. To center a model, we pick a constant value, usually the mean of the independent variable, and subtract that constant from every value of the independent variable. 

In this example, we want to center the value for `alive_pre`, the independent variable. First, we must pick the value that we will center by. Here, we will use the mean of `alive_pre`. Once we find this value, we will subtract it from every `alive_pre` value. We have, thereby, changed the meaning of `alive_pre`. It now means the number of days a candidate has been alive, as of Election Day, relative to the average days alive of all candidates on their respective election days.

```{r}
ch9_gov$alive_pre <- ch9_gov$alive_pre - mean(ch9_gov$alive_pre)
```



```{r, cache = TRUE}
fit_gov_1.centered <- stan_glm(data = ch9_gov,
                      formula = alive_post ~ alive_pre,
                      refresh = 0)

print(fit_gov_1.centered, detail = FALSE)
```

In this model, we can see that the intercept has increased while the slope has stayed the same. When we interpret this model, we only have to change the definition of the intercept. Rather than the intercept representing the lifespan of a candidate who was alive for zero days before running for governor, it now represents the post-election lifespan of a gubernatorial candidate who was alive for the mean number of days before running. In this instance, our center value is approximately 18,892, so we will use this value in our interpretation. If a candidate was alive for 18,892 days before running for governor (the mean value in this data set), they are expected to live about 10,300 days after the election, on average. 

<!-- DK: Really need the ideal and actual Preceptor Table for this example. -->

Always think in terms of unknown parameters. What do they mean? Which population do they represent? What ideal Preceptor Table would make their calculation easy? In this case, $\beta_0$ could be interpreted as the average number of days which gubernatorial candidates live after election day. If we had the ideal Preceptor Table, this would be trivial to calculate. Just take the average! No estimation required. But, our actual Preceptor Table has lots of missing values. In particular, many gubernatorial candidates have not . . . uh . . . died. (How inconsiderate!) So, we can't know how many days they will live. All we can do is estimate a posterior probability distribution for $\beta_0$. 

Let's now regress `sex` on `alive_post` to see how candidates' post-election lifespans differ by sex. 

```{r, cache = TRUE}
fit_gov_2 <- stan_glm(data = ch9_gov,
                      formula = alive_post ~ sex - 1,
                      refresh = 0)
```

Note that this workflow. Try one model. Interpret it. Try another model. And then another. There is no one "true" model. There is an infinite space of possible models. Good data science involves an intelligent tour of that space.


```{r}
print(fit_gov_2, detail = FALSE)
```


In this regression, we use the -1 in the formula to make the output more straightforward, with no intercept to interpret. The math of this model is the same as those we saw in Chapter \@ref(three-parameters):

$$ \underbrace{y_i}_{outcome} = \underbrace{\beta_1 x_{f,i} + \beta_2 x_{m,i}}_{model} + \underbrace{\epsilon_i}_{not\ in\ the\ model}$$

where \n
$$x_{f,i}, x_{m,i} \in \{0,1\}$$ \n
$$x_{f,i} +  x_{m,i} = 1$$ \n
$$\epsilon_i \sim N(0, \sigma^2)$$  

<!-- DK: Provide more details on what each of the items in the formula means, even though this is a repeat of what we did in chapter 8. -->

The meanings of $y_i$ and $\epsilon_i$ are the same as in the first models. Indeed, they are the same throughout these exercises.  $x_{f,i}$ and $x_{m,i}$ are 0/1 variables, just like last chapter. They are *variables* whose value varies across individuals.

The important parameters are $\beta_1$ and $\beta_2$. They are the average days-lived post-election for, respectively, women and men. Again, this is not the "average" for the data we have. That is easy to calculate! No estimation required. $\beta_1$ and $\beta_2$ are averages for the entire "population," however we have chosen to define it. Those averages can not be calculated directly. They can only be estimated, by creating a posterior probability distribution.


Looking back to the regression model we just created, we see that there is no intercept. Instead of having a $b_0$ value, we have $b_1$ and $b_2$ for female and male. This makes things easier to interpret. Without having to add or subtract anything from an intercept, this regression tells us that on average, women are expected to live about 6,000 days after running for governor, and men are expected to live 10,000 days.

This is a strange result. Why would men live twice as long as women after the election. One explanation for this might be that women don't run for governor until later in life, and therefore are not expected to live as long.

<!-- DK: Weird result, huh? Men live twice as long as women! Useful to discuss why that might be the case in more detail. -->

Now that we have interpreted the model using a -1 in the formula to get both a $b_1$ value and a $b_2$ value, let's take away the -1 and regress `alive_post` on `sex` to see how our equation changes.

```{r, cache = TRUE}
fit_gov_2a <- stan_glm(data = ch9_gov,
                      formula = alive_post ~ sex,
                      refresh = 0)
```

```{r}
print(fit_gov_2a, detail = FALSE)
```

From this result, we can see that we no longer have a value for female, however we do have an intercept. In this regression our mathematical regression formula is $\hat{alive\_post_i} = b_0 + b_1 x_{m, i}$. $ b_0 $ is our intercept value which here would be around 5850. You may notice that this is very similar to the female value from before. In this type of model, our intercept represents the characteristic of the variable that is left unrepresented in the model. Here our slope, or $b_1$ value is for when the candidate is male. Therefore, the intercept value represents those who are not male: females.

When the candidate is a male, we add the coefficient for male to the intercept value, which gives us the average lifespan of a male gubernatorial candidate after an election. As we can see from adding $b_0$ and $b_1$, this value is the same as what we got for males in the previous model.

Be careful with notation! $\beta_1$ in the no-intercept model is different from $\beta_1$  in the model with an intercept! Notation varies. We must pay attention each time we make a model.

<!-- DK: Clean up and explain this plot. It is relevant. But I am not exactly sure what to do with it. Maybe it really belongs in the last chapter? Or maybe we need to show how to make it correctly by using fitted(). Either way, I think it is always (?) useful to how the raw data and the fitted values in the same graphic. -->



The posterior distribution for $\beta_0 + \beta_1$ can be constructed via simple addition.


```{r}
fit_gov_2a %>% 
  as_tibble() %>% 
  mutate(male_intercept = `(Intercept)` + sexMale) %>% 
  ggplot(aes(male_intercept)) + 
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100, ) +
    labs(title = "Posterior Distribution of Average Male Candidate Days Left`",
         y = "Probability",
         x = "Male Days To Live After the Election")
```

<!-- DK: Maybe graph the posteriors from both versions of the model and show they are the same? -->

The interpretation of this model is the same as we have seen before. There is a true average, across the entire population, of the number of days that male candidates live after the election. We can never know what that true average is. But, it seems highly likely that the true average is somewhere between 10,000 and 10,750 days.


## Multiple Variable Regression

We are going to transition to working with a model that has more than one explanatory variable, x. Our outcome variable will be alive_post, but now we will have two different explanatory variables: alive_pre and sex. Note that sex is a continuous explanatory variable and alive_pre is numerical explanatory variable.


Here is the math we will be using: 

$$ y_i =  \beta_0 + \beta_1 male_i + \beta_2 alive\_pre_i+ \epsilon_i $$
Great! Now that we have our model, let's break it down to explain what each part means. 

* $$ y_i$$ is our outcome variable. In our case, the outcome variable is alive_post, the number of days a person is alive after the election. $$male_i$$ is one of our explanatory variables. If we are predicting the amount of days a male candidate lives after the election, this value will be 1. When we are making this prediction for female candidate, this value will be 0. $$alive_pre$$ is our other explanatory variable that represents the number of days a candidate has lived before the election. 

* $$\beta_0$$ is the avg. number of days lived after the election for women, who on the day of election, have been alive the avg. number of days of all candidates (i.e. both male and female). $$\beta_0$$ is also the intercept of the equation. In other words, $$\beta_0$$ is the expected value of $$ y_i$$ if $$male_i$$ = 0 and $$alive_pre$$ = 0. However, we can ignore that for our purpose because no one running for governor would have been alive 0 days before the election!

* $$\beta_1$$ is insignificant by itself. The only time it has meaning is when its value is connected to our intercept (i.e. $$\beta_0$$ + $$\beta_1$$). When the two are added together, you get the avg. number of days lived after the election for men, who on the day of election, have been alive the avg. number of days of all candidates.

* $$\beta_2$$ is, for the entire population, the avg. difference in $$y_i$$  between two individuals, one of whom has an alive_pre value of 1 greater than the other. Why is this important? Well, it just serves as a comparison for two otherwise identical people. 



Now that we understand our model, let's translate the following model into code. 

```{r, cache = TRUE}
fit_gov_3 <- stan_glm(data = ch9_gov,
                      formula = alive_post ~ sex + alive_pre,
                      refresh = 0)
```

```{r}
print(fit_gov_3, detail = FALSE)
```


Looking above at our results, you can see that our intercept value is 8069. What does this mean? Well, applying this to our discussion above, its significance would be that the average female candidate would be alive 8069 days lived after the election, who on the day of election, have been alive the avg. number of days of all candidates. Now take a look at the value 2281 next to sexMale. We need to connect this value to our intercept value to get something meaningful. Using our formula $$\beta_0$$ + $$\beta_1$$, we find out how many days the average male candidate would be alive after the election, who on the day of election, have been alive the avg. number of days of all candidates. Our answer would be 10305. 


Now take a look at the coefficient for alive_pre, $$\beta_2$$. The median of the posterior, -0.8, represents the slope of the model. For every unit increase in our independent variable, our dependent variable will change by this coefficient. It makes sense that this value is negative. Think about it...the more days a candidate has has lived, the fewer days the candidate has left to live. So, for every extra day a candidate is alive before an election, their lifespan after the election will be 0.8 days lower, on average.


If we applied this knowledge to the model, we would get the following:

$$ y_i =  8052.5 + 2309male_i + -.8 alive\_pre_i+ \epsilon_i $$


Let’s look at some posteriors.



<!-- BG: why isn't this working? It works, but when I try to use pivotlonger(), it fails.  -->

```{r}
fit_gov_3 %>% 
  as_tibble() %>% 
  mutate(male_days = `(Intercept)` + sexMale) %>% 
  rename(female_days = `(Intercept)`) %>% 
  select(female_days, male_days) %>% 
  pivot_longer(cols = female_days:male_days, 
               names_to = "parameters",
               values_to = "days") %>% 
  ggplot(aes(days, color = parameters)) +
    geom_density() +
     labs(title = "Posterior Probability Distribution",
         subtitle = "Men live longer",
         x = "Average Days Lived Post Election",
         y = "Probability") + 
    theme_classic() + 
    scale_y_continuous(labels=scales::percent_format())
```





<!-- BG: This code chunk will show posterior for Beta 2 -->

```{r fit_gov3-p1, exercise = TRUE}
# fit_gov_3 %>% as_tibble() %>% mutate( slope = -.8 * alive_pre ) %>%  ggplot(aes(slope)) + geom_density()

```




<!-- DK: Show three posterior plots: one for beta_2, one for beta_0 (intercept) and one for beta_0 + beta_1.  Interpret them. This is a parallel slopes model. Can we show a parallel slopes graph by hand? Skip this for now. -->


<!-- DK: Cool, eh? It is (mostly?) an age effect! Controlling for age at the election, women live about as long as men, but they are much more likely be candidates as an older age. Again, what we are doing is slowly building more complex models, understanding what they mean. -->

<!-- Also, note that this is a "parallel slopes" model. Explain slowly what that means.  Create a plot. -->

<!-- Side note: I would have thought that overlapping CIs for male/female would equate to insignificance of sex dummy when the regression includes a intercept. But that does not seem to be the case. Am I missing something? -->

<!-- Make the graphic which shows two parallel lines, by calling two layers of geom_smooth().  -->

<!-- With this as warm-up, we are now in a position to explore interactions. -->


Let's now model the numerical outcome variable of alive_post as a function of the two explanatory variables we used above: alive_pre and sex. 

Here is the math we will be using: 

<!-- BG: is this right for the model? model look different now its as a function of?-->


$$ y_i =  \beta_0 + \beta_1 male_i + \beta_2 alive\_pre_i+ \beta_3 male_i *  alive\_pre_i + \epsilon_i$$

Ok, great! Now that we have our model, let's break it down to explain what each part means. Note that a few of our meanings will be the same as our previous model above.

* $$ y_i$$, our outcome variable, is still alive_post. We want to know how many days a candidate will live after an election. 

<!-- BG: little confused about how does the description for the paragraph of explanatory varirables changes now its a function. -->

* explanatory variables paragraph:

* $$\beta_0$$ is the avg. number of days lived after the election for women, who on the day of election, have been alive the avg. number of days of all female candidates. Note that this is different than our previous example where it was the avg. of all candidates. $$\beta_0$$ is also the intercept of the equation. 

* $$\beta_2$$ is the coefficient of alive_pre. It it just the slope for women. **Note**: in our last example,  $$\beta_2$$ wqs the slope for the whole population. Now we are getting more specific. 

* $$\beta_3$$ is insignificant. However, it gains meaning when it is added to $$\beta_2$$, which results in the slope for men.

```{r, cache = TRUE}
fit_gov_4 <- stan_glm(data = ch9_gov,
                      formula = alive_post ~ sex*alive_pre,
                      refresh = 0)
```

```{r}
print(fit_gov_4, detail = FALSE)
```

<!-- DK: Take your time explaining what this means! It is not trivial. Note that this is a non-parallel slopes model. -->

Unlike our last model, we now have two intercepts to consider: one for females and one for males. Our intercept above,  $$\beta_0$$, is 6008.4. This is the intercept for females. This means  the avg. number of days lived after the election for women is 6008.4, The second intercept is for males. This is calculated by adding the value 4332.1 to the intercept for females 6008.4, which is 10340.5. 

The coefficient for alive_pre is  -.1. What does this mean? It is the slope for only females. So for every extra day a feamle candidate is alive before an election, their lifespan after the election will be 0.1 days lower, on average. Now take a look below at the coefficient of sexMale:alive_pre, which is -.8, It is the slope for only males. So for every extra day a male candidate is alive before an election, their lifespan after the election will be 0.8 days lower, on average.




Now let's take a look at some posterior graphics:



<!-- b0 and b0+1 on same plot  -->

```{r}

```

 Talk about how you can see from this that see  men live longer etc. 
 

<!-- b2, and b2+b3 - put on same plot  -->

```{r}

```

Talk about avg. differnece for women then men, slope for men is steeper etc. 
 
 




Let's now go crazy. Let's create a model. 

```{r, cache = TRUE}
fit_gov_5 <- stan_glm(data = ch9_gov,
                      formula = alive_post ~ state + sex*alive_pre,
                      refresh = 0,
                      iter = 10000)
```


$$ y_i =  \beta_0 +  \beta_1 x_{AK,i} + \beta_1 x_{AR,i} + ... \beta_{49} x_{WY,i} + \beta_{50} male_i + \beta_{51} alive\_pre_i+ \beta_{52} male_i *  alive\_pre_i + \epsilon_i$$



Single plot that shows int + 1 state thatws very negative and int + 1 state thats neutral and int + 1 state thats very positive 
```{r}

```


Notes:

* takes awhile to run. Got 55 parameters.

* Shrink parameter values.

* Pick two unique posterior parameters to interpret.


```{r}
print(fit_gov_5, detail = FALSE)
```

<!-- DK: Need to modify this to only show one of the more interesting state coefficients. (Although we might show all the state pdfs in the graphic.) This a good place to discuss shrinkage, which is what is happening here. And that is probably enough models. -->

## Temperance

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Temperance"}
knitr::include_graphics("other/images/Temperance.jpg")
```





## EDA of `shaming`

<!-- Use print(fit_obj, digits = 4) if you need to see more digits. -->


<!-- Models to use-->


<!-- primary_06 ~ sex - 1 -->
<!-- primary_06 ~ age -->
<!-- primary_06 ~ treatment - 1 (plot all five posteriors together) -->
<!-- primary_06 ~ sex + solo + treatment -->
<!-- primary_06 ~ sex + solo + treatment + solo * treatment -->



Imagine you are running for Governor and want to do a better job of getting your voters to vote. You recently read about a large-scale experiment showing the effect of sending out a voting reminder that "shames" citizens who do not vote. You are considering sending out a "shaming" voting reminder yourself. What will happen if you do? Will more voters show up to the polls? Additionally, on the day of the election a female citizen is randomly selected. What is the probability she will vote?    

Consider a new data set, `shaming`, corresponding to an experiment carried out by Gerber, Green, and Larimer (2008) titled "Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment". This experiment used several hundred thousand registered voters and a series of mailings to determine the effect of social pressure on voter turnout. 

Let's now do another EDA, starting off by running `glimpse()`.

```{r}
glimpse(shaming)
```

Here we see that `glimpse()` gives us a look at the raw data contained within the `shaming` data set. At the very top of the output, we can see the number of rows and columns, or observations and variables respectively. We see that there are 344,084 observations, with each row corresponding to a unique respondent. The "Columns: 10" tells us that there are 10 variables within this data set. Below this, we see a cutoff version of the entire data set that has the variables on the left as rows and the observations as a list separated by commas, as compared to the tibble output that presents with the variables as columns and the observations as rows running horizontally.

<!-- It may be worth mentioning that general_04 is always "yes", unlike all the other voting history variables which have values of "yes" and "no". Why is that? I *think* that this is caused by their sampling plan. They found all the people who voted in the 2004 general election. Then, the authors found their history. As we would expect, those people had sometimes voted in the past and sometime not. Then, the authors sent the mailing. The key dependent variable, primary_06, is coding 0/1, since that makes doing the statistics easier. -->

From this summary, we get an idea about some of the variables we will be dealing with. Some variables that will be of interest to us are `sex`, `hh_size`, and `primary_06`. The variable `hh_size` tells us the size of the respondent's household, and `primary_06` tells us whether or not the respondent voted in the 2006 Primary election. 

There are a few things to note while exploring this data set. You may -- or may not -- have noticed that the only response to the `general_04` variable is "Yes". In their published article, the authors note that "Only registered voters who voted in November 2004 were selected for our sample" (Gerber, Green, Larimer, 2008). After this, the authors found their history then sent out the mailings.

It is also important to identify the dependent variable and its meaning. In this shaming experiment, the dependent variable is `primary_06`, which is a variable coded either 0 or 1 for whether or not the respondent voted in the 2006 primary election. This is the dependent variable because the authors are trying to measure the effect that the treatments have on the proportion of people who vote in the 2006 general election.

<!-- HV: Should I include discussion of the left-hand variable (treatment?) here? Or wait until we move into the regressions? -->

The voting results from other years, such as 2002 and 2004, are of less interest to us and can be removed from the abbreviated data set. In addition to removing `general_04`, `primary_02`, `general_02`, or `primary_04`, we also will not be taking particular interest in `birth_year`, or `no_of_names` within this chapter.
<!-- HV: should I explain why we are not using any of these variables? why they are not of great use to us? -->

By narrowing down the set of variables we are looking at and investigating, we will find more meaningful relationships among them. However, we have not yet discussed the most important variable of them all: `treatment`. The `treatment` variable is a factor variable with 5 levels, including the control. Since we are curious as to how sending mailings affects voter turnout, the treatment variable will tell us about the impact each type of mailing can make. Let's start off by taking a broad look at the different treatments.
<!-- HV: Is it okay to say the first sentence of this paragraph? -->

```{r}
shaming %>%
  count(treatment)
```

Four types of treatments were used in the experiment, with voters receiving one type of mailing. All of the mailing treatments carried the message, "DO YOUR CIVIC DUTY - VOTE!". 

The first treatment, Civic Duty, also read, “Remember your rights and responsibilities as a citizen. Remember to vote." This message acted as a baseline for the other treatments, since it carried a message very similar to the one displayed on all the mailings.

In the second treatment, Hawthorne, households received a mailing which told the voters that they were being studied and their voting behavior would be examined through public records. This adds a small amount of social pressure to the households receiving this mailing.

In the third treatment, Self, the mailing includes the recent voting record of each member of the household, placing the word "Voted" next to their name if they did in fact vote in the 2004 election or a blank space next to the name if they did not. In this mailing, the households were also told, “we intend to mail an updated chart" with the voting record of the household members after the 2006 primary. By emphasizing the public nature of voting records, this type of mailing exerts more social pressure on voting than the Hawthorne treatment.

The fourth treatment, Neighbors, provides the household members' voting records, as well as the voting records of those who live nearby. This mailing also told recipients, "we intend to mail an updated chart" of who voted in the 2006 election.

For now, let's focus on a subset of the data.

```{r}
ch9 <- shaming %>% 
  mutate(solo = ifelse(hh_size == 1, TRUE, FALSE)) %>% 
  mutate(age = 2006 - birth_year) %>% 
  select(primary_06, treatment, solo, sex, age)
```

We create the variable `solo`, which is TRUE for voters who live alone and FALSE for those that do not. We are curious to see if the treatment effect, if any, is the same for voters who live alone as it is for those who do not.

```{r}
ch9 %>% 
  skim()
```


<!-- DK: Add discussion of what you see here. No need to drop missing values since there aren't any. I think this next discussion can be dropped. -->


While some summarizing commands such as `glimpse()` gives us a good look at the possible values for the variables, it is difficult to read it in terms of individual observations. Recall that the *observational unit* is what is being measured. With the `shaming` data set, the observational unit would be the voter respondent. To get a better sense of some respondents' information, let's use `sample_n()` to gather a random sample of *n* observations from the data set.
<!-- HV: Does this belong here? -->

```{r}
ch9 %>% 
  sample_n(10)
```

Now we have a table with 5 random observations and the respondents' information in a regular table output. By taking a few random samples, we may start to see some patterns within the data. 

Now we have a table with 5 random observations and the respondents' information in a regular table output. By taking a few random samples, we may start to see some patterns within the data. Do you notice anything in particular about the variable `treatment`?

One other helpful summarizing technique we can use is `skim()`. To make the information it contains simpler, we will only be looking at three variables: `primary_06`, `treatment`, and `sex`. 

```{r}
shaming %>% 
  select(primary_06, treatment, sex) %>% 
  skim()
```

By running the `skim()` command, we get a summary of the data set as a whole, as well as the types of variables and individual variable summaries. At the top we see the number of columns and rows within the selected data set. Below this we are given a list with the different types of variables, or columns, and how often they appear within the data we are skimming. Following this, the variables are then separated by their column type, and we are given individual summaries based on the type. 

<!-- 2) `primary_06` as a function of `treatment` and `solo` and of their interaction. We will build up this model step-by-step, very similar to how we explored the effect of treatment in chapter 8. But we go deeper because  we are learning about interactions. Key thing is to go through all the themes.Rmd issues, at least until prediction. Note that this situation is different from Chapter 8 in that fitted values and predicted values are not the same thing! The fitted value, for a combination of values for treatment and solo, is something 0.30, meaning that 30% of the people in this bucket votes. But the predicted value must be 0 or 1. Either you voted or you didn't. This example is clearly causal and so you need a Rubin Table with 4 potential outcome columns. The key difference in this chapter is that we are using lots of right hand side variables, both continuous and discrete. -->

<!-- 3) EDA of `nes`. This can be fairly quick. Again, we won't use all the variables. Only discuss the ones we do use. One of the variables should be year. We want to show off Gelman's trick by plotting things by year. -->

<!-- 4) We are making a predictive model of what as a function of what other stuff? Want to use a continuous variable and some discrete variables as well. Interactions to.  -->

<!-- 5) I don't think we bother with a scenario which requires a bootstrap. Or should we?  -->

<!-- 6) Prediction. How do we use our model to make predictions? Bring the discussion back to the way that we opened the chapter with a problem. -->



Having created models with one parameter in Chapter \@ref(one-parameter) and two parameters in Chapter \@ref(two-parameters), you are now ready to make the jump to $N$ parameters.  The more parameters we include in our models, the more flexible they can become. But we must be careful of *overfitting*, of making models which are inaccurate because they don't have enough data to accurately estimate those parameters. The tension between overfitting and underfitting is central to the practice of data science.


<!-- DK: Note that this situation is different from Chapter 8 in that fitted values and predicted values are not the same thing! The fitted value, for a combination of values for treatment and solo, is something 0.30, meaning that 30% of the people in this bucket votes. But the predicted value must be 0 or 1. Either you voted or you didn't. This example is clearly causal and so you need a Rubin Table with 5 potential outcome columns. The key difference in this chapter is that we are using lots of right hand side variables, both continuous and discrete. -->

### Wisdom

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Wisdom"}
knitr::include_graphics("other/images/Wisdom.jpg")
```


### Justice

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Justice"}
knitr::include_graphics("other/images/Justice.jpg")
```


### Courage

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Courage"}
knitr::include_graphics("other/images/Courage.jpg")
```


### Temperance

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Temperance"}
knitr::include_graphics("other/images/Temperance.jpg")
```


## Causal Effects of `treament`

We will now be looking into the causal effect of the treatment variable on the 2006 primary election. To start, let's build a model using `stan_glm()` followed by a regression table. 

```{r, cache=TRUE}
# obj.1 <- stan_glm(data = shaming, 
#                 formula = primary_06 ~ treatment - 1, 
#                 family = gaussian(), 
#                 refresh = 0)
```


This table shows us each of the five treatments and their beta coefficients, along with a 95% Confidence Interval for these coefficients. The Control provides us with a baseline.



<!-- Talk about what these results mean.  Then, create the same model but with a different structure. -->


```{r, cache=TRUE}
# obj.2 <- stan_glm(data = shaming, 
#                 formula = primary_06 ~ treatment, 
#                 family = gaussian(), 
#                 refresh = 0)
```


<!-- Explain how these two models are the same, except in how they define the parameters. Show us the math like Gelman does. Write down the math. For simple. -->



<!-- Once we talk about these things --- and, again, this is exactly what we have talked about in chapter 8 --- we can do a bit more. Like discuss how we are using 99%, because there is nothing magical aboyt 95%, other than convention. I also think it would be fun to show a nice graphic of this, highlighting how the estimates for Civic and Hawthorne overlap.  -->


### Interactions

<!-- This is new. With only two parameters, we can't really look at interaction effects. Need to discuss interaction effects in general. Also, note that heterogenous treatment effects are the same thing as interaction effects that involve a treatment effect as one of the variables.  -->

<!-- Feel free to build up this code, and other examples, more slowly than I am doing it here. -->

```{r, cache=TRUE}
# obj.3 <- stan_glm(data = shaming, 
#                 formula = primary_06 ~ sex + treatment + sex:treatment, 
#                 family = gaussian(), 
#                 refresh = 0)
```


<!-- Takes a while to explain what all this means. -->

<!-- Two key issues: 1) Interpreting lots of parameters in a model. interactions, heterogenous treatment effects. shaming using lm().  -->

<!-- Treatment effect is not the same thing as a coefficient. -->

<!-- 
intercept

Interactions --- use: income ~ party*something

heterogeneous treatment effects --- use:  att_start ~ treatment*something 
just a fancy way of saying interaction effects, but with a variable which us causal


What problems do we face? All the things that make modeling difficult. Why is this so hard? -->

<!-- Centering. -->

<!-- Might naively just take the value for each bucket. But that overfits! Need to put down some structure, like ordering. -->

<!-- income category, party id, pooling, age, -->

<!-- overfitting/underfitting bias/variance -->

<!-- We must have left bootstrapping behind by now. No more bootstraps, at least for the purpose of calculating uncertainty. (We will use it later for the purpose of out-of-sample testing and avoiding overfitting.) Key lesson is that overfitting is easy. You can't just estimate a value for each cell. You need to smooth and borrow strength. Of course, the only way to do that is with a Bayesian approach. Right? We don't want to get into hacked likelihood approaches. -->

<!-- cces looks perfect for this project. There are 400,000 rows, so it seems like you ought to have plenty of data for anything you want, but once you realize there are 51 states and 10 years, things get sparse fast. We only have 15 observations, for example, for Wyoming in 2007. Once you start subsetting by race and education, you have no choice but to start borrowing strength.  -->

<!-- So, just what will we use? rstanarm(). If so (and if we have not introduced it earlier), we can begin with seeing how it is similar to lm() and then expand. This means that, in one paramter chapter, we should be doing lm( ~ 1). In two parameter, lm( ~ treatment) --- if treatment is zero one --- or, perhaps better, lm( ~ -1 + treatment) if treatment is a factor/character with two levels. We might also have introduced  -->

<!-- MF: Is basic structure for this chapter meant to be [1] life span section, [2] shame section, and [3] interactions? -->

