---
output_yaml:
  - _output.yml
---

# Continuous Response {#continuous-response}

<!-- DK: Error message from running nn fit_resamples. -->

<!-- DK: Should, in each section, "discover" some weirdness in the plot, go back and add a step_function, and discover that the weirdness is "fixed" in the plot. -->

<!-- DK: Should we add more detail in terms of how we work with the fit_samples object? Probably not. Too complex for this class. If anything, we need more time discussing how we should investigate what step_* functions to add and why. -->

<!-- DK: Need to use our chosen model to answer a question of some sort. Let's compare group X with group Y. How do they differ in predicted ideology?  If we have enough members of group X and group Y, we can just compare their real ideology! But, often we won't have enough. So, we need to use the model to make the comparison. -->

<!-- DK: What about code like this for comparing a bunch of models all at once? -->

<!-- ```{r} -->
<!-- ch11_formulas <- tibble(formula = c(age_form, -->
<!--                                    race_gender_form, -->
<!--                                    full_form, -->
<!--                                    interact_form), -->
<!--                        group = c("Age only model", -->
<!--                                  "Race and gender model", -->
<!--                                  "Full model without interaction", -->
<!--                                  "Interaction model")) -->
<!-- ``` -->

<!-- Now, we can use `map_*` to apply all of these models and view their metrics to see which ones have the lowest rmse values. -->

<!-- ```{r cache=TRUE, message=FALSE} -->
<!-- set.seed(10) -->
<!-- folds_metrics <- ch11_formulas %>% -->
<!--   mutate(metrics = map(formula, ~ fit_resamples(object = lm_model, -->
<!--                                                 preprocessor = ., -->
<!--                                                 resamples = ch11_folds) %>% -->
<!--                          collect_metrics())) -->
<!-- ``` -->

Chapter \@ref(model-choice) showed us the *tidymodels* framework for model building and testing. In this chapter, we will use it to explore models in which the outcome variable is *continuous*. In Chapter \@ref(discrete-response), we learn about outcome variables which are *discrete*, meaning that they are members of distinct categories like TRUE/FALSE or yes/no. 

We use `nes` from the `PPBDS.data` package. `nes` contains data from the American National Election Survey for every presidential election year since 1952. Along with demographic details, such as race, gender, and age, the survey also includes respondents' ideological identification. Because `ideology` is measured on a scale from -3 to 3, we can treat it as a continuous outcome variable.


## Exploratory Data Analysis

Packages:

```{r message=FALSE}
library(PPBDS.data)
library(skimr)
library(tidyverse)
library(tidymodels)
library(rstanarm)
```

Explore `nes`:


```{r}
glimpse(nes)
```

Variables available in `nes`. Note that the full NES has many more variables available.

<!-- DK: Is voted in previous election or in this one? -->

* `year`: the year the study was conducted.
* `state`: abbreviation for state of residence.
* `gender`: identifies respondents with values "Male" and "Female".
* `race`: race/ethnicity respondent identification.
* `income`: 5 income groups described with percentile range: 0-16, 17-33, 34 to 67, 68-95, and 96-100. 
* `age`: ranges for respondents' age.
* `education`: seven categories of educational achievement.
* `pres_appr`: respondents' self-reported approval of the sitting president.
* `voted`: whether the respondent had voted in the presidential election.
* `ideology` a continuous variable with -3 corresponding to strongly Democrat and 3 corresponding to strongly Republican.
* `region`: US region: Northeast, Midwest, West, and South. 

Given knowledge of these variables for a new person, our goal is to predict their ideology correctly. 

```{r}
skim(nes)
```

There are three data types: factors, characters, and numeric. `income`, `age`, and `education` are ordered factors. This means that each factor level has a relationship with the others. In the `age` variable, for example, `25 - 34` is bigger `17 - 24` and smaller than `35 - 44`. But there is no sense of *how much* bigger or smaller it is. Ordered factors can have weird effects in certain models, so we need to be wary.

`nes` covers 1952 through 2016, but we will narrow our scope to just 2016 because of the changing nature of what makes someone ideologically liberal or ideologically conservative over time. Keep in mind that, when feeding in new data to our model, it is most accurately applied to voters in or around 2016. 

Note that all of the variables are incomplete, meaning they contain `NA` values. While there are methods to impute missing data, we will simply remove these values for now.


```{r message=FALSE}
ch11 <- nes %>%
  select(year, gender, race, income, age, education, ideology, region) %>%
  filter(year == 2016) %>% 
  drop_na()
```

<!-- DK: What should I do with ordered factors? Change them here? Change them in nes itself? If I leave them, will our various models be able to do something with them? Perhaps I will need to use a step_* to fix? Or maybe step_dummy() takes care of that? -->

The first step in a proper data science project is to split our data set into training and testing samples, and then to create cross-validations from the training data.

```{r}
set.seed(10)
ch11_split <- initial_split(ch11, prob = 0.80)
ch11_train <- training(ch11_split)
ch11_test  <- testing(ch11_split)
ch11_folds <- vfold_cv(ch11_train, v = 10)
```


We can now explore three different models for this problem: a traditional linear model, a Bayesian linear model, and a neural network.

<!-- DK: Could give a brief review of each model at the start, perhaps even fit it on the whole data set and discuss what you see. Or maybe do that all at once, in this section here. -->

## Linear model

Create the workflow object with the model engine.

```{r}
lm_wfl <- workflow() %>% 
  add_model(linear_reg() %>%
            set_engine("lm") %>%
            set_mode("regression"))
```

The ***parsnip*** package allows us to create an engine that can run this linear regression model easily and repeatedly. `linear_reg()` tells the engine that this is a linear regression. `set_engine("lm")` tells the engine to use the `lm()` function. `set_mode()` has two options: "regression" and "classification." Since our left-hand variable is continuous, we will set it to "regression." If it were categorical, we would set the mode to "classification". The default is "regression," so the `set_mode()` command has no effect in this case.


Add a recipe.

```{r}
lm_wfl <- workflow() %>% 
  add_model(linear_reg() %>%
            set_engine("lm") %>%
            set_mode("regression")) %>% 
  add_recipe(recipe(ideology ~ gender + race + region + 
                      income + age + education,
                    data = ch11_train)) 
  
```

<!-- DK: Why don't I need step_dummy(all_nominal())? Especially since age is an ordered factor. How does lm magically handle this? -->

The required parts of the recipe are the model and a data set. The data set does not really matter that much since we are not really using it for anything at this stage of the process. Its purpose is to ensure that tidymodels can process the recipe. To do so, it needs to know if, for example, `gender` is character or factor or numeric. You would get the same result whether you used `ch_11`, `ch_11_train` or `ch_11_test` as the value for `data`. But, it is a good habit to never use the test data until the very end, even if doing so is harmless.

The formula portion of the recipe will, depending in the model, look a lot like the formulas which you have passed to `stan_glm()`. But, in tidymodels, we are not allowed to do any mathematical operations in the receipt itself, beyond addition. Any such operations need to be placed in `step_*` functions. Recall how, in Chapter \@ref(model-choice), we need to use `step_interact()` to add an interaction term. Figuring out which `step_*` functions we want to use and why is the hardest part of tidymodels.


Examine performance on the cross-validation samples.

```{r}
lm_metrics <- lm_wfl %>% 
  fit_resamples(resamples = ch11_folds) %>% 
  collect_metrics()

lm_metrics
```

Check the predictions against the actual values for the training data.

```{r}
lm_wfl %>% 
  fit(data = ch11_train) %>% 
  predict(new_data = ch11_train) %>% 
  bind_cols(ch11_train %>% select(ideology)) %>% 
  ggplot(aes(y = ideology, x = `.pred`)) +
    geom_jitter(height = 0.2, alpha = 0.1) +
    labs(title = "Predicting Ideology",
         subtitle = "Using a linear model with demographic predictors",
         x = "Predicted Ideology",
         y = "Ideology",
         caption = "Source: 2016 CCES") 
```

Is this result good or bad? Depends on your point of view! If we predict that someone's ideology is 0, then we don't know that much about their ideology. It could be an anywhere from -3 (strong Democrat) to +3 (strong Republican). 

<!-- DK: We need more instruction. But do we need it here or in an appendix? We should look at this plot, change the workflow, look at the plot again, and so on. -->

## Bayesian linear model

Create the workflow object with the model engine. Note that every part is the same as the linear model we just completed *except* that the required engine is "stan". 

```{r}
stan_wfl <- workflow() %>% 
  add_model(linear_reg() %>%
            set_engine("stan") %>%
            set_mode("regression"))
```

Add a recipe.

```{r}
stan_wfl <- workflow() %>% 
  add_model(linear_reg() %>%
            set_engine("stan") %>%
            set_mode("regression")) %>% 
  add_recipe(recipe(ideology ~ gender + race + region + 
                      income + age + education,
                    data = ch11_train)) 
```

Examine performance on the cross-validation samples.

<!-- DK: Unlike with lm() above, we get different answers depending on whether or not we use step_dummy(all_nominal()). But the differences are very small (4th decimal, at least in the collected metrics), so let's ignore it for now. -->

```{r}
stan_metrics <- stan_wfl %>% 
  fit_resamples(resamples = ch11_folds) %>% 
  collect_metrics()

stan_metrics
```

Check the predictions against the actual values.

```{r}
stan_wfl %>% 
  fit(data = ch11_train) %>% 
  predict(new_data = ch11_train) %>% 
  bind_cols(ch11_train %>% select(ideology)) %>% 
  ggplot(aes(y = ideology, x = `.pred`)) +
    geom_jitter(height = 0.2, alpha = 0.1) +
    labs(title = "Predicting Ideology",
         subtitle = "Using a Bayesian linear model with demographic predictors",
         x = "Predicted Ideology",
         y = "Ideology",
         caption = "Source: 2016 CCES")
```

<!-- DK: Add more discussion. Perhaps discuss the posterior probability distribution for true ideology given predicted ideology. Note that the model never predicts ideology 2 or 3. Is that good or bad? -->

## Random forest

Random forests are a powerful "non-parametric" approach to forecasting. Recall that, in both lm() and stan_glm() models, there are specified parameters which we are trying to estimate. We "care" about these parameters because they often correspond to real world entities, like the average treatment effect. 

With non-parametric models, there are no parameters which we care about. (That is a loose and not-quite-correct definition.) Instead, the model is a "black box" into which predictors are fed and from which predicted outcomes emerge. 

Create the workflow object with the model engine.

```{r}
rf_wfl <- workflow() %>% 
  add_model(rand_forest() %>%
            set_engine("ranger") %>% 
            set_mode("regression"))
```

It is possible to pass the model directly as an argument to the model creation function. So, the above would often be written as:

```{r}
rf_wfl <- workflow() %>% 
  add_model(rand_forest(mode = "regression") %>%
            set_engine("ranger"))
```

The objects are identical. Either way, the next step is to add a recipe.

```{r}
rf_wfl <- workflow() %>% 
  add_model(rand_forest('regression') %>%
            set_engine("ranger")) %>% 
  add_recipe(recipe(ideology ~ gender + race + region + 
                      income + age + education,
                    data = ch11_train)) 
```

Note that there are improvements we could make to this model. As a rule of thumb, whenever using non-parametric models, you should use `step_normalize(all_numeric())` which, as you might guess, normalizes all numeric variables. We will leave that out here.

Examine performance on the cross-validation samples.


```{r}
rf_metrics <- rf_wfl %>% 
  fit_resamples(resamples = ch11_folds) %>% 
  collect_metrics()

rf_metrics 
```

Check the predictions against the actual values.

```{r}
rf_wfl %>% 
  fit(data = ch11_train) %>% 
  predict(new_data = ch11_train) %>% 
  bind_cols(ch11_train %>% select(ideology)) %>% 
  ggplot(aes(y = ideology, x = `.pred`)) +
    geom_jitter(width = 0.2, alpha = 0.1) +
    labs(title = "Predicting Ideology",
         subtitle = "Using a random forest with demographic predictors",
         x = "Predicted Ideology",
         y = "Ideology",
         caption = "Source: 2016 CCES")
```

<!-- DK: The last two graphs are different. Discuss! See how well this model does with low predictions. Every time predicted ideology is below -2.5, the true ideology is -3. The linear models are much more likely to make mistakes when predicting very negative ideology values. -->


## Model comparison

The most common method for deciding which model to choose is to look at which one does the best predicting out of sample. As a reminder, here is a summary of how well our three different models performed.

```{r}
tibble(model = rep(c("Linear Model", 
                     "Bayesian Linear Model", 
                     "Random Forest"), each = 2)) %>% 
  bind_cols(bind_rows(lm_metrics, stan_metrics, rf_metrics)) 


```
The results for the random forest model are worse than those for either linear model. The results for the two linear models are so similar that there is no particular reason, in terms of accuracy, to prefer one model over the others. 


## Cardinal virtues

```{r echo=FALSE, fig.margin=TRUE}
knitr::include_graphics("other/images/Wisdom.jpg")
```


```{r echo=FALSE, fig.margin=TRUE}
knitr::include_graphics("other/images/Justice.jpg")
```

The lessons of the Cardinal Virtues continue to apply.

*Wisdom* asks us if the problem we are trying to solve is "close enough" to the data we have that we might fairly consider both to come from the same population. Using US data from 2016 to create a model of ideology for the US in 2020 is reasonable. Using US data from 2016 to create a model of ideology for Brazil in 1966 is less so. 

*Justice* instructs us to distinguish causal from predictive models. This mode is clearly predictive. None of our demographic covariates were randomly assigned. It is possible to manipulate a variable like income, and therefore to consider (at least) two potential outcomes for a given person: ideology if rich and ideology if poor. But, although a causal model for the effect of income on ideology is at least conceivable, it is highly unlikely that such a model would provide a good estimate of the causal effect because income was not assigned randomly. Without random assignment, estimate causal effects is very, very difficult. *Regressions and Other Stories* by Andrew Gelman,  Jennifer Hill, and Aki Vehtari is a good introduction.

 
Recall that all we care about in a predictive model is forecasting some value $y_i$ given that we know $x_{i_1}, x_{i_2}, ... x_{i_n}$. The $y_i$ in our case is `ideology`. The $x_{i_1}, x_{i_2}, ... x_{i_n}$ are certain known variables, such as `state`, `age`, and `income`, among others. $\beta$ stands the list of unknown parameters which must be estimated. 

The following equation calculates the `ideology` of the *i*th respondent, $y_i$, as a function of the data and the unknown parameters. 

$$y_i = f(x_{i_1}, x_{i_2}, ..., \beta)$$

Keep in mind that our goal is to create the best possible model to predict someone's ideology given a number of demographic variables. That is to say, we plan on using our model on out-of-sample data. Cross validation is the method we will use to forecast how well the model will work on this unseen data.

```{r echo=FALSE, fig.margin=TRUE}
knitr::include_graphics("other/images/Courage.jpg")
```

*Courage* helps us to fit the models, as we have above, and then to select the one we want to use. We should only favor more complex models (or formulas) if the additional complexity is *warranted*. This is a philosophical principle known as "Occam's Razor." It states that, "all other things being equal, simpler solutions are more likely to be correct than complex ones." When viewed in a modeling framework, Occam's Razor can be restated as, "all other things being equal, simpler models are to be preferred over complex ones." In other words, we should only favor the more complex model/formula if the additional complexity makes the model meaningfully better.

In general, linear models are much simpler than Bayesian models. So, given that these two models did about as well under cross-validation, we should select the basic linear model.


<!-- Time to make up some imaginary people. Let's say we have four individuals whose ideology in 2016 we wanted to predict. We can create a tibble with the values of their demographic information, like so: -->

<!-- Just because you have a variable in this training data today, does not mean you are going to get it in your production data tomorrow. -->

<!-- DK: Do these have to be factors? Or will characters work as well, like they do with posterior_predict? -->


<!-- ```{r} -->
<!-- new_people <- tibble("name" = c("Alice", "Betty", "Chelsea", "Danielle"), -->
<!--                      "region" = as.factor(c("Midwest", "Northeast", "South", "West")), -->
<!--                      "gender" = as.factor(c("Female", "Female", "Female", "Female")), -->
<!--                      "income" = as.factor(c("34 - 67", "34 - 67", "34 - 67", "34 - 67")), -->
<!--                      "age" = c("17 - 24", "17 - 24", "17 - 24", "17 - 24"), -->
<!--                      "education" = c("College", "College", "College", "College"), -->
<!--                      "race" = c("White", "White", "White", "White")) -->
<!-- ``` -->

<!-- Now, let's predict each new person's ideology using the linear regression model we just created. -->

<!-- ```{r message=FALSE} -->
<!-- lm_model %>% -->
<!--   fit(full_form, data = ch11_train) %>% -->
<!--   predict(new_data = new_people) %>% -->
<!--   bind_cols(new_people) %>% -->
<!--   rename("ideology" = ".pred") -->
<!-- ``` -->


```{r echo=FALSE, fig.margin=TRUE}
knitr::include_graphics("other/images/Temperance.jpg")
```

*Temperance* reminds us to be suspicious of all models, especially our own. The world is always changing. Our models will rarely work as well in the future as they have using data from the past.

## Summary

The purpose of this chapter way to practice using tidymodels tools for model selection. The most important measure of model quality is how well the model does on out-of-sample data. Cross validation is only one of many ways to estimate that performance, but it is probably the most popular. For details on other approaches, read [Chapter 10](https://www.tmwr.org/resampling.html) of *Tidy Modeling with R* by Max Kuhn and Julia Silge.
