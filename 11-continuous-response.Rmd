---
output_yaml:
  - _output.yml
---

# Continuous Response {#continuous-response}

<!-- DK: Should, in each section, "discover" some weirdness in the plot, go back and add a step_function, and discover that the weirdness is "fixed" in the plot. -->

<!-- DK: Should we add more detail in terms of how we work with the fit_samples object? Probably not. Too complex for this class. If anything, we need more time discussing how we should investigate what step_* functions to add and why. -->

<!-- DK: Need to use our chosen model to answer a question of some sort. Let's compare group X with group Y. How do they differ in predicted ideology?  If we have enough members of group X and group Y, we can just compare their real ideology! But, often we won't have enough. So, we need to use the model to make the comparison. -->

<!-- DK: What about code like this for comparing a bunch of models all at once? -->

<!-- ```{r} -->
<!-- ch11_formulas <- tibble(formula = c(age_form, -->
<!--                                    race_gender_form, -->
<!--                                    full_form, -->
<!--                                    interact_form), -->
<!--                        group = c("Age only model", -->
<!--                                  "Race and gender model", -->
<!--                                  "Full model without interaction", -->
<!--                                  "Interaction model")) -->
<!-- ``` -->

<!-- Now, we can use `map_*` to apply all of these models and view their metrics to see which ones have the lowest rmse values. -->

<!-- ```{r cache=TRUE, message=FALSE} -->
<!-- set.seed(10) -->
<!-- folds_metrics <- ch11_formulas %>% -->
<!--   mutate(metrics = map(formula, ~ fit_resamples(object = lm_model, -->
<!--                                                 preprocessor = ., -->
<!--                                                 resamples = ch11_folds) %>% -->
<!--                          collect_metrics())) -->
<!-- ``` -->

Chapter \@ref(model-choice) showed us the *tidymodels* framework for model building and testing. In this chapter, we will use it to explore models in which the outcome variable is *continuous*. In Chapter \@ref(discrete-response), we learn about outcome variables which are *discrete*, meaning that they are members of distinct categories like TRUE/FALSE or yes/no. 

We use `nes` from the `PPBDS.data` package. `nes` contains data from the American National Election Survey for every presidential election year since 1952. Along with demographic details, such as race, gender, and age, the survey also includes respondents' ideological identification. Because `ideology` is measured on a scale from -3 to 3, we can treat it as a continuous outcome variable.

Our question:

*How has ideology varied with age in the United States?*

To answer this question, we need to build a model.

## Exploratory Data Analysis

Packages:

```{r message=FALSE}
library(PPBDS.data)
library(skimr)
library(tidyverse)
library(tidymodels)
library(rstanarm)
```

Explore `nes`:


```{r}
glimpse(nes)
```

Variables available in `nes`. Note that the full NES has many more variables available.

<!-- DK: Is voted in previous election or in this one? -->

* `year`: the year the study was conducted.
* `state`: abbreviation for state of residence.
* `gender`: identifies respondents with values "Male" and "Female".
* `race`: race/ethnicity respondent identification.
* `income`: 5 income groups described with percentile range: 0-16, 17-33, 34 to 67, 68-95, and 96-100. 
* `age`: ranges for respondents' age.
* `education`: seven categories of educational achievement.
* `pres_appr`: respondents' self-reported approval of the sitting president.
* `voted`: whether the respondent had voted in the presidential election.
* `ideology` a continuous variable with -3 corresponding to strongly Democrat and 3 corresponding to strongly Republican.
* `region`: US region: Northeast, Midwest, West, and South. 

Given knowledge of these variables for a new person, our goal is to predict their ideology correctly. 

```{r}
skim(nes)
```

There are three data types: factors, characters, and numeric. `income`, `age`, and `education` are ordered factors. This means that each factor level has a relationship with the others. In the `age` variable, for example, `25 - 34` is bigger `17 - 24` and smaller than `35 - 44`. But there is no sense of *how much* bigger or smaller it is. Ordered factors can have weird effects in certain models, so we need to be wary.

Note that all of the variables are incomplete, meaning they contain `NA` values. While there are methods to impute missing data, we will simply remove these values for now.


```{r message=FALSE}
ch11 <- nes %>%
  select(year, gender, race, income, age, education, ideology, region) %>%
  drop_na()
```

<!-- DK: What should I do with ordered factors? Change them here? Change them in nes itself? If I leave them, will our various models be able to do something with them? Perhaps I will need to use a step_* to fix? Or maybe step_dummy() takes care of that? -->

The first step in a proper data science project is to split our data set into training and testing samples, and then to create cross-validations from the training data.

```{r}
set.seed(10)
ch11_split <- initial_split(ch11, prob = 0.80)
ch11_train <- training(ch11_split)
ch11_test  <- testing(ch11_split)
ch11_folds <- vfold_cv(ch11_train, v = 10)
```


We can now explore three different models for this problem: a traditional linear model, a Bayesian linear model, and a neural network.

<!-- DK: Could give a brief review of each model at the start, perhaps even fit it on the whole data set and discuss what you see. Or maybe do that all at once, in this section here. -->

## Linear model

Create the workflow object with the model engine.

```{r}
lm_wfl <- workflow() %>% 
  add_model(linear_reg() %>%
            set_engine("lm") %>%
            set_mode("regression"))
```

The ***parsnip*** package allows us to create an engine that can run this linear regression model easily and repeatedly. `linear_reg()` tells the engine that this is a linear regression. `set_engine("lm")` tells the engine to use the `lm()` function. `set_mode()` has two options: "regression" and "classification." Since our left-hand variable is continuous, we will set it to "regression." If it were categorical, we would set the mode to "classification". The default is "regression," so the `set_mode()` command has no effect in this case.


Add a recipe.

```{r}
lm_wfl <- workflow() %>% 
  add_model(linear_reg() %>%
            set_engine("lm") %>%
            set_mode("regression")) %>% 
  add_recipe(recipe(ideology ~  gender + race + region + 
                      income + age + education,
                    data = ch11_train) %>% 
             step_dummy(all_nominal())
             ) 
  
```

<!-- DK: Why don't I need step_dummy(all_nominal())? Especially since age is an ordered factor. How does lm magically handle this? -->

The required parts of the recipe are the model and a data set. The data set does not really matter that much since we are not really using it for anything at this stage of the process. Its purpose is to ensure that tidymodels can process the recipe. To do so, it needs to know if, for example, `gender` is character or factor or numeric. You would get the same result whether you used `ch_11`, `ch_11_train` or `ch_11_test` as the value for `data`. But, it is a good habit to never use the test data until the very end, even if doing so is harmless.

The formula portion of the recipe will, depending in the model, look a lot like the formulas which you have passed to `stan_glm()`. But, in tidymodels, we are not allowed to do any mathematical operations in the recipe itself, beyond addition. Any such operations need to be placed in `step_*` functions. Recall how, in Chapter \@ref(model-choice), we used `step_interact()` to add an interaction term. Figuring out which `step_*` functions we want to use and why is the hardest part of tidymodels.


Examine performance on the cross-validation samples.

```{r}
lm_metrics <- lm_wfl %>% 
  fit_resamples(resamples = ch11_folds) %>% 
  collect_metrics()

lm_metrics
```

Check the predictions against the actual values for the training data.

```{r}
lm_wfl %>% 
  fit(data = ch11_train) %>% 
  predict(new_data = ch11_train) %>% 
  bind_cols(ch11_train %>% select(ideology)) %>% 
  ggplot(aes(y = ideology, x = .pred)) +
    geom_jitter(height = 0.2, alpha = 0.01) +
    labs(title = "Predicting Ideology",
         subtitle = "Using a linear model with demographic predictors",
         x = "Predicted Ideology",
         y = "Ideology",
         caption = "Source: CCES") 
```

Is this result good or bad? Depends on your point of view! If we predict that someone's ideology is 0, then we don't know that much about their ideology. It could be an anywhere from -3 (strong Democrat) to +3 (strong Republican). 

<!-- DK: We need more instruction. But do we need it here or in an appendix? We should look at this plot, change the workflow, look at the plot again, and so on. -->

## Bayesian linear model

Create the workflow object with the model engine. Note that every part is the same as the linear model we just completed *except* that the required engine is "stan". 

```{r}
stan_wfl <- workflow() %>% 
  add_model(linear_reg() %>%
            set_engine("stan") %>%
            set_mode("regression"))
```

Add a recipe.

```{r}
stan_wfl <- workflow() %>% 
  add_model(linear_reg() %>%
            set_engine("stan") %>%
            set_mode("regression")) %>% 
  add_recipe(recipe(ideology ~  gender + race + region + 
                      income + age + education,
                    data = ch11_train) %>% 
             step_dummy(all_nominal())
             ) 
```

Examine performance on the cross-validation samples.

<!-- DK: Unlike with lm() above, we get different answers depending on whether or not we use step_dummy(all_nominal()). But the differences are very small (4th decimal, at least in the collected metrics), so let's ignore it for now. -->

```{r}
stan_metrics <- stan_wfl %>% 
  fit_resamples(resamples = ch11_folds) %>% 
  collect_metrics()

stan_metrics
```

Check the predictions against the actual values.

```{r}
stan_wfl %>% 
  fit(data = ch11_train) %>% 
  predict(new_data = ch11_train) %>% 
  bind_cols(ch11_train %>% select(ideology)) %>% 
  ggplot(aes(y = ideology, x = .pred)) +
    geom_jitter(height = 0.2, alpha = 0.01) +
    labs(title = "Predicting Ideology",
         subtitle = "Using a Bayesian linear model with demographic predictors",
         x = "Predicted Ideology",
         y = "Ideology",
         caption = "Source: CCES")
```

<!-- DK: Add more discussion. Perhaps discuss the posterior probability distribution for true ideology given predicted ideology. Note that the model never predicts ideology 2 or 3. Is that good or bad? -->

## Neural networks

<!-- DK: Switch this to neural network. Make it work better than use it with temperance. Show how you can't just use coefficients with serious models. Save random forest for chapter 12. -->

Neural networks are a powerful "non-parametric" approach to forecasting. Recall that, in both `lm()` and `stan_glm()` models, there are specified parameters which we are trying to estimate. We "care" about these parameters because they often correspond to real world entities, like the average treatment effect. 

With non-parametric models, there are no parameters which we care about. (That is a loose and not-quite-correct definition.) Instead, the model is a "black box" into which predictors are fed and from which predicted outcomes emerge. 

Create the workflow object with the model engine.

```{r}
nn_wfl <- workflow() %>% 
  add_model(mlp(epochs = 100, 
                hidden_units = 5, 
                dropout = 0.1) %>%
            set_engine("keras", verbose = 0) %>% 
            set_mode("regression"))
```

The next step is to add a recipe.

<!-- DK: Why can't we include year here? -->

```{r}
nn_wfl <- workflow() %>% 
  add_model(mlp(epochs = 100, 
                hidden_units = 5, 
                dropout = 0.1) %>%
            set_engine("nnet") %>% 
            set_mode("regression")) %>%
  add_recipe(recipe(ideology ~  gender + race + region + 
                      income + age + education,
                    data = ch11_train) %>% 
             step_dummy(all_nominal())
             )

             
```

Note that there are improvements we could make to this model. As a rule of thumb, whenever using non-parametric models, you should use `step_normalize(all_numeric())` which, as you might guess, normalizes all numeric variables. 

Examine performance on the cross-validation samples.


```{r}
nn_metrics <- nn_wfl %>% 
  fit_resamples(resamples = ch11_folds) %>% 
  collect_metrics()

nn_metrics 
```

Check the predictions against the actual values.

```{r}
nn_wfl %>% 
  fit(data = ch11_train) %>% 
  predict(new_data = ch11_train) %>% 
  bind_cols(ch11_train %>% select(ideology)) %>% 
  ggplot(aes(y = ideology, x = .pred)) +
    geom_jitter(width = 0.2, alpha = 0.01) +
    labs(title = "Predicting Ideology",
         subtitle = "Using a neural network with demographic predictors",
         x = "Predicted Ideology",
         y = "Ideology",
         caption = "Source: CCES")
```

<!-- DK: The last two graphs are different. Discuss! See how well this model does with low predictions. Every time predicted ideology is below -2.5, the true ideology is -3. The linear models are much more likely to make mistakes when predicting very negative ideology values. -->


## Model comparison

The most common method for deciding which model to choose is to look at which one does the best predicting out of sample. As a reminder, here is a summary of how well our three different models performed.

```{r}
tibble(model = rep(c("Linear Model", 
                     "Bayesian Linear Model", 
                     "Neural Network"), each = 2)) %>% 
  bind_cols(bind_rows(lm_metrics, stan_metrics, nn_metrics)) 


```
The results for the neural network model are slightly better than those for either linear model. 


## Cardinal virtues

```{r echo=FALSE, fig.margin=TRUE}
knitr::include_graphics("other/images/Wisdom.jpg")
```


```{r echo=FALSE, fig.margin=TRUE}
knitr::include_graphics("other/images/Justice.jpg")
```

The lessons of the Cardinal Virtues continue to apply.

*Wisdom* asks us if the problem we are trying to solve is "close enough" to the data we have that we might fairly consider both to come from the same population. Using US data from 2016 to create a model of ideology for the US in 2020 is reasonable. Using US data from 2016 to create a model of ideology for Brazil in 1966 is less so. 

*Justice* instructs us to distinguish causal from predictive models. This mode is clearly predictive. None of our demographic covariates were randomly assigned. It is possible to manipulate a variable like income, and therefore to consider (at least) two potential outcomes for a given person: ideology if rich and ideology if poor. But, although a causal model for the effect of income on ideology is at least conceivable, it is highly unlikely that such a model would provide a good estimate of the causal effect because income was not assigned randomly. Without random assignment, estimate causal effects is very, very difficult. *Regressions and Other Stories* by Andrew Gelman,  Jennifer Hill, and Aki Vehtari is a good introduction.

 
Recall that all we care about in a predictive model is forecasting some value $y_i$ given that we know $x_{i_1}, x_{i_2}, ... x_{i_n}$. The $y_i$ in our case is `ideology`. The $x_{i_1}, x_{i_2}, ... x_{i_n}$ are certain known variables, such as `state`, `age`, and `income`, among others. $\beta$ stands the list of unknown parameters which must be estimated. 

The following equation calculates the `ideology` of the *i*th respondent, $y_i$, as a function of the data and the unknown parameters. 

$$y_i = f(x_{i_1}, x_{i_2}, ..., \beta)$$

Keep in mind that our goal is to create the best possible model to predict someone's ideology given a number of demographic variables. That is to say, we plan on using our model on out-of-sample data. Cross validation is the method we will use to forecast how well the model will work on this unseen data.

```{r echo=FALSE, fig.margin=TRUE}
knitr::include_graphics("other/images/Courage.jpg")
```

*Courage* helps us to fit the models, as we have above, and then to select the one we want to use. We should only favor more complex models (or formulas) if the additional complexity is *warranted*. This is a philosophical principle known as "Occam's Razor." It states that, "all other things being equal, simpler solutions are more likely to be correct than complex ones." When viewed in a modeling framework, Occam's Razor can be restated as, "all other things being equal, simpler models are to be preferred over complex ones." In other words, we should only favor the more complex model/formula if the additional complexity makes the model meaningfully better.

Is the neural network model meaningfully better than the linear models? Perhaps. Average RMSE is lower but, given the standard error associated with that measure, it is close call. Regardless, let's use the neural network model.

Now that we have chosen a model, we are finally ready to use our test data.

```{r}
nn_wfl %>% 
  fit(data = ch11_train) %>% 
  predict(new_data = ch11_test) %>% 
  bind_cols(ch11_test %>% select(ideology)) %>% 
  metrics(truth = ideology, estimate = .pred)
```

Our best guess for the RMSE we will see in the future is approximately 1.97. 

<!-- DK: Why not use full data below? -->

The full plot is:

```{r}
nn_wfl %>% 
  fit(data = ch11_train) %>% 
  predict(new_data = ch11_test) %>% 
  bind_cols(ch11_test %>% select(ideology)) %>% 
  ggplot(aes(y = ideology, x = .pred)) +
    geom_jitter(height = 0.2, alpha = 0.01) +
    labs(title = "Predicting Ideology",
         subtitle = "Using a neural network model with demographic predictors",
         x = "Predicted Ideology",
         y = "Ideology",
         caption = "Source: CCES") 
```



<!-- Time to make up some imaginary people. Let's say we have four individuals whose ideology in 2016 we wanted to predict. We can create a tibble with the values of their demographic information, like so: -->

<!-- Just because you have a variable in this training data today, does not mean you are going to get it in your production data tomorrow. -->

<!-- DK: Do these have to be factors? Or will characters work as well, like they do with posterior_predict? -->


<!-- ```{r} -->
<!-- new_people <- tibble("name" = c("Alice", "Betty", "Chelsea", "Danielle"), -->
<!--                      "region" = as.factor(c("Midwest", "Northeast", "South", "West")), -->
<!--                      "gender" = as.factor(c("Female", "Female", "Female", "Female")), -->
<!--                      "income" = as.factor(c("34 - 67", "34 - 67", "34 - 67", "34 - 67")), -->
<!--                      "age" = c("17 - 24", "17 - 24", "17 - 24", "17 - 24"), -->
<!--                      "education" = c("College", "College", "College", "College"), -->
<!--                      "race" = c("White", "White", "White", "White")) -->
<!-- ``` -->

<!-- Now, let's predict each new person's ideology using the linear regression model we just created. -->

<!-- ```{r message=FALSE} -->
<!-- lm_model %>% -->
<!--   fit(full_form, data = ch11_train) %>% -->
<!--   predict(new_data = new_people) %>% -->
<!--   bind_cols(new_people) %>% -->
<!--   rename("ideology" = ".pred") -->
<!-- ``` -->


```{r echo=FALSE, fig.margin=TRUE}
knitr::include_graphics("other/images/Temperance.jpg")
```

*Temperance* reminds us to be suspicious of all models, especially our own. The world is always changing. Our models will rarely work as well in the future as they have using data from the past. But models are better than nothing, so we will use our neural network model to answer the question with which we started this chapter:

*How has ideology varied with age in the United States?*

### Ideology and age without a model

To some extent, we can answer this question without a model. Algebra is a powerful tool.

```{r}
ch11 %>% 
  group_by(age) %>% 
  summarize(avg_ideology = mean(ideology), .groups = "drop") %>% 
  ggplot(aes(x = age, y = avg_ideology)) +
    geom_point() +
    labs(title = "Ideology and Age 1954 -- 2016",
         subtitle = "Elderly Americans are less liberal on -3 to 3 Scale",
         caption = "National Election Survey", 
         y = "Average Ideology",
         x = "Age Category")
```

That works fine, but we are not interested in just average ideology for everyone in, say, the 35--44 age group. We want to see how ideology differs by gender, race, region, education, income and so on. For example, how what about Black women in the West with Advanced Degrees?

```{r}
ch11 %>%
  filter(gender == "Female",
         race == "Black",
         region == "West",
         education == "Adv. Degree") %>% 
  group_by(age) %>% 
  summarize(avg_ideology = mean(ideology), .groups = "drop") %>% 
  ggplot(aes(x = age, y = avg_ideology)) +
    geom_point() +
    scale_x_discrete(drop = FALSE) +
    labs(title = "Black Women in the West with Advanced Degrees",
         subtitle = "Only data for 6 people total, in 4 age categories",
         caption = "National Election Survey", 
         y = "Average Ideology",
         x = "Age Category")
```

<!-- DK: Calculate these numbers live? They wont be right when we add the new NES data. -->

We have zero observations for 3 of the age categories. The other four averages only have a total 6 observations to work with. The technical term for a graphic like this is "garbage."

Does that mean that we know *nothing* about Black women in the West with advanced degrees who are 75+? No! Just because there are no observations for a specific bucket does not mean that we are blind. The ideology of Black women in other regions tells us something about Black women in the West. The ideology of women with advanced degrees in the West tells us something about similar Black women who do not appear in our sample. And so on. *A model allows us to make inferences about places where our data is sparse.*

<!-- DK: Talk about infinite Preceptor Table? -->

### Ideology and age with a model

Now that we have selected our model, it is time to use all the data to fit it. Recall that the purpose of not using all the data up until model selection was to avoid fooling ourselves into thinking that a specific model was better than it actually was. Not looking at the test data was the discipline we needed. With that stage of the modeling complete, we now want to create the best model we can, which means using all the data available to us.

Since this fit might take some time, we just do it once and save the resulting object.

<!-- DK: Annoying that confidence intervals are not built in. But this scatter approach isn't bad . . . I think. -->

```{r}
nn_fit <- nn_wfl %>% 
  fit(data = ch11)
```

Note our use of a "fit" suffix to indicate that this is a fitted object. We can use the fitted object to estimate the expected ideology for a specific category of person. For example:

```{r}
new_obs <- tibble(gender = "Female", 
                  race = "Black",
                  region = "West",
                  age = "75 +",
                  education = "Adv. Degree",
                  income = "68 - 95")

predict(nn_fit, new_data = new_obs)
```

Again, we have no individuals in our data set, despite having 
`r scales::comma(nrow(ch11))` rows, with these characteristics. If we were not willing to model, we wouldn't be able to know anything. Does that seem sensible to you? The average ideology for the whole sample is `r mean(ch11$ideology)`. Among subgroups, the average is `r mean(ch11$ideology[ch11$gender == "Female"])` for women, `r mean(ch11$ideology[ch11$education == "Adv. Degree"])` for those with advanced degrees and `r mean(ch11$ideology[ch11$race == "Black"])`. Given that information, would you really think that it is a 50/50 bet whether or not female, Black, advanced degree holders (regardless of their other attributes) are above or below the overall average? Of course not! You would expect them to be below that average.

In addition to looking at specific categories of people, we can look at a range.


```{r}
new_obs <- tibble(gender = "Female", 
                  race = "Black",
                  region = "West",
                  age = "75 +",
                  education = "Adv. Degree",
                  income = levels(ch11$income))

predict(nn_fit, new_data = new_obs) %>% 
  bind_cols(tibble(income = levels(ch11$income)))
  
```

Again, these are estimated values for individuals for whom we have not direct data. There is not a single individual in our sample with these characteristics. But, because we have a model, we can estimate what the average ideology is for them. 

The difficulty arises because we have tens of thousands of types of people for whom we would like an estimated average ideology. We certainly don't want to type in all the different types by hand. `expand_grid()` comes to the rescue 

```{r}
new_obs <- expand_grid(ch11 %>% select(-ideology, -year))

sample_n(new_obs, 5)
```

`new_obs` has `r scales::comma(nrow(new_obs))` rows, meaning that many unique types of people. For some types of people, we have a fair amount of data:

```{r}
ch11 %>% 
  count(gender, race, income, age, education, region) %>% 
  arrange(desc(n))
```

But many of the individual cells are empty. Without a model, we can't make any estimates for average ideology for those cells. With a model, we can:


```{r}
us_ideology <- predict(nn_fit, new_data = new_obs) %>% 
  bind_cols(new_obs) %>% 
  filter(! gender == "Other") %>% 
  filter(race %in% c("White", "Black", "Hispanic")) 

```


```{r}
us_ideology %>% 
  ggplot(aes(x = age, y = .pred, color = education)) +
    geom_point(alpha = 0.01) +
    facet_wrap(~ gender*race)

```


<!-- DK: Use a plot like this one: https://www.tidymodels.org/start/models/ -->

## Summary

The purpose of this chapter way to practice using tidymodels tools for model selection. The most important measure of model quality is how well the model does on out-of-sample data. Cross validation is only one of many ways to estimate that performance, but it is probably the most popular. For details on other approaches, read [Chapter 10](https://www.tmwr.org/resampling.html) of *Tidy Modeling with R* by Max Kuhn and Julia Silge.
