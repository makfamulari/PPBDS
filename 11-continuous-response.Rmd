---
output_yaml:
  - _output.yml
---

# Continuous Response {#continuous-response}

**DRAFT: Final version available soon.**

<!-- DK: Error message from running nn fit_resamples. -->

Chapter \@ref(model-choice) showed us the *tidymodels* framework for model building and testing. In this chapter, we will use it to explore models in which the outcome variable is *continuous*. In Chapter \@ref(discrete-response), we learn about outcome variables which are *discrete*, meaning that they are members of distinct categories like TRUE/FALSE or yes/no. 

We use `nes` from the `PPBDS.data` package. `nes` contains data from the American National Election Survey for every presidential election year since 1952. Along with demographic details, such as race, gender, and age, the survey also contains respondents' ideological identification. Because `ideology` is measured on a scale from -3 to 3, we can treat it as a continuous outcome variable.

<!-- DK: Is this a good question? Maybe save for the end? -->

<!-- Throughout this chapter, we seek to answer this question: Which variables are associated with one's ideology, and how so? -->

## Exploratory Data Analysis

Packages:

```{r message=FALSE}
library(PPBDS.data)
library(skimr)
library(tidyverse)
library(tidymodels)
library(rstanarm)
```

The **tidymodels** framework makes it easy to use functions from different model packages in constructing, fitting, and testing models. 


```{r}
glimpse(nes)
```

Variables available in `nes`:

<!-- DK: Is voted in previous election or in this one? -->

* `year`: the year the study was conducted.
* `state`: abbreviation for state of residence.
* `gender`: identifies respondents with values "Male" and "Female".
* `race`: race/ethnicity respondent identification.
* `income`: 5 income groups described with percentile range: 0-16, 17-33, 34 to 67, 68-95, and 96-100. 
* `age`: ranges for respondents' age.
* `education`: seven categories of educational achievement.
* `pres_appr`: respondents' self-reported approval of the sitting president.
* `voted`: whether the respondent had voted in the presidential election.
* `ideology` a continuous variable with -3 corresponding to strongly Democrat and 3 corresponding to strongly Republican.
* `region`: US region: Northeast, Midwest, West, and South. 

Given knowledge of these variables for a new person, our goal is to predict their ideology correctly. 

```{r}
skim(nes)
```

There are three data types: factors, characters, and numeric. `income`, `age`, and `education` are ordered factors. This means that each factor level has a relationship with the others. In the `age` variable, for example, `25 - 34` is bigger `17 - 24` and smaller than `35 - 44`. But there is no sense of *how much* bigger or smaller it is. Ordered factors can have weird effects in certain models, so we need to be wary.

`nes` covers 1952 through 2016, but we will narrow our scope to just 2016 because of the changing nature of what makes someone ideologically liberal or ideologically conservative over time. Keep in mind that, when feeding in new data to our model, it is most accurately applied to voters in or around 2016. 

Note that all of the variables are incomplete, meaning they contain `NA` values. While there are methods to impute missing data, we will simply remove these values for now.


```{r message=FALSE}
ch11 <- nes %>%
  select(year, gender, race, income, age, education, ideology, region) %>%
  filter(year == 2016) %>% 
  drop_na()
```

<!-- DK: What should I do with ordered factors? Change them here? Change them in nes itself? If I leave them, will our various models be able to do something with them? Perhaps I will need to use a step_* to fix? Or maybe step_dummy() takes care of that? -->

The first step in data science is to split our data set into training and testing samples, and then to create cross-validations of the training data.

```{r}
set.seed(10)
ch11_split <- initial_split(ch11, prob = 0.80)
ch11_train <- training(ch11_split)
ch11_test  <- testing(ch11_split)
ch11_folds <- vfold_cv(ch11_train, v = 10)
```


We can now explore three different models for this problem: a traditional linear model, a Bayesian linear model, and a neural network.

<!-- DK: Could give a brief review of each model at the start, perhaps even fit it on the whole data set and discuss what you see. Or maybe do that all at once, in this section here. -->

## Linear model

Create the workflow object with the model engine.

```{r}
lm_wfl <- workflow() %>% 
  add_model(linear_reg() %>%
            set_engine("lm") %>%
            set_mode("regression"))
```

The ***parsnip*** package allows us to create an engine that can run this linear regression model easily and repeatedly. `linear_reg()` tells the engine that this is a linear regression. `set_engine("lm")` tells the engine to use the `lm()` function. `set_mode()` has two options: "regression" and "classification." Since our left-hand variable is continuous, we will set it to "regression." If it were categorical, we would set the mode to "classification". The default is "regression," so the `set_mode()` command has no effect in this case.


Add a recipe.

```{r}
lm_wfl <- workflow() %>% 
  add_model(linear_reg() %>%
            set_engine("lm") %>%
            set_mode("regression")) %>% 
  add_recipe(recipe(ideology ~ gender + race + region + 
                      income + age + education,
                    data = ch11_train)) %>%
  step_dummy(all_nominal())
  
```

Examine performance on the cross-validation samples.

```{r}
lm_metrics <- lm_wfl %>% 
  fit_resamples(resamples = ch11_folds) %>% 
  collect_metrics()

lm_metrics
```

Check the predictions against the actual values.

```{r}
lm_wfl %>% 
  fit(data = ch11_train) %>% 
  predict(new_data = ch11_train) %>% 
  bind_cols(ch11_train %>% select(ideology)) %>% 
  ggplot(aes(y = ideology, x = `.pred`)) +
    geom_jitter(height = 0.2, alpha = 0.1) +
    labs(title = "Predicting Ideology",
         subtitle = "Using a linear model with demographic predictors",
         x = "Predicted Ideology",
         y = "Ideology",
         caption = "Source: 2016 CCES") 
```

Is this result good or bad? Depends on your point of view! If we predict that someone's ideology is 0, then we don't know that much about their ideology. It could be an anywhere from -3 (strong Democrat) to +3 (strong Republican). 

## Bayesian linear model

Create the workflow object with the model engine.

```{r}
stan_wfl <- workflow() %>% 
  add_model(linear_reg() %>%
            set_engine("stan") %>%
            set_mode("regression"))
```

Add a recipe.

```{r}
stan_wfl <- workflow() %>% 
  add_model(linear_reg() %>%
            set_engine("stan") %>%
            set_mode("regression")) %>% 
  add_recipe(recipe(ideology ~ gender + race + region + 
                      income + age + education,
                    data = ch11_train)) %>%
  step_dummy(all_nominal())
```

Examine performance on the cross-validation samples.

```{r}
stan_metrics <- stan_wfl %>% 
  fit_resamples(resamples = ch11_folds) %>% 
  collect_metrics()

stan_metrics
```

Check the predictions against the actual values.

```{r}
stan_wfl %>% 
  fit(data = ch11_train) %>% 
  predict(new_data = ch11_train) %>% 
  bind_cols(ch11_train %>% select(ideology)) %>% 
  ggplot(aes(y = ideology, x = `.pred`)) +
    geom_jitter(height = 0.2, alpha = 0.1) +
    labs(title = "Predicting Ideology",
         subtitle = "Using a Bayesian linear model with demographic predictors",
         x = "Predicted Ideology",
         y = "Ideology",
         caption = "Source: 2016 CCES")
```



## Random forest


Create the workflow object with the model engine.

```{r}
rf_wfl <- workflow() %>% 
  add_model(rand_forest('regression') %>%
            set_engine("ranger"))
```

Add a recipe.

```{r}
rf_wfl <- workflow() %>% 
  add_model(rand_forest('regression') %>%
            set_engine("ranger")) %>% 
  add_recipe(recipe(ideology ~ gender + race + region + 
                      income + age + education,
                    data = ch11_train)) %>%
  # step_naomit(everything(), skip = FALSE) %>%
  # step_normalize(all_numeric()) %>% 
  step_dummy(all_nominal()) 

  
```

Examine performance on the cross-validation samples.


```{r}
rf_metrics <- rf_wfl %>% 
  fit_resamples(resamples = ch11_folds) %>% 
  collect_metrics()

rf_metrics 
```

Check the predictions against the actual values.

```{r}
rf_wfl %>% 
  fit(data = ch11_train) %>% 
  predict(new_data = ch11_train) %>% 
  bind_cols(ch11_train %>% select(ideology)) %>% 
  ggplot(aes(y = ideology, x = `.pred`)) +
    geom_jitter(width = 0.2, alpha = 0.1) +
    labs(title = "Predicting Ideology",
         subtitle = "Using a random forest with demographic predictors",
         x = "Predicted Ideology",
         y = "Ideology",
         caption = "Source: 2016 CCES")
```


## Model comparison

The most common method for deciding which model to choose is to look at which one does the best predicting out of sample. As a reminder, here is a summary of how well our three different models performed.

```{r}
tibble(model = rep(c("Linear Model", 
                     "Bayesian Linear Model", 
                     "Random Forest"), each = 2)) %>% 
  bind_cols(bind_rows(lm_metrics, stan_metrics, rf_metrics)) 


```
The results for the random forest model are worse than those for either linear model. The results for the imear models are so similar that there is no particular reason, in terms of accuracy, to prefer one model over the others.


## Cardinal virtues

```{r echo=FALSE, fig.margin=TRUE}
knitr::include_graphics("other/images/Wisdom.jpg")
```


```{r echo=FALSE, fig.margin=TRUE}
knitr::include_graphics("other/images/Justice.jpg")
```

The lessons of the Cardinal Virtues continue to apply.

*Wisdom* asks us if the problem we are trying to solve is "close enough" to the data we have that we might fairly consider both to come from the same population. Using US data from 2016 to create a model of ideology for the US in 2020 is reasonable. Using US data from 2016 to create a model of ideology for Brazil in 1966 is less so. 

*Justice* instructs us to distinguish causal from predictive models. This mode is clearly predictive. None of our demographic covariates were randomly assigned. It is possible to manipulate a variable like income, and therefore to consider (at least) two potential outcomes for a given person: ideology if rich and ideology if poor. But, although a causal model for the effect of income on ideology is at least conceivable, it is highly unlikely that such a model would provide a good estimate of the causal effect because income was not assigned randomly. Without random assignment, estimate causal effects is very, very difficult. *Regressions and Other Stories* by Andrew Gelman,  Jennifer Hill, and Aki Vehtari is a good introduction.

 
Recall that all we care about in a predictive model is forecasting some value $y_i$ given that we know $x_{i_1}, x_{i_2}, ... x_{i_n}$. The $y_i$ in our case is `ideology`. The $x_{i_1}, x_{i_2}, ... x_{i_n}$ are certain known variables, such as `state`, `age`, and `income`, among others. $\beta$ stands the list of unknown parameters which must be estimated. 

The following equation calculates the `ideology` of the *i*th respondent, $y_i$, as a function of the data and the unknown parameters. 

$$y_i = f(x_{i_1}, x_{i_2}, ..., \beta)$$

Keep in mind that our goal is to create the best possible model to predict someone's ideology given a number of demographic variables. That is to say, we plan on using our model on out-of-sample data. Cross validation is the method we will use to forecast how well the model will work on this unseen data.

```{r echo=FALSE, fig.margin=TRUE}
knitr::include_graphics("other/images/Courage.jpg")
```

*Courage* helps us to fit the models, as we have above, and then to select the one we want to use. We should only favor more complex models (or formulas) if the additional complexity is *warranted*. This is a philosophical principle known as "Occam's Razor." It states that, "all other things being equal, simpler solutions are more likely to be correct than complex ones." When viewed in a modeling framework, Occam's Razor can be restated as, "all other things being equal, simpler models are to be preferred over complex ones." In other words, we should only favor the more complex model/formula if the additional complexity makes the model meaningfully better.

In general, linear models are much simpler than Bayesian models. So, given that these two models did about as well under cross-validation, we should select the linear model.


<!-- Time to make up some imaginary people. Let's say we have four individuals whose ideology in 2016 we wanted to predict. We can create a tibble with the values of their demographic information, like so: -->

<!-- Just because you have a variable in this training data today, does not mean you are going to get it in your production data tomorrow. -->

<!-- DK: Do these have to be factors? Or will characters work as well, like they do with posterior_predict? -->


<!-- ```{r} -->
<!-- new_people <- tibble("name" = c("Alice", "Betty", "Chelsea", "Danielle"), -->
<!--                      "region" = as.factor(c("Midwest", "Northeast", "South", "West")), -->
<!--                      "gender" = as.factor(c("Female", "Female", "Female", "Female")), -->
<!--                      "income" = as.factor(c("34 - 67", "34 - 67", "34 - 67", "34 - 67")), -->
<!--                      "age" = c("17 - 24", "17 - 24", "17 - 24", "17 - 24"), -->
<!--                      "education" = c("College", "College", "College", "College"), -->
<!--                      "race" = c("White", "White", "White", "White")) -->
<!-- ``` -->

<!-- Now, let's predict each new person's ideology using the linear regression model we just created. -->

<!-- ```{r message=FALSE} -->
<!-- lm_model %>% -->
<!--   fit(full_form, data = ch11_train) %>% -->
<!--   predict(new_data = new_people) %>% -->
<!--   bind_cols(new_people) %>% -->
<!--   rename("ideology" = ".pred") -->
<!-- ``` -->

<!-- As you can see, the constant variables are `gender`, `income`, `education`, `age`, and `race`. The most conservative woman was Chelsea, from the South, and the most liberal woman was Betty, from the Northeast. This, of course, can simply be calculated algebraically from the `lm` model. However, we cannot generalize by saying that all young women can be expected to follow this pattern based off of their region. This prediction is only valid for the levels of the variables that we have set. -->



<!-- ```{r} -->
<!-- ch11_formulas <- tibble(formula = c(age_form, -->
<!--                                    race_gender_form, -->
<!--                                    full_form, -->
<!--                                    interact_form), -->
<!--                        group = c("Age only model", -->
<!--                                  "Race and gender model", -->
<!--                                  "Full model without interaction", -->
<!--                                  "Interaction model")) -->
<!-- ``` -->

<!-- Now, we can use `map_*` to apply all of these models and view their metrics to see which ones have the lowest rmse values. -->

<!-- ```{r cache=TRUE, message=FALSE} -->
<!-- set.seed(10) -->
<!-- folds_metrics <- ch11_formulas %>% -->
<!--   mutate(metrics = map(formula, ~ fit_resamples(object = lm_model, -->
<!--                                                 preprocessor = ., -->
<!--                                                 resamples = ch11_folds) %>% -->
<!--                          collect_metrics())) -->
<!-- ``` -->

<!-- Let's present the results stored in our `folds_metrics` object. We are simply extracting the `rmse` metric from each formula: -->

<!-- ```{r} -->
<!-- folds_metrics %>% -->
<!--   mutate(mean_rmse = map_dbl(metrics, ~ filter(., .metric == "rmse") %>% pull(mean)), -->
<!--          se_rmse = map_dbl(metrics, ~ filter(., .metric == "rmse") %>% pull(std_err))) %>% -->
<!--   select(group, mean_rmse, se_rmse) -->
<!-- ``` -->

<!-- Looking at the results, all of the formulas seem to yield a relatively close mean `rmse`. -->

<!-- The models with the lowest rmse value seems to be the Full model without interaction. The mean squared error is close in value to that of the interaction model. -->





```{r echo=FALSE, fig.margin=TRUE}
knitr::include_graphics("other/images/Temperance.jpg")
```

*Temperance* reminds us to be suspicious of all models, especially our own. The world is always changing. Our models will rarely work as well in the future as they have using data from the past.



<!-- However, remember that `rmse` and `rsq` values are not end-all-be-all. There are other ways to to determine the best model: Did it predict low ideologies well? What about high ideologies? Did it do a good job of generally sorting people into the right end of politically liberal or politically conservative? Were there frequent outlandish answers generated, such as a quantity of `ideology` too high to exist? -->

<!-- Indeed, even the `rmse` value's validity can be up for debate, as it takes the mean squared error. Why not take an absolute value so that larger differences do not seem even larger after getting squared? These are questions you should be asking when relying on a metric to compare models. -->


<!-- Let's loop back to the original guiding question. How does age affect ideology for respondents from different regions? -->

<!-- Assume that the segment of the population we are interested in is college educated men making an average income. These are vital assumptions to make, as without setting baselines for the other predictor variables, we cannot make any comparisons on region and age. -->

<!-- Say we are specifically interested in the different effects of ages on college-educated men in the Northeast versus the South. Let's use our second model, `stan_model`, to explore this problem. -->

<!-- First, create a tibble in which all respondents are college-educated white men making average income. Half of the group will be from the Northeast, and half from the South. -->

<!-- ```{r} -->
<!-- wrap_up <- tibble("region" = as.factor(c("Northeast", "South", "Northeast", "South", -->
<!--                                             "Northeast", "South", "Northeast", "South", -->
<!--                                             "Northeast", "South", "Northeast", "South", -->
<!--                                             "Northeast", "South")), -->
<!--                      "gender" = as.factor(c("Male", "Male", "Male", "Male", -->
<!--                                             "Male", "Male", "Male", "Male", -->
<!--                                             "Male", "Male", "Male", "Male", -->
<!--                                             "Male", "Male")), -->
<!--                      "income" = as.factor(c("34 - 67", "34 - 67", "34 - 67", "34 - 67", -->
<!--                                             "34 - 67", "34 - 67", "34 - 67", "34 - 67", -->
<!--                                             "34 - 67", "34 - 67", "34 - 67", "34 - 67", -->
<!--                                             "34 - 67", "34 - 67")), -->
<!--                      "age" = c("17 - 24", "17 - 24", "25 - 34", "25 - 34", -->
<!--                                "35 - 44", "35 - 44", "45 - 54", "45 - 54", -->
<!--                                "55 - 64", "55 - 64", "65 - 74", "65 - 74", "75 +", "75 +"), -->
<!--                      "education" = c("College", "College", "College", "College", "College", "College", "College", "College", "College", "College", "College", "College", "College", "College"), -->
<!--                      "race" = c("White", "White", "White", "White", "White", "White", "White", "White", "White", "White", "White", "White", "White", "White")) -->
<!-- ``` -->

<!-- Next, obtain the `ideology` values by fitting like we did before. -->


## Summary

The purpose of this chapter way to practice using tidymodels tools for model selection. The most important measure of model quality is how well the model does on out-of-sample data. Cross validation is only one of many ways to estimate that performance, but it is probably the most popular. For details on other approaches, read [Chapter 10](https://www.tmwr.org/resampling.html) of *Tidy Modeling with R* by Max Kuhn and Julia Silge.
